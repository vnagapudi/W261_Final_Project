{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages \n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"project\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"csv\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"header\", \"false\") \\\n",
    "               .option(\"delimiter\", \"\\t\")\\\n",
    "               .load(\"data/sample_training.txt\") #Adjust path as needed\n",
    "\n",
    "df = df.withColumnRenamed(\"_c0\", \"CTR\") \\\n",
    "       .withColumnRenamed(\"_c1\", \"NumVar1\") \\\n",
    "       .withColumnRenamed(\"_c2\", \"NumVar2\") \\\n",
    "       .withColumnRenamed(\"_c3\", \"NumVar3\") \\\n",
    "       .withColumnRenamed(\"_c4\", \"NumVar4\") \\\n",
    "       .withColumnRenamed(\"_c5\", \"NumVar5\") \\\n",
    "       .withColumnRenamed(\"_c6\", \"NumVar6\") \\\n",
    "       .withColumnRenamed(\"_c7\", \"NumVar7\") \\\n",
    "       .withColumnRenamed(\"_c8\", \"NumVar8\") \\\n",
    "       .withColumnRenamed(\"_c9\", \"NumVar9\") \\\n",
    "       .withColumnRenamed(\"_c10\", \"NumVar10\") \\\n",
    "       .withColumnRenamed(\"_c11\", \"NumVar11\") \\\n",
    "       .withColumnRenamed(\"_c12\", \"NumVar12\") \\\n",
    "       .withColumnRenamed(\"_c13\", \"NumVar13\") \\\n",
    "        .withColumnRenamed(\"_c14\", \"NumVar14\") \\\n",
    "        .withColumnRenamed(\"_c15\", \"NumVar15\") \\\n",
    "        .withColumnRenamed(\"_c16\", \"NumVar16\") \\\n",
    "        .withColumnRenamed(\"_c17\", \"NumVar17\") \\\n",
    "        .withColumnRenamed(\"_c18\", \"NumVar18\") \\\n",
    "        .withColumnRenamed(\"_c19\", \"NumVar19\") \\\n",
    "        .withColumnRenamed(\"_c20\", \"NumVar20\") \\\n",
    "        .withColumnRenamed(\"_c21\", \"NumVar21\") \\\n",
    "        .withColumnRenamed(\"_c22\", \"NumVar22\") \\\n",
    "        .withColumnRenamed(\"_c23\", \"NumVar23\") \\\n",
    "        .withColumnRenamed(\"_c24\", \"NumVar24\") \\\n",
    "        .withColumnRenamed(\"_c25\", \"NumVar25\") \\\n",
    "        .withColumnRenamed(\"_c26\", \"NumVar26\") \\\n",
    "        .withColumnRenamed(\"_c27\", \"NumVar27\") \\\n",
    "        .withColumnRenamed(\"_c28\", \"NumVar28\") \\\n",
    "        .withColumnRenamed(\"_c29\", \"NumVar29\") \\\n",
    "        .withColumnRenamed(\"_c30\", \"NumVar30\") \\\n",
    "        .withColumnRenamed(\"_c31\", \"NumVar31\") \\\n",
    "        .withColumnRenamed(\"_c32\", \"NumVar32\") \\\n",
    "        .withColumnRenamed(\"_c33\", \"NumVar33\") \\\n",
    "        .withColumnRenamed(\"_c34\", \"NumVar34\") \\\n",
    "        .withColumnRenamed(\"_c35\", \"NumVar35\") \\\n",
    "        .withColumnRenamed(\"_c36\", \"NumVar36\") \\\n",
    "        .withColumnRenamed(\"_c37\", \"NumVar37\") \\\n",
    "        .withColumnRenamed(\"_c38\", \"NumVar38\") \\\n",
    "        .withColumnRenamed(\"_c39\", \"NumVar39\")\n",
    "\n",
    "\n",
    "#df.select(\"*\").show(5)\n",
    "#df.select(\"CTR\", \"NumVar1\", \"NumVar2\", \"NumVar3\", \"NumVar4\", \"NumVar5\", \"NumVar6\", \"NumVar7\", \"NumVar8\", \"NumVar9\", \"NumVar10\", \"NumVar11\", \"NumVar12\", \"NumVar13\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_ctr1 = df.filter(col(\"CTR\") == 1) #get all of the CTR == 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few (hopefully useful :-) ) pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587833"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute total count of 1 in target column\n",
    "#in the line below one could simply do .count(), since you've filtered previously. However,\n",
    "#I feel the alternative below is safer, if somebody changes the code later on and removes \n",
    "#the previous filtering...\n",
    "count_1 = subset_ctr1.groupBy().sum('CTR').collect()[0][0]\n",
    "count_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This column has: 10182 unique elements.\n",
      "This column has: 8043 unique elements.\n"
     ]
    }
   ],
   "source": [
    "#Select columns for computation\n",
    "compute_cols = ['NumVar20','NumVar28']\n",
    "\n",
    "#The one below counts the number of unique elements in the columns\n",
    "from pyspark.sql.functions import col\n",
    "for col in subset_ctr1[compute_cols]:\n",
    "    print('This column has: ' + str(subset_ctr1.select(col).distinct().count()) + ' unique elements.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|NumVar20|count|\n",
      "+--------+-----+\n",
      "|1c86e0eb|18899|\n",
      "|468a0854|11136|\n",
      "|7195046d| 8185|\n",
      "|407438c8| 6152|\n",
      "|dc7659bd| 6119|\n",
      "|8379baa1| 5037|\n",
      "|5e64ce5f| 4937|\n",
      "|fe4dce68| 4900|\n",
      "|81bb0302| 4608|\n",
      "|5d87968e| 4055|\n",
      "|ad3508b1| 3939|\n",
      "|d2dbdfe6| 3847|\n",
      "|90a2c015| 3372|\n",
      "|3f4ec687| 3286|\n",
      "|38eb9cf4| 3055|\n",
      "|45e063a0| 2874|\n",
      "|4aa938fc| 2873|\n",
      "|ce4f7f55| 2828|\n",
      "|8f572b5e| 2617|\n",
      "|7227c706| 2587|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-----+\n",
      "|NumVar28|count|\n",
      "+--------+-----+\n",
      "|2d0bb053| 7751|\n",
      "|d345b1a0| 5681|\n",
      "|46ed0b3c| 5342|\n",
      "|f6b23a53| 5336|\n",
      "|10935a85| 5234|\n",
      "|10040656| 5121|\n",
      "|36721ddc| 4889|\n",
      "|f3002fbd| 4742|\n",
      "|e1ac77f7| 4521|\n",
      "|cae64906| 4517|\n",
      "|310d155b| 4333|\n",
      "|34cce7d2| 4091|\n",
      "|3628a186| 4078|\n",
      "|9efd8b77| 3690|\n",
      "|a9d1ba1a| 3630|\n",
      "|25753fb1| 3596|\n",
      "|0af7c64c| 3539|\n",
      "|af56328b| 3360|\n",
      "|14108df6| 3246|\n",
      "|e8d4033b| 3224|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select columns for computation\n",
    "compute_cols = ['NumVar20','NumVar28']\n",
    "\n",
    "#The one below partitions the column by unique elements and counts the number of rows\n",
    "#for each unique element\n",
    "from pyspark.sql.functions import col, desc\n",
    "for col in subset_ctr1[compute_cols]:\n",
    "    df = subset_ctr1.groupBy([col]).count().sort(desc('count')).cache()\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------------+\n",
      "|NumVar20|count|         count_frequ|\n",
      "+--------+-----+--------------------+\n",
      "|1c86e0eb|18899| 0.03215028758167711|\n",
      "|468a0854|11136|0.018944155908225634|\n",
      "|7195046d| 8185| 0.01392402263908287|\n",
      "|407438c8| 6152|0.010465557394702237|\n",
      "|dc7659bd| 6119|0.010409419001655233|\n",
      "|8379baa1| 5037|0.008568760175083739|\n",
      "|5e64ce5f| 4937|0.008398643832517058|\n",
      "|fe4dce68| 4900|0.008335700785767387|\n",
      "|81bb0302| 4608|0.007838961065472676|\n",
      "|5d87968e| 4055|0.006898217691078929|\n",
      "|ad3508b1| 3939|0.006700882733701579|\n",
      "|d2dbdfe6| 3847|0.006544375698540232|\n",
      "|90a2c015| 3372|0.005736323071348...|\n",
      "|3f4ec687| 3286| 0.00559002301674115|\n",
      "|38eb9cf4| 3055|0.005197054265412...|\n",
      "|45e063a0| 2874|0.004889143685366422|\n",
      "|4aa938fc| 2873|0.004887442521940755|\n",
      "|ce4f7f55| 2828|0.004810890167785749|\n",
      "|8f572b5e| 2617|0.004451944684970051|\n",
      "|7227c706| 2587|0.004400909782200047|\n",
      "+--------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-----+--------------------+\n",
      "|NumVar28|count|         count_frequ|\n",
      "+--------+-----+--------------------+\n",
      "|2d0bb053| 7751|0.013185717712343472|\n",
      "|d345b1a0| 5681|0.009664309421213168|\n",
      "|46ed0b3c| 5342|0.009087615019912117|\n",
      "|f6b23a53| 5336|0.009077408039358118|\n",
      "|10935a85| 5234|0.008903889369940103|\n",
      "|10040656| 5121|0.008711657902839752|\n",
      "|36721ddc| 4889|0.008316987988085051|\n",
      "|f3002fbd| 4742| 0.00806691696451203|\n",
      "|e1ac77f7| 4521|0.007690959847439664|\n",
      "|cae64906| 4517|0.007684155193736997|\n",
      "|310d155b| 4333|0.007371141123414303|\n",
      "|34cce7d2| 4091|0.006959459574402934|\n",
      "|3628a186| 4078|0.006937344449869266|\n",
      "|9efd8b77| 3690|0.006277293040710542|\n",
      "|a9d1ba1a| 3630|0.006175223235170533|\n",
      "|25753fb1| 3596|0.006117383678697...|\n",
      "|0af7c64c| 3539|0.006020417363434...|\n",
      "|af56328b| 3360|0.005715909110240494|\n",
      "|14108df6| 3246|0.005521976479714477|\n",
      "|e8d4033b| 3224|0.005484550884349806|\n",
      "+--------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select columns for computation\n",
    "compute_cols = ['NumVar20','NumVar28']\n",
    "\n",
    "#The one below partitions the column by unique elements and counts the number of rows\n",
    "#for each unique element, then divides the number of rows for each unique element\n",
    "#by the total number of 1s (again, this should be the frequency you are referring to?)\n",
    "from pyspark.sql.functions import col, desc\n",
    "for col in subset_ctr1[compute_cols]:\n",
    "    df = subset_ctr1.groupBy([col]).count().sort(desc('count')).cache()\n",
    "    df = df.withColumn('count_frequ', df['count']/count_1)\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------------+-------------------+\n",
      "|NumVar20|count|         count_frequ|             cumsum|\n",
      "+--------+-----+--------------------+-------------------+\n",
      "|1c86e0eb|18899| 0.03215028758167711|0.03215028758167711|\n",
      "|468a0854|11136|0.018944155908225634|0.05109444348990275|\n",
      "|7195046d| 8185| 0.01392402263908287|0.06501846612898561|\n",
      "|407438c8| 6152|0.010465557394702237|0.07548402352368785|\n",
      "|dc7659bd| 6119|0.010409419001655233|0.08589344252534309|\n",
      "|8379baa1| 5037|0.008568760175083739|0.09446220270042682|\n",
      "|5e64ce5f| 4937|0.008398643832517058|0.10286084653294388|\n",
      "|fe4dce68| 4900|0.008335700785767387|0.11119654731871126|\n",
      "|81bb0302| 4608|0.007838961065472676|0.11903550838418393|\n",
      "|5d87968e| 4055|0.006898217691078929|0.12593372607526288|\n",
      "|ad3508b1| 3939|0.006700882733701579|0.13263460880896447|\n",
      "|d2dbdfe6| 3847|0.006544375698540232| 0.1391789845075047|\n",
      "|90a2c015| 3372|0.005736323071348...| 0.1449153075788532|\n",
      "|3f4ec687| 3286| 0.00559002301674115|0.15050533059559434|\n",
      "|38eb9cf4| 3055|0.005197054265412...|0.15570238486100646|\n",
      "|45e063a0| 2874|0.004889143685366422|0.16059152854637287|\n",
      "|4aa938fc| 2873|0.004887442521940755|0.16547897106831364|\n",
      "|ce4f7f55| 2828|0.004810890167785749|0.17028986123609938|\n",
      "|8f572b5e| 2617|0.004451944684970051|0.17474180592106942|\n",
      "|7227c706| 2587|0.004400909782200047|0.17914271570326945|\n",
      "+--------+-----+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-----+--------------------+--------------------+\n",
      "|NumVar28|count|         count_frequ|              cumsum|\n",
      "+--------+-----+--------------------+--------------------+\n",
      "|2d0bb053| 7751|0.013185717712343472|0.013185717712343472|\n",
      "|d345b1a0| 5681|0.009664309421213168| 0.02285002713355664|\n",
      "|46ed0b3c| 5342|0.009087615019912117|0.031937642153468754|\n",
      "|f6b23a53| 5336|0.009077408039358118| 0.04101505019282687|\n",
      "|10935a85| 5234|0.008903889369940103|0.049918939562766974|\n",
      "|10040656| 5121|0.008711657902839752|0.058630597465606726|\n",
      "|36721ddc| 4889|0.008316987988085051| 0.06694758545369178|\n",
      "|f3002fbd| 4742| 0.00806691696451203| 0.07501450241820382|\n",
      "|e1ac77f7| 4521|0.007690959847439664| 0.08270546226564349|\n",
      "|cae64906| 4517|0.007684155193736997| 0.09038961745938048|\n",
      "|310d155b| 4333|0.007371141123414303| 0.09776075858279479|\n",
      "|34cce7d2| 4091|0.006959459574402934| 0.10472021815719772|\n",
      "|3628a186| 4078|0.006937344449869266| 0.11165756260706698|\n",
      "|9efd8b77| 3690|0.006277293040710542| 0.11793485564777753|\n",
      "|a9d1ba1a| 3630|0.006175223235170533| 0.12411007888294806|\n",
      "|25753fb1| 3596|0.006117383678697...|  0.1302274625616459|\n",
      "|0af7c64c| 3539|0.006020417363434...| 0.13624787992508078|\n",
      "|af56328b| 3360|0.005715909110240494| 0.14196378903532128|\n",
      "|14108df6| 3246|0.005521976479714477| 0.14748576551503575|\n",
      "|e8d4033b| 3224|0.005484550884349806| 0.15297031639938558|\n",
      "+--------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select columns for computation\n",
    "compute_cols = ['NumVar20','NumVar28']\n",
    "\n",
    "#The one below partitions the column by unique elements and counts the number of rows\n",
    "#for each unique element, then divides the number of rows for each unique element\n",
    "#by the total number of 1s (again, this should be the frequency you are referring to?)\n",
    "from pyspark.sql.functions import col, desc\n",
    "from pyspark.sql import window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "win_spec = (window.Window\n",
    "                  .partitionBy()\n",
    "                  .rowsBetween(window.Window.unboundedPreceding, 0))\n",
    "\n",
    "#adapted from here: https://stackoverflow.com/questions/34726268/how-to-calculate-cumulative-sum-using-sqlcontext\n",
    "for col in subset_ctr1[compute_cols]:\n",
    "    df = subset_ctr1.groupBy([col]).count().sort(desc('count')).cache()\n",
    "    df = df.withColumn('count_frequ', df['count']/count_1).cache()\n",
    "    df = df.withColumn('cumsum',F.sum(df.count_frequ).over(win_spec))\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.filter(col('cumsum')<0.75).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
