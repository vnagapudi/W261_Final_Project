{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to account for the interaction term, we expand to include 2nd Degree polynomial features. The below equation represents the formulation.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} +  \\sum_{i=1}^n \\sum_{j=i+1}^n w_{ij} x_{i} x_{j}\n",
    "\\end{align}\n",
    "\n",
    "The challenge with solving the above equation is that the time complexity is $O(n^2)$\n",
    "\n",
    "In order to work with this, we use a matrix factorization technique for the interaction terms, inspired by Matrix factorization. We introduce a hyperpameter K which represent the latent factors for factorizing the weight vector $w_{ij}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\sum_{i=1}^{n} \\sum_{j=i+1}^n \\langle \\textbf{v}_i , \\textbf{v}_{j} \\rangle x_i x_{j}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Using the computation specified in Stephen Rendles paper, we can simplify the interaction term to the below equation.\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j}\n",
    "&= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j} - \\frac{1}{2} \\sum_{i=1}^n \\langle \\textbf{v}_i , \\textbf{v}_{i} \\rangle x_{i} x_{i}  \\\\\n",
    "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j} \\right)\\frac{1}{2}\\left( \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
    "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j}  -  \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
    "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left(\\sum_{i=1}^n v_{i,f}x_{i} \\right) \\left( \\sum_{j=1}^n v_{j,f}x_{j} \\right) - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right) \\\\\n",
    "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align}\n",
    "\n",
    "So, we can rewrite the equation to compute in $O(n)$ as\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "For our classification problem, we can define the gradients as :\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial\\theta}\\hat{y}(\\textbf{x}) =\n",
    "\\begin{cases}\n",
    "1,  & \\text{if $\\theta$ is $w_0$} \\\\\n",
    "x_i, & \\text{if $\\theta$ is $w_i$} \\\\\n",
    "x_i\\sum_{j=1}^{n} v_{j,f}x_j - v_{i,f}x_{i}^2 & \\text{if $\\theta$ is $v_{i,f}$}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial x}\\hat{y}(\\textbf{x}) =\n",
    "\\frac{d}{dx}\\left[ \\ln \\big(e^{-yx} + 1 \\big) \\right] \n",
    "&= \\frac{1}{e^{-yx} + 1} \\cdot  \\frac{d}{dx}\\left[e^{-yx} + 1 \\right] \\\\\n",
    "&= -\\frac{y}{e^{yx} + 1}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.storagelevel import *\n",
    "import pyspark.mllib.linalg\n",
    "import numpy as np\n",
    "from sklearn.metrics import auc, roc_curve, average_precision_score, log_loss, mean_squared_error\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize']=(16.0, 12.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"final_project\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[(1,1,2,1,0),1],[(1,0,2,1,0),0],[(0,1,2,1,0),0],[(1,3,2,1,0),1],[(1,1,2,1,0),0],[(1,1,5,1,0),1],[(1,4,2,1,1),0]]\n",
    "trainRDD = sc.parallelize(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmLoss(dataRDD, w) :\n",
    "    \"\"\"\n",
    "    Computes the logloss given the data and model W\n",
    "    dataRDD - array of features, label\n",
    "    \"\"\"\n",
    "    w_bc = sc.broadcast(w)\n",
    "    def probability_value(x,W): \n",
    "        xa = np.array([x])\n",
    "        V =  xa.dot(W)\n",
    "        V_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(V*V - V_square).sum()\n",
    "        return 1.0/(1.0 + np.exp(-phi))\n",
    "    \n",
    "    loss = dataRDD.map(lambda x:  (probability_value(x[0],w_bc.value), x[1])).map(lambda x: -(x[1] * np.log(x[0]) - (1-x[1])*np.log(1-x[0]))).mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmGradUpdate(dataRDD, w, alpha, regParam):\n",
    "    \"\"\"\n",
    "    Computes the gradient and updates the model\n",
    "    \"\"\"\n",
    "    \n",
    "    G = np.zeros(np.shape(w))\n",
    "    w_bc = sc.broadcast(w)\n",
    "    a_bc = sc.broadcast(alpha)\n",
    "    def row_grad(x, y, W, regParam):\n",
    "        xa = np.array([x])\n",
    "        x_matrix = xa.T.dot(xa)\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum()\n",
    "        expnyt = np.exp(y*phi)\n",
    "        result = (-y)/(1+expnyt)* (np.dot(xa, W))\n",
    "        \n",
    "        return regParam*W + result\n",
    "      \n",
    "    grad = dataRDD.map(lambda x: (1, row_grad(x[0], x[1], w_bc.value, regParam))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    Grad = grad.map(lambda x: (1,np.square(x[1]))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    model = w - (alpha * (grad.values().collect()[0]) )/ np.sqrt(Grad.values().collect()[0])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.02685949 -0.66814272]\n",
      " [-0.99975465 -0.66884647]\n",
      " [-1.0101119  -0.66776546]\n",
      " [-1.01337432 -0.63827766]\n",
      " [-1.00511465 -0.64297301]]\n",
      "[[0.04492211 0.1114009 ]\n",
      " [0.43213412 0.10134743]\n",
      " [0.28417333 0.11679029]\n",
      " [0.23756734 0.5380447 ]\n",
      " [0.35556262 0.47096824]]\n",
      "[[0.05492211 0.1214009 ]\n",
      " [0.44213412 0.11134743]\n",
      " [0.29417333 0.12679029]\n",
      " [0.24756734 0.5480447 ]\n",
      " [0.36556262 0.48096824]]\n"
     ]
    }
   ],
   "source": [
    "model = fmGradUpdate(trainRDD, wInit, 0.01, 0.01)\n",
    "print(wInit)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(trainRDD, testRDD, wInit, nSteps = 20, \n",
    "                    learningRate = 0.01, regParam = 0.01, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of OLS gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    for idx in range(nSteps): \n",
    "        \n",
    "        ############## YOUR CODE HERE #############\n",
    "        model = fmGradUpdate(trainRDD, model, learningRate, regParam)\n",
    "        training_loss = fmLoss(trainRDD, model) \n",
    "        test_loss = fmLoss(testRDD, model) \n",
    "        ############## (END) YOUR CODE #############\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[k for k in model]}\")\n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wInitialization(dataRDD, factor):\n",
    "    nrFeat = len(trainRDD.first()[0])\n",
    "    np.random.seed(int(time.time())) \n",
    "    w =  np.random.ranf((nrFeat, factor))\n",
    "    w = w / np.sqrt((w*w).sum())\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.39320266 -0.60822455]\n",
      " [-0.40194714 -0.62841343]\n",
      " [-0.398838   -0.60968196]\n",
      " [-0.41437717 -0.60547251]\n",
      " [-0.41587132 -0.60513439]]\n",
      "----------\n",
      "STEP: 1\n",
      "training loss: -1.1433005523978539\n",
      "test loss: -1.1433005523978539\n",
      "Model: [array([0.34636381, 0.42599445]), array([0.22144265, 0.13758193]), array([0.265859  , 0.40517433]), array([0.04387084, 0.46530939]), array([0.02252584, 0.47013962])]\n",
      "[[-0.36361933 -0.55725663]\n",
      " [-0.37236381 -0.5774455 ]\n",
      " [-0.36925466 -0.55871403]\n",
      " [-0.38479383 -0.55450458]\n",
      " [-0.38628798 -0.55416646]]\n",
      "----------\n",
      "STEP: 2\n",
      "training loss: -1.2029827884266122\n",
      "test loss: -1.2029827884266122\n",
      "Model: [array([0.35436381, 0.43399445]), array([0.22944265, 0.14558193]), array([0.273859  , 0.41317433]), array([0.05187084, 0.47330939]), array([0.03052584, 0.47813962])]\n",
      "[[-0.33468252 -0.50859341]\n",
      " [-0.343427   -0.52878228]\n",
      " [-0.34031786 -0.51005081]\n",
      " [-0.35585703 -0.50584136]\n",
      " [-0.35735118 -0.50550324]]\n",
      "----------\n",
      "STEP: 3\n",
      "training loss: -1.26428226594989\n",
      "test loss: -1.26428226594989\n",
      "Model: [array([0.36236381, 0.44199445]), array([0.23744265, 0.15358193]), array([0.281859  , 0.42117433]), array([0.05987084, 0.48130939]), array([0.03852584, 0.48613962])]\n",
      "[[-0.30663117 -0.46241623]\n",
      " [-0.31537565 -0.4826051 ]\n",
      " [-0.3122665  -0.46387364]\n",
      " [-0.32780567 -0.45966418]\n",
      " [-0.32929982 -0.45932607]]\n",
      "----------\n",
      "STEP: 4\n",
      "training loss: -1.327209255984777\n",
      "test loss: -1.327209255984777\n",
      "Model: [array([0.37036381, 0.44999445]), array([0.24544265, 0.16158193]), array([0.289859  , 0.42917433]), array([0.06787084, 0.48930939]), array([0.04652584, 0.49413962])]\n",
      "[[-0.27965497 -0.41884507]\n",
      " [-0.28839945 -0.43903394]\n",
      " [-0.28529031 -0.42030247]\n",
      " [-0.30082948 -0.41609302]\n",
      " [-0.30232363 -0.4157549 ]]\n",
      "----------\n",
      "STEP: 5\n",
      "training loss: -1.3917765051226552\n",
      "test loss: -1.3917765051226552\n",
      "Model: [array([0.37836381, 0.45799445]), array([0.25344265, 0.16958193]), array([0.297859  , 0.43717433]), array([0.07587084, 0.49730939]), array([0.05452584, 0.50213962])]\n",
      "[[-0.2538976  -0.37794487]\n",
      " [-0.26264209 -0.39813375]\n",
      " [-0.25953294 -0.37940228]\n",
      " [-0.27507211 -0.37519283]\n",
      " [-0.27656626 -0.37485471]]\n",
      "----------\n",
      "STEP: 6\n",
      "training loss: -1.457998654409224\n",
      "test loss: -1.457998654409224\n",
      "Model: [array([0.38636381, 0.46599445]), array([0.26144265, 0.17758193]), array([0.305859  , 0.44517433]), array([0.08387084, 0.50530939]), array([0.06252584, 0.51013962])]\n",
      "[[-0.2294608  -0.33973266]\n",
      " [-0.23820528 -0.35992154]\n",
      " [-0.23509614 -0.34119007]\n",
      " [-0.25063531 -0.33698061]\n",
      " [-0.25212946 -0.3366425 ]]\n",
      "----------\n",
      "STEP: 7\n",
      "training loss: -1.525891659982278\n",
      "test loss: -1.525891659982278\n",
      "Model: [array([0.39436381, 0.47399445]), array([0.26944265, 0.18558193]), array([0.313859  , 0.45317433]), array([0.09187084, 0.51330939]), array([0.07052584, 0.51813962])]\n",
      "[[-0.20640916 -0.30418483]\n",
      " [-0.21515364 -0.3243737 ]\n",
      " [-0.2120445  -0.30564224]\n",
      " [-0.22758367 -0.30143278]\n",
      " [-0.22907782 -0.30109467]]\n",
      "----------\n",
      "STEP: 8\n",
      "training loss: -1.5954722450108716\n",
      "test loss: -1.5954722450108716\n",
      "Model: [array([0.40236381, 0.48199445]), array([0.27744265, 0.19358193]), array([0.321859  , 0.46117433]), array([0.09987084, 0.52130939]), array([0.07852584, 0.52613962])]\n",
      "[[-0.18477521 -0.27124446]\n",
      " [-0.19351969 -0.29143334]\n",
      " [-0.19041055 -0.27270187]\n",
      " [-0.20594972 -0.26849241]\n",
      " [-0.20744387 -0.2681543 ]]\n",
      "----------\n",
      "STEP: 9\n",
      "training loss: -1.6667574038746593\n",
      "test loss: -1.6667574038746593\n",
      "Model: [array([0.41036381, 0.48999445]), array([0.28544265, 0.20158193]), array([0.329859  , 0.46917433]), array([0.10787084, 0.52930939]), array([0.08652584, 0.53413962])]\n",
      "[[-0.16456448 -0.24082821]\n",
      " [-0.17330896 -0.26101708]\n",
      " [-0.17019982 -0.24228562]\n",
      " [-0.18573899 -0.23807616]\n",
      " [-0.18723314 -0.23773805]]\n",
      "----------\n",
      "STEP: 10\n",
      "training loss: -1.739763971610225\n",
      "test loss: -1.739763971610225\n",
      "Model: [array([0.41836381, 0.49799445]), array([0.29344265, 0.20958193]), array([0.337859  , 0.47717433]), array([0.11587084, 0.53730939]), array([0.09452584, 0.54213962])]\n",
      "[[-0.1457604  -0.21283264]\n",
      " [-0.15450488 -0.23302151]\n",
      " [-0.15139574 -0.21429005]\n",
      " [-0.16693491 -0.21008059]\n",
      " [-0.16842906 -0.20974247]]\n",
      "----------\n",
      "STEP: 11\n",
      "training loss: -1.8145082649028834\n",
      "test loss: -1.8145082649028834\n",
      "Model: [array([0.42636381, 0.50599445]), array([0.30144265, 0.21758193]), array([0.345859  , 0.48517433]), array([0.12387084, 0.54530939]), array([0.10252584, 0.55013962])]\n",
      "[[-0.12832886 -0.18713985]\n",
      " [-0.13707334 -0.20732873]\n",
      " [-0.1339642  -0.18859726]\n",
      " [-0.14950337 -0.18438781]\n",
      " [-0.15099752 -0.18404969]]\n",
      "----------\n",
      "STEP: 12\n",
      "training loss: -1.8910057955263753\n",
      "test loss: -1.8910057955263753\n",
      "Model: [array([0.43436381, 0.51399445]), array([0.30944265, 0.22558193]), array([0.353859  , 0.49317433]), array([0.13187084, 0.55330939]), array([0.11052584, 0.55813962])]\n",
      "[[-0.11222228 -0.16362238]\n",
      " [-0.12096676 -0.18381125]\n",
      " [-0.11785761 -0.16507978]\n",
      " [-0.13339679 -0.16087033]\n",
      " [-0.13489094 -0.16053221]]\n",
      "----------\n",
      "STEP: 13\n",
      "training loss: -1.9692710531375615\n",
      "test loss: -1.9692710531375615\n",
      "Model: [array([0.44236381, 0.52199445]), array([0.31744265, 0.23358193]), array([0.361859  , 0.50117433]), array([0.13987084, 0.56130939]), array([0.11852584, 0.56613962])]\n",
      "[[-0.09738318 -0.14214726]\n",
      " [-0.10612766 -0.16233614]\n",
      " [-0.10301851 -0.14360467]\n",
      " [-0.11855769 -0.13939522]\n",
      " [-0.12005183 -0.1390571 ]]\n",
      "----------\n",
      "STEP: 14\n",
      "training loss: -2.0493173516089853\n",
      "test loss: -2.0493173516089853\n",
      "Model: [array([0.45036381, 0.52999445]), array([0.32544265, 0.24158193]), array([0.369859  , 0.50917433]), array([0.14787084, 0.56930939]), array([0.12652584, 0.57413962])]\n",
      "[[-0.08374727 -0.12257947]\n",
      " [-0.09249175 -0.14276835]\n",
      " [-0.08938261 -0.12403688]\n",
      " [-0.10492178 -0.11982743]\n",
      " [-0.10641593 -0.11948931]]\n",
      "----------\n",
      "STEP: 15\n",
      "training loss: -2.1311567314418234\n",
      "test loss: -2.1311567314418234\n",
      "Model: [array([0.45836381, 0.53799445]), array([0.33344265, 0.24958193]), array([0.377859  , 0.51717433]), array([0.15587084, 0.57730939]), array([0.13452584, 0.58213962])]\n",
      "[[-0.07124601 -0.10478456]\n",
      " [-0.07999049 -0.12497344]\n",
      " [-0.07688135 -0.10624197]\n",
      " [-0.09242052 -0.10203252]\n",
      " [-0.09391467 -0.1016944 ]]\n",
      "----------\n",
      "STEP: 16\n",
      "training loss: -2.2147999100351945\n",
      "test loss: -2.2147999100351945\n",
      "Model: [array([0.46636381, 0.54599445]), array([0.34144265, 0.25758193]), array([0.385859  , 0.52517433]), array([0.16387084, 0.58530939]), array([0.14252584, 0.59013962])]\n",
      "[[-0.05980866 -0.08863077]\n",
      " [-0.06855314 -0.10881965]\n",
      " [-0.065444   -0.09008818]\n",
      " [-0.08098317 -0.08587873]\n",
      " [-0.08247732 -0.08554061]]\n",
      "----------\n",
      "STEP: 17\n",
      "training loss: -2.3002562714873416\n",
      "test loss: -2.3002562714873416\n",
      "Model: [array([0.47436381, 0.55399445]), array([0.34944265, 0.26558193]), array([0.393859  , 0.53317433]), array([0.17187084, 0.59330939]), array([0.15052584, 0.59813962])]\n",
      "[[-0.049364   -0.07399061]\n",
      " [-0.05810848 -0.09417949]\n",
      " [-0.05499933 -0.07544802]\n",
      " [-0.0705385  -0.07123856]\n",
      " [-0.07203265 -0.07090045]]\n",
      "----------\n",
      "STEP: 18\n",
      "training loss: -2.3875338879692185\n",
      "test loss: -2.3875338879692185\n",
      "Model: [array([0.48236381, 0.56199445]), array([0.35744265, 0.27358193]), array([0.401859  , 0.54117433]), array([0.17987084, 0.60130939]), array([0.15852584, 0.60613962])]\n",
      "[[-0.03984155 -0.06074194]\n",
      " [-0.04858603 -0.08093082]\n",
      " [-0.04547688 -0.06219935]\n",
      " [-0.06101606 -0.0579899 ]\n",
      " [-0.06251021 -0.05765178]]\n",
      "----------\n",
      "STEP: 19\n",
      "training loss: -2.476639565395904\n",
      "test loss: -2.476639565395904\n",
      "Model: [array([0.49036381, 0.56999445]), array([0.36544265, 0.28158193]), array([0.409859  , 0.54917433]), array([0.18787084, 0.60930939]), array([0.16652584, 0.61413962])]\n",
      "[[-0.03117262 -0.04876879]\n",
      " [-0.0399171  -0.06895767]\n",
      " [-0.03680795 -0.0502262 ]\n",
      " [-0.05234713 -0.04601674]\n",
      " [-0.05384128 -0.04567863]]\n",
      "----------\n",
      "STEP: 20\n",
      "training loss: -2.567578906964429\n",
      "test loss: -2.567578906964429\n",
      "Model: [array([0.49836381, 0.57799445]), array([0.37344265, 0.28958193]), array([0.417859  , 0.55717433]), array([0.19587084, 0.61730939]), array([0.17452584, 0.62213962])]\n",
      "[[-0.02329097 -0.03796175]\n",
      " [-0.03203545 -0.05815062]\n",
      " [-0.0289263  -0.03941916]\n",
      " [-0.04446547 -0.0352097 ]\n",
      " [-0.04595962 -0.03487159]]\n",
      "----------\n",
      "STEP: 21\n",
      "training loss: -2.6603563890692876\n",
      "test loss: -2.6603563890692876\n",
      "Model: [array([0.50636381, 0.58599445]), array([0.38144265, 0.29758193]), array([0.425859  , 0.56517433]), array([0.20387084, 0.62530939]), array([0.18252584, 0.63013962])]\n",
      "[[-0.01613331 -0.02821824]\n",
      " [-0.02487779 -0.04840712]\n",
      " [-0.02176865 -0.02967565]\n",
      " [-0.03730782 -0.02546619]\n",
      " [-0.03880197 -0.02512808]]\n",
      "----------\n",
      "STEP: 22\n",
      "training loss: -2.7549754450064157\n",
      "test loss: -2.7549754450064157\n",
      "Model: [array([0.51436381, 0.59399445]), array([0.38944265, 0.30558193]), array([0.433859  , 0.57317433]), array([0.21187084, 0.63330939]), array([0.19052584, 0.63813962])]\n",
      "[[-0.00963963 -0.01944255]\n",
      " [-0.01838412 -0.03963143]\n",
      " [-0.01527497 -0.02089996]\n",
      " [-0.03081414 -0.01669051]\n",
      " [-0.03230829 -0.01635239]]\n",
      "----------\n",
      "STEP: 23\n",
      "training loss: -2.851438552794981\n",
      "test loss: -2.851438552794981\n",
      "Model: [array([0.52236381, 0.60199445]), array([0.39744265, 0.31358193]), array([0.441859  , 0.58117433]), array([0.21987084, 0.64130939]), array([0.19852584, 0.64613962])]\n",
      "[[-0.00375338 -0.01154576]\n",
      " [-0.01249786 -0.03173464]\n",
      " [-0.00938872 -0.01300317]\n",
      " [-0.02492789 -0.00879371]\n",
      " [-0.02642204 -0.0084556 ]]\n",
      "----------\n",
      "STEP: 24\n",
      "training loss: -2.949747324185432\n",
      "test loss: -2.949747324185432\n",
      "Model: [array([0.53036381, 0.60999445]), array([0.40544265, 0.32158193]), array([0.449859  , 0.58917433]), array([0.22787084, 0.64930939]), array([0.20652584, 0.65413962])]\n",
      "[[ 0.00157849 -0.00444552]\n",
      " [-0.00716599 -0.02463439]\n",
      " [-0.00405685 -0.00590293]\n",
      " [-0.01959602 -0.00169347]\n",
      " [-0.02109017 -0.00135536]]\n",
      "----------\n",
      "STEP: 25\n",
      "training loss: -3.0369430732644247\n",
      "test loss: -3.0369430732644247\n",
      "Model: [array([0.52236381, 0.61799445]), array([0.41344265, 0.32958193]), array([0.457859  , 0.59717433]), array([0.23587084, 0.65730939]), array([0.21452584, 0.66213962])]\n",
      "[[ 0.00470169  0.0008522 ]\n",
      " [-0.00292279 -0.01933668]\n",
      " [ 0.00018635 -0.00060521]\n",
      " [-0.01535282  0.00360425]\n",
      " [-0.01684697  0.00394236]]\n",
      "----------\n",
      "STEP: 26\n",
      "training loss: -3.054319140259408\n",
      "test loss: -3.054319140259408\n",
      "Model: [array([0.51436381, 0.60999445]), array([0.42144265, 0.33758193]), array([0.449859  , 0.60517433]), array([0.24387084, 0.64930939]), array([0.22252584, 0.65413962])]\n",
      "[[ 4.59247168e-03  5.96936849e-04]\n",
      " [-1.91200934e-03 -1.84719397e-02]\n",
      " [ 7.71353318e-05  2.59528069e-04]\n",
      " [-1.43420362e-02  3.34898263e-03]\n",
      " [-1.58361861e-02  3.68709870e-03]]\n",
      "----------\n",
      "STEP: 27\n",
      "training loss: -3.039626010562413\n",
      "test loss: -3.039626010562413\n",
      "Model: [array([0.50636381, 0.60199445]), array([0.42944265, 0.34558193]), array([0.441859  , 0.59717433]), array([0.25187084, 0.64130939]), array([0.23052584, 0.64613962])]\n",
      "[[ 0.00276673 -0.00146208]\n",
      " [-0.00261775 -0.01941096]\n",
      " [-0.00174861 -0.00179949]\n",
      " [-0.01504778  0.00128996]\n",
      " [-0.01654193  0.00162808]]\n",
      "----------\n",
      "STEP: 28\n",
      "training loss: -3.098094931265972\n",
      "test loss: -3.098094931265972\n",
      "Model: [array([0.49836381, 0.60999445]), array([0.43744265, 0.35358193]), array([0.449859  , 0.60517433]), array([0.25987084, 0.63330939]), array([0.23852584, 0.63813962])]\n",
      "[[ 0.00452717  0.00228884]\n",
      " [ 0.00026268 -0.01566004]\n",
      " [ 0.00113183  0.00195143]\n",
      " [-0.01216734  0.00392089]\n",
      " [-0.01366149  0.004259  ]]\n",
      "----------\n",
      "STEP: 29\n",
      "training loss: -3.0594315300578754\n",
      "test loss: -3.0594315300578754\n",
      "Model: [array([0.49036381, 0.60199445]), array([0.42944265, 0.36158193]), array([0.441859  , 0.59717433]), array([0.26787084, 0.62530939]), array([0.24652584, 0.63013962])]\n",
      "[[ 0.00203759 -0.00097895]\n",
      " [-0.00222689 -0.01780783]\n",
      " [-0.00135775 -0.00131636]\n",
      " [-0.01353692  0.00065309]\n",
      " [-0.01503107  0.00099121]]\n",
      "----------\n",
      "STEP: 30\n",
      "training loss: -3.117693241032826\n",
      "test loss: -3.117693241032826\n",
      "Model: [array([0.48236381, 0.60999445]), array([0.43744265, 0.36958193]), array([0.449859  , 0.60517433]), array([0.27587084, 0.61730939]), array([0.25452584, 0.62213962])]\n",
      "[[ 0.00374653  0.00270799]\n",
      " [ 0.00060205 -0.01412088]\n",
      " [ 0.0014712   0.00237059]\n",
      " [-0.01070798  0.00322004]\n",
      " [-0.01220213  0.00355816]]\n",
      "----------\n",
      "STEP: 31\n",
      "training loss: -3.078385452765287\n",
      "test loss: -3.078385452765287\n",
      "Model: [array([0.47436381, 0.60199445]), array([0.42944265, 0.37758193]), array([0.441859  , 0.59717433]), array([0.28387084, 0.60930939]), array([0.26252584, 0.61413962])]\n",
      "[[ 1.26232947e-03 -5.52861837e-04]\n",
      " [-1.88215154e-03 -1.62617384e-02]\n",
      " [-1.01300687e-03 -8.90270616e-04]\n",
      " [-1.20721784e-02 -4.08160590e-05]\n",
      " [-1.35663283e-02  2.97300016e-04]]\n",
      "----------\n",
      "STEP: 32\n",
      "training loss: -3.1577392883665434\n",
      "test loss: -3.1577392883665434\n",
      "Model: [array([0.46636381, 0.60999445]), array([0.43744265, 0.38558193]), array([0.449859  , 0.60517433]), array([0.29187084, 0.61730939]), array([0.27052584, 0.60613962])]\n",
      "[[ 0.0039736   0.00425791]\n",
      " [ 0.00194911 -0.01145097]\n",
      " [ 0.00281826  0.0039205 ]\n",
      " [-0.00824091  0.00476996]\n",
      " [-0.00973506  0.00398807]]\n",
      "----------\n",
      "STEP: 33\n",
      "training loss: -3.117660530280183\n",
      "test loss: -3.117660530280183\n",
      "Model: [array([0.45836381, 0.60199445]), array([0.42944265, 0.39358193]), array([0.441859  , 0.59717433]), array([0.29987084, 0.60930939]), array([0.27852584, 0.59813962])]\n",
      "[[ 0.00155368  0.00107086]\n",
      " [-0.0004708  -0.01351802]\n",
      " [ 0.00039835  0.00073345]\n",
      " [-0.00954082  0.0015829 ]\n",
      " [-0.01103497  0.00080102]]\n",
      "----------\n",
      "STEP: 34\n",
      "training loss: -3.1011358707553924\n",
      "test loss: -3.1011358707553924\n",
      "Model: [array([0.45036381, 0.59399445]), array([0.43744265, 0.40158193]), array([0.433859  , 0.58917433]), array([0.30787084, 0.60130939]), array([0.28652584, 0.59013962])]\n",
      "[[-0.00031918 -0.00105873]\n",
      " [-0.00122366 -0.01452761]\n",
      " [-0.00147452 -0.00139614]\n",
      " [-0.01029369 -0.00054668]\n",
      " [-0.01178784 -0.00132857]]\n",
      "----------\n",
      "STEP: 35\n",
      "training loss: -3.203460778755507\n",
      "test loss: -3.203460778755507\n",
      "Model: [array([0.45836381, 0.60199445]), array([0.44544265, 0.40958193]), array([0.441859  , 0.59717433]), array([0.31587084, 0.60930939]), array([0.29452584, 0.59813962])]\n",
      "[[ 0.00402926  0.00473685]\n",
      " [ 0.00312478 -0.00873203]\n",
      " [ 0.00287392  0.00439944]\n",
      " [-0.00594525  0.0052489 ]\n",
      " [-0.0074394   0.00446701]]\n",
      "----------\n",
      "STEP: 36\n",
      "training loss: -3.162269763069183\n",
      "test loss: -3.162269763069183\n",
      "Model: [array([0.45036381, 0.59399445]), array([0.43744265, 0.41758193]), array([0.433859  , 0.58917433]), array([0.32387084, 0.60130939]), array([0.30252584, 0.59013962])]\n",
      "[[ 0.00161686  0.00158082]\n",
      " [ 0.00071237 -0.01076806]\n",
      " [ 0.00046152  0.00124341]\n",
      " [-0.00723765  0.00209286]\n",
      " [-0.0087318   0.00131098]]\n",
      "----------\n",
      "STEP: 37\n",
      "training loss: -3.1210068202006944\n",
      "test loss: -3.1210068202006944\n",
      "Model: [array([0.44236381, 0.58599445]), array([0.42944265, 0.42558193]), array([0.425859  , 0.58117433]), array([0.33187084, 0.59330939]), array([0.31052584, 0.58213962])]\n",
      "[[-0.00090922 -0.0017474 ]\n",
      " [-0.0018137  -0.01297628]\n",
      " [-0.00206456 -0.00208481]\n",
      " [-0.00864373 -0.00123535]\n",
      " [-0.01013788 -0.00201724]]\n",
      "----------\n",
      "STEP: 38\n",
      "training loss: -3.223509038321484\n",
      "test loss: -3.223509038321484\n",
      "Model: [array([0.45036381, 0.59399445]), array([0.43744265, 0.43358193]), array([0.433859  , 0.58917433]), array([0.33987084, 0.60130939]), array([0.31852584, 0.59013962])]\n",
      "[[ 0.00342686  0.00404872]\n",
      " [ 0.00252238 -0.00718016]\n",
      " [ 0.00227152  0.00371131]\n",
      " [-0.00430765  0.00456076]\n",
      " [-0.0058018   0.00377888]]\n",
      "----------\n",
      "STEP: 39\n",
      "training loss: -3.1813380681410246\n",
      "test loss: -3.1813380681410246\n",
      "Model: [array([0.44236381, 0.58599445]), array([0.42944265, 0.44158193]), array([0.425859  , 0.58117433]), array([0.34787084, 0.59330939]), array([0.32652584, 0.58213962])]\n",
      "[[ 9.88603174e-04  8.51657054e-04]\n",
      " [ 8.41221622e-05 -9.25721954e-03]\n",
      " [-1.66733169e-04  5.14248275e-04]\n",
      " [-5.62590471e-03  1.36370283e-03]\n",
      " [-7.12005462e-03  5.81818907e-04]]\n",
      "----------\n",
      "STEP: 40\n",
      "training loss: -3.1640469395445265\n",
      "test loss: -3.1640469395445265\n",
      "Model: [array([0.43436381, 0.57799445]), array([0.42144265, 0.44958193]), array([0.433859  , 0.57317433]), array([0.35587084, 0.58530939]), array([0.33452584, 0.57413962])]\n",
      "[[-0.00077467 -0.00081623]\n",
      " [-0.00167915 -0.0098051 ]\n",
      " [-0.00081001 -0.00115364]\n",
      " [-0.00626918 -0.00030418]\n",
      " [-0.00776333 -0.00108607]]\n",
      "----------\n",
      "STEP: 41\n",
      "training loss: -3.267264736820003\n",
      "test loss: -3.267264736820003\n",
      "Model: [array([0.44236381, 0.58599445]), array([0.42944265, 0.45758193]), array([0.441859  , 0.58117433]), array([0.36387084, 0.59330939]), array([0.34252584, 0.58213962])]\n",
      "[[ 0.00348874  0.00480491]\n",
      " [ 0.00258426 -0.00418397]\n",
      " [ 0.0034534   0.0044675 ]\n",
      " [-0.00200577  0.00531695]\n",
      " [-0.00349992  0.00453507]]\n",
      "----------\n",
      "STEP: 42\n",
      "training loss: -3.223946234131007\n",
      "test loss: -3.223946234131007\n",
      "Model: [array([0.43436381, 0.57799445]), array([0.42144265, 0.46558193]), array([0.433859  , 0.57317433]), array([0.37187084, 0.58530939]), array([0.35052584, 0.57413962])]\n",
      "[[ 0.00105853  0.00166228]\n",
      " [ 0.00015405 -0.00620659]\n",
      " [ 0.00102319  0.00132488]\n",
      " [-0.00331598  0.00217433]\n",
      " [-0.00481013  0.00139245]]\n",
      "----------\n",
      "STEP: 43\n",
      "training loss: -3.18055733288984\n",
      "test loss: -3.18055733288984\n",
      "Model: [array([0.42636381, 0.56999445]), array([0.41344265, 0.47358193]), array([0.425859  , 0.56517433]), array([0.37987084, 0.57730939]), array([0.35852584, 0.56613962])]\n",
      "[[-0.00149069 -0.00165694]\n",
      " [-0.00239517 -0.00840582]\n",
      " [-0.00152602 -0.00199435]\n",
      " [-0.00474519 -0.0011449 ]\n",
      " [-0.00623934 -0.00192678]]\n",
      "----------\n",
      "STEP: 44\n",
      "training loss: -3.2839485075149395\n",
      "test loss: -3.2839485075149395\n",
      "Model: [array([0.43436381, 0.57799445]), array([0.42144265, 0.48158193]), array([0.433859  , 0.57317433]), array([0.38787084, 0.58530939]), array([0.36652584, 0.57413962])]\n",
      "[[ 0.00277746  0.00398526]\n",
      " [ 0.00187298 -0.00276361]\n",
      " [ 0.00274213  0.00364786]\n",
      " [-0.00047705  0.00449731]\n",
      " [-0.0019712   0.00371543]]\n",
      "----------\n",
      "STEP: 45\n",
      "training loss: -3.239650776662398\n",
      "test loss: -3.239650776662398\n",
      "Model: [array([0.42636381, 0.56999445]), array([0.41344265, 0.48958193]), array([0.425859  , 0.56517433]), array([0.39587084, 0.57730939]), array([0.37452584, 0.56613962])]\n",
      "[[ 0.00031426  0.00079279]\n",
      " [-0.00059022 -0.00483608]\n",
      " [ 0.00027893  0.00045539]\n",
      " [-0.00182024  0.00130484]\n",
      " [-0.00331439  0.00052296]]\n",
      "----------\n",
      "STEP: 46\n",
      "training loss: -3.220315312161017\n",
      "test loss: -3.220315312161017\n",
      "Model: [array([0.41836381, 0.56199445]), array([0.42144265, 0.49758193]), array([0.417859  , 0.55717433]), array([0.40387084, 0.56930939]), array([0.38252584, 0.55813962])]\n",
      "[[-0.00163007 -0.00140514]\n",
      " [-0.00141455 -0.00591402]\n",
      " [-0.00166541 -0.00174255]\n",
      " [-0.00264458 -0.0008931 ]\n",
      " [-0.00413873 -0.00167498]]\n",
      "----------\n",
      "STEP: 47\n",
      "training loss: -3.3243825709033894\n",
      "test loss: -3.3243825709033894\n",
      "Model: [array([0.42636381, 0.56999445]), array([0.42944265, 0.50558193]), array([0.425859  , 0.56517433]), array([0.41187084, 0.57730939]), array([0.39052584, 0.56613962])]\n",
      "[[ 2.58907111e-03  4.13708924e-03]\n",
      " [ 2.80459010e-03 -3.71787357e-04]\n",
      " [ 2.55373476e-03  3.79968046e-03]\n",
      " [ 1.57456323e-03  4.64913502e-03]\n",
      " [ 8.04133183e-05  3.86725109e-03]]\n",
      "----------\n",
      "STEP: 48\n",
      "training loss: -3.2546019986328956\n",
      "test loss: -3.2546019986328956\n",
      "Model: [array([0.41836381, 0.56199445]), array([0.42144265, 0.51358193]), array([0.417859  , 0.55717433]), array([0.40387084, 0.56930939]), array([0.38252584, 0.55813962])]\n",
      "[[-0.00046275 -0.00010518]\n",
      " [-0.00024723 -0.00349405]\n",
      " [-0.00049809 -0.00044259]\n",
      " [-0.00147726  0.00040687]\n",
      " [-0.00297141 -0.00037501]]\n",
      "----------\n",
      "STEP: 49\n",
      "training loss: -3.337088052151496\n",
      "test loss: -3.337088052151496\n",
      "Model: [array([0.42636381, 0.56999445]), array([0.42944265, 0.52158193]), array([0.425859  , 0.56517433]), array([0.41187084, 0.56130939]), array([0.39052584, 0.56613962])]\n",
      "[[0.00266013 0.00421979]\n",
      " [0.00287565 0.00083091]\n",
      " [0.00262479 0.00388238]\n",
      " [0.00164562 0.00361183]\n",
      " [0.00015147 0.00394995]]\n",
      "----------\n",
      "STEP: 50\n",
      "training loss: -3.232873721610322\n",
      "test loss: -3.232873721610322\n",
      "Model: [array([0.41836381, 0.56199445]), array([0.42144265, 0.51358193]), array([0.417859  , 0.55717433]), array([0.40387084, 0.55330939]), array([0.38252584, 0.55813962])]\n",
      "\n",
      "... trained 50 iterations in 33.972219467163086 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "wInit = wInitialization(trainRDD, 2)\n",
    "logerr_train, logerr_test, models = GradientDescent(trainRDD, trainRDD, wInit, nSteps = 50,\n",
    "                                                    learningRate = 0.008, regParam = 0.01, verbose = True)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06795161 0.03869574]\n",
      " [0.0912008  0.38786739]\n",
      " [0.51665977 0.33872864]\n",
      " [0.47325027 0.01188713]\n",
      " [0.24568004 0.41107944]]\n"
     ]
    }
   ],
   "source": [
    "print(wInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
