{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n",
    "## Supplemental Notebook - Logistic Regression Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team 24   \n",
    "### Vivian Lu, Siddhartha Jakkamreddy, Venky Nagapudi, Luca Garre   \n",
    "### Summer 2019, sections 4 and 5   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Production Pipeline\n",
    "### Logistic Regression Sample run and Full dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Important articles to look at\n",
    "\n",
    "https://towardsdatascience.com/apache-spark-mllib-tutorial-7aba8a1dce6e\n",
    "\n",
    "https://stackoverflow.com/questions/32982425/encode-and-assemble-multiple-features-in-pyspark\n",
    "\n",
    "https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42\n",
    "\n",
    "https://medium.com/future-vision/spark-udfs-we-can-use-them-but-should-we-use-them-2c5a561fde6d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Basic pipeline based production model for Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in  training data and convert to dataframe\n",
    "start = time.time()\n",
    "train_sample = sc.textFile('data/sample_train.txt')\\\n",
    "                 .map(lambda x: x.split('\\t'))\\\n",
    "                 .toDF().repartition(16).write.parquet(\"sample_train\")\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+\n",
      "| _1| _2| _3| _4| _5|  _6| _7| _8| _9|_10|_11|_12|_13|_14|     _15|     _16|     _17|     _18|     _19|     _20|     _21|     _22|     _23|     _24|     _25|     _26|     _27|     _28|     _29|     _30|     _31|     _32|     _33|     _34|     _35|_36|     _37|     _38|     _39|     _40|\n",
      "+---+---+---+---+---+----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+\n",
      "|  0|  1|  1|  5|  0|1382|  4| 15|  2|181|  1|  2|   |  2|68fd1e64|80e26c9b|fb936136|7b4723c4|25c83c98|7e0ccccf|de7995b8|1f89b562|a73ee510|a8cd5504|b2cb9c98|37c9c164|2824a5f6|1adce6ef|8ba8b39a|891b62e7|e5ba7672|f54016b9|21ddcdc9|b1252a9d|07b5194c|   |3a171ecb|c5c50484|e8b83407|9727dd16|\n",
      "|  0|  2|  0| 44|  1| 102|  8|  2|  2|  4|  1|  1|   |  4|68fd1e64|f0cf0024|6f67f7e5|41274cd7|25c83c98|fe6b92e5|922afcc0|0b153874|a73ee510|2b53e5fb|4f1b46f3|623049e6|d7020589|b28479f6|e6c5b5cd|c92f3b61|07c540c4|b04e4670|21ddcdc9|5840adea|60f6221e|   |3a171ecb|43f13e8b|e8b83407|731c3655|\n",
      "+---+---+---+---+---+----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read in dataframe - this is the tiny dataset with 100K lines\n",
    "#Read in dataframe (need to do this in parquet)\n",
    "start = time.time()\n",
    "df = spark.read.parquet(\"sample_train\")\n",
    "\n",
    "df.show(2)\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and numerical columns\n",
    "categoricalCols = []\n",
    "numericalCols = []\n",
    "numericalColsImputed = []\n",
    "numericalColsLog = []\n",
    "for c in range(2,41):\n",
    "    col = \"_\"+str(c)\n",
    "    colImp = str(c)+\"_imp\"\n",
    "    colLog = str(c)+\"_log\"\n",
    "    if (c < 15):\n",
    "        numericalCols.append(col)\n",
    "        numericalColsImputed.append(colImp)\n",
    "        numericalColsLog.append(colLog)\n",
    "    else:\n",
    "        categoricalCols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+---+---+---+---+---+---+---+---+\n",
      "| _2| _3| _4| _5|   _6| _7| _8| _9|_10|_11|_12|_13|_14|\n",
      "+---+---+---+---+-----+---+---+---+---+---+---+---+---+\n",
      "|  1|  1|  5|  0| 1382|  4| 15|  2|181|  1|  2|   |  2|\n",
      "|  2|  0| 44|  1|  102|  8|  2|  2|  4|  1|  1|   |  4|\n",
      "|  2|  0|  1| 14|  767| 89|  4|  2|245|  1|  3|  3| 45|\n",
      "|   |893|   |   | 4392|   |  0|  0|  0|   |  0|   |   |\n",
      "|  3| -1|   |  0|    2|  0|  3|  0|  0|  1|  1|   |  0|\n",
      "|   | -1|   |   |12824|   |  0|  0|  6|   |  0|   |   |\n",
      "|   |  1|  2|   | 3168|   |  0|  1|  2|   |  0|   |   |\n",
      "|  1|  4|  2|  0|    0|  0|  1|  0|  0|  1|  1|   |  0|\n",
      "|   | 44|  4|  8|19010|249| 28| 31|141|   |  1|   |  8|\n",
      "|   | 35|   |  1|33737| 21|  1|  2|  3|   |  1|   |  1|\n",
      "+---+---+---+---+-----+---+---+---+---+---+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(numericalCols).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [] # stages in our Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "for col in numericalCols:\n",
    "    df = df.withColumn(col, df[col].cast(\"double\"))\n",
    "    df= df.withColumn(col, when(df[col]<0, None).otherwise(df[col]))\n",
    "#Also do this for column 0\n",
    "df = df.withColumn(\"_1\", df[\"_1\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+-------+-----+----+----+-----+----+---+----+----+\n",
      "|  _2|   _3|  _4|  _5|     _6|   _7|  _8|  _9|  _10| _11|_12| _13| _14|\n",
      "+----+-----+----+----+-------+-----+----+----+-----+----+---+----+----+\n",
      "| 1.0|  1.0| 5.0| 0.0| 1382.0|  4.0|15.0| 2.0|181.0| 1.0|2.0|null| 2.0|\n",
      "| 2.0|  0.0|44.0| 1.0|  102.0|  8.0| 2.0| 2.0|  4.0| 1.0|1.0|null| 4.0|\n",
      "| 2.0|  0.0| 1.0|14.0|  767.0| 89.0| 4.0| 2.0|245.0| 1.0|3.0| 3.0|45.0|\n",
      "|null|893.0|null|null| 4392.0| null| 0.0| 0.0|  0.0|null|0.0|null|null|\n",
      "| 3.0| null|null| 0.0|    2.0|  0.0| 3.0| 0.0|  0.0| 1.0|1.0|null| 0.0|\n",
      "|null| null|null|null|12824.0| null| 0.0| 0.0|  6.0|null|0.0|null|null|\n",
      "|null|  1.0| 2.0|null| 3168.0| null| 0.0| 1.0|  2.0|null|0.0|null|null|\n",
      "| 1.0|  4.0| 2.0| 0.0|    0.0|  0.0| 1.0| 0.0|  0.0| 1.0|1.0|null| 0.0|\n",
      "|null| 44.0| 4.0| 8.0|19010.0|249.0|28.0|31.0|141.0|null|1.0|null| 8.0|\n",
      "|null| 35.0|null| 1.0|33737.0| 21.0| 1.0| 2.0|  3.0|null|1.0|null| 1.0|\n",
      "+----+-----+----+----+-------+-----+----+----+-----+----+---+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confirming that numerical columns with -1s etc are changed to Null\n",
    "df.select(numericalCols).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline Stages- 6 of them\n",
    "#Stage 1 - First transform numerical columns with imputer\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(inputCols=numericalCols, outputCols=numericalColsImputed)\n",
    "stages += [imputer]\n",
    "#imputer_model = imputer.fit(df)\n",
    "\n",
    "#Stage 2 - Compute log transforms\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "sqlTrans = SQLTransformer(statement=\"SELECT *, \\\n",
    "                          log(2_imp+1) AS 2_log, \\\n",
    "                          log(3_imp+1) AS 3_log, \\\n",
    "                          log(4_imp+1) AS 4_log, \\\n",
    "                          log(5_imp+1) AS 5_log, \\\n",
    "                          log(6_imp+1) AS 6_log, \\\n",
    "                          log(7_imp+1) AS 7_log, \\\n",
    "                          log(8_imp+1) AS 8_log, \\\n",
    "                          log(9_imp+1) AS 9_log, \\\n",
    "                          log(10_imp+1) AS 10_log, \\\n",
    "                          log(11_imp+1) AS 11_log, \\\n",
    "                          log(12_imp+1) AS 12_log, \\\n",
    "                          log(13_imp+1) AS 13_log, \\\n",
    "                          log(14_imp+1) AS 14_log \\\n",
    "                          FROM __THIS__\")\n",
    "stages += [sqlTrans]\n",
    "\n",
    "#Stage 3 - Assemble the column vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "numericalAssembler = VectorAssembler(inputCols=numericalColsLog, outputCol=\"log_numerical_feature_vec\")\n",
    "stages += [numericalAssembler]\n",
    "\n",
    "#Stage 4 - Now normalize these numerical features\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"log_numerical_feature_vec\", outputCol=\"scaled_features\")\n",
    "stages += [scaler]\n",
    "\n",
    "#Stage 5 - Next transform Categorical Features with FeatureHasher\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "hasher = FeatureHasher(numFeatures=256, inputCols=categoricalCols,outputCol=\"categorical_features\")\n",
    "stages += [hasher]\n",
    "\n",
    "#Stage 6 - Now create vector with numerical and categorical features\n",
    "finalAssembler = VectorAssembler(inputCols=[\"scaled_features\", \"categorical_features\"], outputCol=\"features\")\n",
    "stages += [finalAssembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----+---+------+---+----+---+-----+---+---+----+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+\n",
      "| _1| _2| _3|  _4| _5|    _6| _7|  _8| _9|  _10|_11|_12| _13|_14|     _15|     _16|     _17|     _18|     _19|     _20|     _21|     _22|     _23|     _24|     _25|     _26|     _27|     _28|     _29|     _30|     _31|     _32|     _33|     _34|     _35|_36|     _37|     _38|     _39|     _40|\n",
      "+---+---+---+----+---+------+---+----+---+-----+---+---+----+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+\n",
      "|0.0|1.0|1.0| 5.0|0.0|1382.0|4.0|15.0|2.0|181.0|1.0|2.0|null|2.0|68fd1e64|80e26c9b|fb936136|7b4723c4|25c83c98|7e0ccccf|de7995b8|1f89b562|a73ee510|a8cd5504|b2cb9c98|37c9c164|2824a5f6|1adce6ef|8ba8b39a|891b62e7|e5ba7672|f54016b9|21ddcdc9|b1252a9d|07b5194c|   |3a171ecb|c5c50484|e8b83407|9727dd16|\n",
      "|0.0|2.0|0.0|44.0|1.0| 102.0|8.0| 2.0|2.0|  4.0|1.0|1.0|null|4.0|68fd1e64|f0cf0024|6f67f7e5|41274cd7|25c83c98|fe6b92e5|922afcc0|0b153874|a73ee510|2b53e5fb|4f1b46f3|623049e6|d7020589|b28479f6|e6c5b5cd|c92f3b61|07c540c4|b04e4670|21ddcdc9|5840adea|60f6221e|   |3a171ecb|43f13e8b|e8b83407|731c3655|\n",
      "+---+---+---+----+---+------+---+----+---+-----+---+---+----+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dataframe before pipeline\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally create a pipeline and verify\n",
    "from pyspark.ml import Pipeline\n",
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(df)\n",
    "preppedDataDF = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_1 |features                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0|(269,[0,1,2,4,5,6,7,8,9,10,11,12,17,18,21,27,37,49,59,60,72,78,85,105,121,141,155,170,178,182,194,212,230,233,236,242,249],[0.8424647817521497,0.32042365542048934,1.3726393141434567,2.4674304811383085,0.959212884097188,2.0272546381730643,0.9429602595014984,3.1254006250382753,2.3123625476234517,1.493294454490586,2.0040756904855566,1.1073404480904998,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|0.0|(269,[0,2,3,4,5,6,7,8,9,10,11,12,14,18,49,53,70,82,85,93,105,116,119,121,141,143,148,149,150,168,178,189,202,219,244],[1.3352750872553905,2.9162254637812075,0.7815767963893725,1.5812852931503845,1.3095293130310162,0.8032806452293358,0.9429602595014984,0.9665895068838533,2.3123625476234517,0.9421639021813691,2.0040756904855566,1.6222244348725063,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0])            |\n",
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preppedDataDF.select('_1','features').show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: double, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep relevant columns\n",
    "selectedcols = [\"_1\", \"features\"]\n",
    "dataset = preppedDataDF.select(selectedcols)\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70022\n",
      "29978\n"
     ]
    }
   ],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"_1\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7385414763574385"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"_1\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Logistic Regression with 100% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in  training data and convert to dataframe\n",
    "start = time.time()\n",
    "train_sample = sc.textFile('data/train.txt')\\\n",
    "                 .map(lambda x: x.split('\\t'))\\\n",
    "                 .toDF().repartition(16).write.parquet(\"train\")\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+-----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+\n",
      "| _1| _2| _3| _4| _5|   _6| _7| _8| _9|_10|_11|_12|_13|_14|     _15|     _16|     _17|     _18|     _19|     _20|     _21|     _22|     _23|     _24|     _25|     _26|     _27|     _28|     _29|     _30|     _31|     _32|     _33|     _34|     _35|_36|     _37|     _38|     _39|     _40|\n",
      "+---+---+---+---+---+-----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+\n",
      "|  0|   |  3|  3|  3|11826|   |  0|  5|614|   |  0|   |  3|ae82ea21|f8c8e8f8|8117bcef|f18a04d8|4cf72387|fbad5c96|bcb6cb0a|1f89b562|a73ee510|0466803a|7467deef|77c9170c|58e74fa3|b28479f6|b15b8172|1caabec2|1e88c74f|d2f0bce2|21ddcdc9|5840adea|c939ff8f|   |32c7478e|22ff028a|fd2fe0bd|04442161|\n",
      "|  1|   |  7|  5|  2| 7981|  6|  1|  2| 14|   |  1|   |  2|68fd1e64|38d50e09|622d2ce8|51c64c6d|25c83c98|6f6d9be8|1d94dd40|0b153874|a73ee510|efea433b|ccfdca2f|e9521d94|d76cea6e|b28479f6|42b3012c|ab8b968d|d4bb7bd8|582152eb|21ddcdc9|5840adea|ee4fa92e|   |32c7478e|d61a7d0a|001f3601|b29c74dc|\n",
      "+---+---+---+---+---+-----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+--------+--------+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "... Completed in 3.2446086406707764 seconds\n"
     ]
    }
   ],
   "source": [
    "#Read the full dataset into a dataframe\n",
    "start = time.time()\n",
    "df = spark.read.parquet(\"train\")\n",
    "\n",
    "df.show(2)\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "for col in numericalCols:\n",
    "    df = df.withColumn(col, df[col].cast(\"double\"))\n",
    "    df= df.withColumn(col, when(df[col]<0, None).otherwise(df[col]))\n",
    "#Also do this for column 0\n",
    "df = df.withColumn(\"_1\", df[\"_1\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Completed in 1984.4682896137238 seconds\n"
     ]
    }
   ],
   "source": [
    "#Finally create a pipeline and verify\n",
    "start = time.time()\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(df)\n",
    "preppedDataDF = pipelineModel.transform(df)\n",
    "\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: double, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Completed in 0.06742119789123535 seconds\n"
     ]
    }
   ],
   "source": [
    "# Keep relevant columns\n",
    "start = time.time()\n",
    "selectedcols = [\"_1\", \"features\"]\n",
    "dataset = preppedDataDF.select(selectedcols)\n",
    "display(dataset)\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Completed in 0.02073836326599121 seconds\n"
     ]
    }
   ],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "start = time.time()\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Completed in 3428.8369302749634 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create initial LogisticRegression model\n",
    "#Run logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "start = time.time()\n",
    "lr = LogisticRegression(labelCol=\"_1\", featuresCol=\"features\", maxIter=20)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Completed in 0.05697965621948242 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "predictions = lrModel.transform(testData)\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7250146213638494\n",
      "\n",
      "... Completed in 2776.656357526779 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "start = time.time()\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"_1\")\n",
    "print(evaluator.evaluate(predictions))\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for train set is 0.7250146213638572\n"
     ]
    }
   ],
   "source": [
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "predictions = lrModel.transform(datasetTest)\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "start = time.time()\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"_1\")\n",
    "print(evaluator.evaluate(predictions))\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_train)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# View best model's predictions and probabilities of each prediction class\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")\n",
    "display(selected)\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
