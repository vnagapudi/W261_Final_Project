{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team 24   \n",
    "Vivian Lu, Siddhartha Jakkamreddy, Venky Nagapudi, Luca Garre   \n",
    "Summer 2019, sections 4 and 5   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Introduction__\n",
    "Online ad is a multibillion dollar industry fueled by large investments and ever increasing performance goals. Targeted advertisement based on users' browsing industry and demographic, ad features such as overall appearance, employed colors and text, and website features such as ad's relative placement in the webpage, sizes, etc., is receiving more and more interest due to its potential for revenue generation. In this context, machine learning is proving resourceful in the understanding of the features that mostly affect users' Click-Through Rates (CTR) and, based on this understanding, in informing the design of ads that maximize performance metrics such as click and convertion rates. Further, machine learning solutions can easily be deployed in a data pipeline enviroment in order to select and offer, on a user-specific basis, the ad which expectedly maximizes the user's interest. \n",
    "\n",
    "...\n",
    "\n",
    "## __Goal of the analysis__\n",
    "The purpose of the present analysis is to estimate whether a given ad will be clicked based on a set of features describing the ad. \n",
    "\n",
    "...\n",
    "\n",
    "## __Description of the dataset__\n",
    "The dataset is provided by __[put_reference_to_CriteoLabs]__ and is composed of three files, a `readme.txt`, a `train.txt` and a `test.txt` file, respectively. The readme file contains a brief description of the data. The `train.txt` and `test.txt` files contain the train and test data. Both files are formatted as tab separated value tables, and amount to 45840617 and 6042135 rows for the train and test data, respectively. Following the description of the data, each row represents an ad and contains the following fields (see commands below, these expect the data to be contained in a data folder inside the current working directory):\n",
    "\n",
    "- 1 binary field indicating whether the ad has been clicked (1) or not (0). This field is available only for the train data;\n",
    "- 13 fields containing integer features representing counts;\n",
    "- 26 categorical features. These are hashed as 32 bits keys for anonymization purposes;\n",
    "\n",
    "From a printout of the first rows of the data files it appears that the data contain no headers. This implies that, with the sole exception of the first binary field, it is not possible to characterize the various fields in terms of the features these represent. It is also noted that rows in the data can have missing values. This is again noticed when looking at the printed lines, as these have a number of entries which is lower than the number of fields specified in the `readme.txt` file. \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows in the train data\n",
    "!wc -l data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows in the test data\n",
    "!wc -l data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row of the train data\n",
    "!head -1 data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row of the test data\n",
    "!head -1 data/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression belongs to the family of so-called generalized linear models and is by far one of the most known and applied algorithms for the prediction of a target variable $Y$, which represents the possible occurrence of an event of interest $e$. This variable is binary, and usually is encoded such that $Y=1$ represents the occurrence of $e$. More specifically, given a set of explanatory features $X_i$, $i = 1,2, \\dots, n$, logistic regression characterizes the probability of occurrence of $e$, $\\pi[e] \\equiv \\pi$, as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi = \\frac{1}{1 + \\exp^{-z} }\n",
    "\\end{equation}\n",
    "\n",
    "where $z = \\beta_0 + \\sum_{i=1}^n \\beta_i X_i$ and $\\beta_i$ are model parameters. As can be seen from the equation above, and in compliance with probability rules, $\\pi \\in (0, 1)$ for any $\\beta_i$ and $X_i$, owing to the fact that the exponential function is strictly positive, and considering that the denominator is always higher than the numerator. After some algebraic manipulations an equivalent, and more compact, formulation of the above equation can be obtained as:\n",
    "\n",
    "\\begin{equation}\n",
    "log\\left( \\frac{\\pi}{1-\\pi} \\right) = \\beta_0 + \\sum_{i=1}^n \\beta_i X_i\n",
    "\\end{equation}\n",
    "\n",
    "where the left side is usually referred to as the logit function, $logit(\\pi)$, while the right side makes the linear nature of this model explicit. This becomes clearer when considering the decision boundary, i.e., the hypersurface that segments the feature space in positive versus vegative regions. For logistic regression, such boundary is associated with the locus of points in the feature space where $\\pi=0.5$, i.e., the model has no preference as to whether a point in this locus should be assigned to the positive or the negative class. Casting $\\pi=0.5$ in the left side of the equation above renders a linear equation of the decision boundary in the feature space, in compliance with the linear nature of this model.  \n",
    "\n",
    "## Log-loss function and parameter estimation\n",
    "\n",
    "In accordance with established practices in the fields of statistics and machine learning, the parameters $\\beta_i$ of the logistic regression model are estimated via maximization of the log-likelihood function. In essence, for a sample of $m$ data points $(x_{ij}, y_j)$, $i = 1,2,\\dots,n$, $j = 1,2,\\dots,m$, where $x_{ij}$ is the $j$-th record of the $i$-th feature, and $y_j$ is the $j$-th record of the target binary variable $Y$, the parameters $\\beta_i$ are estimated such that the log-likelihood function:\n",
    "\n",
    "\\begin{equation}\n",
    "log\\left[ L(\\beta_i|y_j) \\right] = \\frac{1}{m} log\\left( \\prod_{j = 1}^{m} \\pi_j^{y_j} \\left( 1-\\pi_j \\right)^{1-y_j} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "is maximized. The term in the multiplication corresponds to the likelihood function of the Bernoulli distribution for the (degenerate) case of one single trial and number of successes $y_j = 1$ and $y_j = 0$ for success and failure, respectively.\n",
    "\n",
    "Operationally, the above maximization is usually achieved taking the negative of the log-likelihood function and computing the parameters $\\beta_i$ as the argmin of the negated log-likelihood which, after some manipulations, can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{L} = -log\\left[ L(\\beta_i|y_j) \\right] = - \\frac{1}{m}\\sum_{j=1}^{m} \\left[ y_j \\cdot log(\\pi_j) + (1-y_j) \\cdot log(1-\\pi_j) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "The right term of the equation, also called Cross-Entropy or log-loss, being a function $\\pi$, is ultimately a function of the parameters $\\beta_i$ and the features $X_i$ through the logistic regression relationship. The log-loss gives some insights as to the role of this function during estimation of the parameters. Let us assume that for a certain data point, $(x_{ij}, y_j)$, the target variable is equal to $1$. For this given data point, the right term of the equation simplifies to $-log(\\pi_j)$. Since this term needs to be minimized, the parameters $\\beta_i$ of the model need to be chosen such that $\\pi_j$ approaches $1$ as closely as possible. Conversely for an observation $y_j = 0$, minimization of the log-loss, $-log(1 - \\pi_j)$, requires $\\pi_j$ to approach $0$. This dual role of the log-loss function makes such that likelihood maximization in logistic regression aims to find the set of model parameters which best separate positive from negative observations in the space of the explanatory features $X_i$, in the sense of mapping as closely as possible positive targets to $\\pi = 1$ and negative targets to $\\pi = 0$. Another appealing property, which turns out to the be of paramount importance for the strategy outlined below, is that this log-loss function is convex, i.e., one and only one point of minimum exists in the space of parameters $\\beta_i$.\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Finding the optimum set of parameters requires a suitable optimization framework. Among various approaches, gradient descent of $\\hat{L}$ is a well-established approach for functions. For a certain point of the $n$-th dimensional space of parameters $\\beta_i$, the gradient of the log-loss function, $\\nabla \\hat{L}$ is computed, and thereafter a translation is performed in the parameter space along the gradient direction (the steepest descent).\n",
    "\n",
    "Gradient descent requires the computation of the gradient. In order to derive its formulation, it is convenient to consider the $i$-th component of $\\nabla \\hat{L}$, i.e.:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_i \\hat{L} = \\frac{\\partial}{\\partial \\beta_i} \\hat{L}\n",
    "\\end{equation}\n",
    "\n",
    "Taking the derivative inside the summation and operating on the logarithm yields:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_i \\hat{L} = - \\frac{1}{m} \\sum_{j=1}^{m} \\left( \\frac{y_j}{\\pi_j} - \\frac{1-y_j}{1-\\pi_j} \\right) \\frac{\\partial \\pi_j}{\\partial \\beta_i}\n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the probability with respect to the parameter equates to (refer to the initial logistic regression formulation):\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\pi_j}{\\partial \\beta_i} = \\frac{\\exp^{-z_j}}{(1+\\exp^{-z_j})^2} \\frac{\\partial z_j} {\\partial \\beta_i} = \\frac{\\exp^{-z_j}}{1+\\exp^{-z_j}} \\frac{1}{1+\\exp^{-z_j}} \\frac{\\partial z_j} {\\partial \\beta_i} = (1-\\pi_j) \\pi_j \\frac{\\partial z_j} {\\partial \\beta_i}\n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the linear combination term yields:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial z_j} {\\partial \\beta_i} = x_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "Putting it all together, one finally obtains: \n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_i \\hat{L} = - \\frac{1}{m} \\sum_{j=1}^{m} \\left[ y_j (1-\\pi_j) - (1-y_j) \\pi_j \\right] x_{ij} = \\frac{1}{m}\\sum_{j=1}^{m} (\\pi_j-y_j) x_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "for $i = 1,2,\\dots,n$. \n",
    "\n",
    "Assuming a certain starting point in the space of parameters, $\\beta_i^0$, gradient descent first computes the gradient $\\nabla \\hat{L}$ at this starting point, and shifts the point along the direction of this gradient by computing a new point $\\beta_i^1 = \\beta_i^0 - \\alpha \\cdot \\nabla \\hat{L}$, where $\\alpha$ is a learning rate. This is done iteratively until suitable stopping criteria are met.\n",
    "\n",
    "## Algorithm for scalable implementation of logistic regression\n",
    "\n",
    "- Assume starting values for logistic parameters $\\beta_i^0$\n",
    "- Set learning parameter $\\alpha$\n",
    "- For each iteration $k$:\n",
    "- Broadcast parameters $\\beta_i^{k}$ to all worker nodes\n",
    "- Map: emit key-value pairs. Key: index $j$, values: target variable $y_j$ and array of explanatory features $x_{ij}$, for $j = 1,2,\\dots,n$\n",
    "- Map: for every $j = 1,2,\\dots,n$ compute probability $\\pi_j$ and $\\left[ y_j (1-\\pi_j) - (1-y_j) \\pi_j \\right] x_{ij}$\n",
    "- Reduce: sum over $j$ and divide by $m$, for $i = 1,2,\\dots,n$\n",
    "- Update $\\beta_i^{k}$\n",
    "- Run next iteration\n",
    "\n",
    "__References:__\n",
    "\n",
    "Bilder, C.R. and Loughin, T.M. (2015). Analysis of Categorical Data with R. CRC Press. \n",
    "\n",
    "Kremonic, Z. (2017). Maximum likelihood and gradient descent demonstration. Blog post. Accessed on July 2019 at https://zlatankr.github.io/posts/2017/03/06/mle-gradient-descent.\n",
    "\n",
    "\n",
    "\n",
    "Additional resources:\n",
    "\n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "\n",
    "https://ttic.uchicago.edu/~suriya/website-intromlss2018/course_material/Day3b.pdf \n",
    "\n",
    "http://www.holehouse.org/mlclass/06_Logistic_Regression.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For EDA purposes and notebook display, we randomly sampled 5% of the Criteo labs data via code **[Sid: put reference here]**. \n",
    "* This sampled code has a total length of 2292037 records. \n",
    "* *****Delete or add as needed: address the potential of bias; we could address this via bootstrapping and making sure that our clickthrough rate was robust** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"final_project\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sample training data and convert to dataframe\n",
    "\n",
    "#train_sample = sc.textFile('data/sample_training.txt',2)\\\n",
    "#                 .map(lambda x: x.split('\\t'))\\\n",
    "#                 .toDF().cache()\n",
    "\n",
    "#### EDA version below \n",
    "train_sample_EDA = sqlContext.read.format(\"csv\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"header\", \"false\") \\\n",
    "               .option(\"delimiter\", \"\\t\")\\\n",
    "               .load(\"data/sample_training.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "| _1| _2| _3| _4| _5|  _6| _7| _8| _9|_10|_11|_12|_13|_14|     _15|     _16|     _17|     _18|     _19|     _20|     _21|     _22|     _23|     _24|     _25|     _26|     _27|     _28|     _29|     _30|     _31|     _32|     _33|     _34|     _35|     _36|     _37|     _38|     _39|     _40|\n",
      "+---+---+---+---+---+----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|  0|  1|  1|  5|  0|1382|  4| 15|  2|181|  1|  2|   |  2|68fd1e64|80e26c9b|fb936136|7b4723c4|25c83c98|7e0ccccf|de7995b8|1f89b562|a73ee510|a8cd5504|b2cb9c98|37c9c164|2824a5f6|1adce6ef|8ba8b39a|891b62e7|e5ba7672|f54016b9|21ddcdc9|b1252a9d|07b5194c|        |3a171ecb|c5c50484|e8b83407|9727dd16|\n",
      "|  0|  2|  0| 44|  1| 102|  8|  2|  2|  4|  1|  1|   |  4|68fd1e64|f0cf0024|6f67f7e5|41274cd7|25c83c98|fe6b92e5|922afcc0|0b153874|a73ee510|2b53e5fb|4f1b46f3|623049e6|d7020589|b28479f6|e6c5b5cd|c92f3b61|07c540c4|b04e4670|21ddcdc9|5840adea|60f6221e|        |3a171ecb|43f13e8b|e8b83407|731c3655|\n",
      "|  0|  2|  0|  1| 14| 767| 89|  4|  2|245|  1|  3|  3| 45|287e684f|0a519c5c|02cf9876|c18be181|25c83c98|7e0ccccf|c78204a1|0b153874|a73ee510|3b08e48b|5f5e6091|8fe001f4|aa655a2f|07d13a8f|6dc710ed|36103458|8efede7f|3412118d|        |        |e587c466|ad3062eb|3a171ecb|3b183c5c|        |        |\n",
      "+---+---+---+---+---+----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# head first three rows\n",
    "train_sample.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice immediately from the first three rows that column names are not provided. From the Kaggle contest description, however, we do know that the first column (denoted as `c0`) contains the y variable we are interested in; specifically, a 0 denotes an ad that was not clicked on and a 1 denotes an ad that was clicked on. The following 13 fields (denoted from `_c1` to `_c13`) are numerical features, and the remaining 26 columns (denoted from `_14` to `_39` are categorical features. \n",
    "\n",
    "For easier reference in columns, we will rename the following columns as such with the code provided below: \n",
    "* `_c0` as `CTR` to denote our y variable of interest (click-through rate). \n",
    "* Numerical columns `_1` to `_13` to be denoted as `Var1` to `Var13`. \n",
    "* Categorical columns `_1r` to `_39` to be denoted as `Var14` to `Var39`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Renaming of columns for easy reference \n",
    "#### \n",
    "train_sample_EDA = train_sample_EDA.withColumnRenamed(\"_c0\", \"CTR\") \\\n",
    "       .withColumnRenamed(\"_c1\", \"Var1\") \\\n",
    "       .withColumnRenamed(\"_c2\", \"Var2\") \\\n",
    "       .withColumnRenamed(\"_c3\", \"Var3\") \\\n",
    "       .withColumnRenamed(\"_c4\", \"Var4\") \\\n",
    "       .withColumnRenamed(\"_c5\", \"Var5\") \\\n",
    "       .withColumnRenamed(\"_c6\", \"Var6\") \\\n",
    "       .withColumnRenamed(\"_c7\", \"Var7\") \\\n",
    "       .withColumnRenamed(\"_c8\", \"Var8\") \\\n",
    "       .withColumnRenamed(\"_c9\", \"Var9\") \\\n",
    "       .withColumnRenamed(\"_c10\", \"Var10\") \\\n",
    "       .withColumnRenamed(\"_c11\", \"Var11\") \\\n",
    "       .withColumnRenamed(\"_c12\", \"Var12\") \\\n",
    "       .withColumnRenamed(\"_c13\", \"Var13\") \\\n",
    "        .withColumnRenamed(\"_c14\", \"Var14\") \\\n",
    "        .withColumnRenamed(\"_c15\", \"Var15\") \\\n",
    "        .withColumnRenamed(\"_c16\", \"Var16\") \\\n",
    "        .withColumnRenamed(\"_c17\", \"Var17\") \\\n",
    "        .withColumnRenamed(\"_c18\", \"Var18\") \\\n",
    "        .withColumnRenamed(\"_c19\", \"Var19\") \\\n",
    "        .withColumnRenamed(\"_c20\", \"Var20\") \\\n",
    "        .withColumnRenamed(\"_c21\", \"Var21\") \\\n",
    "        .withColumnRenamed(\"_c22\", \"Var22\") \\\n",
    "        .withColumnRenamed(\"_c23\", \"Var23\") \\\n",
    "        .withColumnRenamed(\"_c24\", \"Var24\") \\\n",
    "        .withColumnRenamed(\"_c25\", \"Var25\") \\\n",
    "        .withColumnRenamed(\"_c26\", \"Var26\") \\\n",
    "        .withColumnRenamed(\"_c27\", \"Var27\") \\\n",
    "        .withColumnRenamed(\"_c28\", \"Var28\") \\\n",
    "        .withColumnRenamed(\"_c29\", \"Var29\") \\\n",
    "        .withColumnRenamed(\"_c30\", \"Var30\") \\\n",
    "        .withColumnRenamed(\"_c31\", \"Var31\") \\\n",
    "        .withColumnRenamed(\"_c32\", \"Var32\") \\\n",
    "        .withColumnRenamed(\"_c33\", \"Var33\") \\\n",
    "        .withColumnRenamed(\"_c34\", \"Var34\") \\\n",
    "        .withColumnRenamed(\"_c35\", \"Var35\") \\\n",
    "        .withColumnRenamed(\"_c36\", \"Var36\") \\\n",
    "        .withColumnRenamed(\"_c37\", \"Var37\") \\\n",
    "        .withColumnRenamed(\"_c38\", \"Var38\") \\\n",
    "        .withColumnRenamed(\"_c39\", \"Var39\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+----+-----+----+----+----+----+-----+-----+-----+-----+\n",
      "|CTR|Var1|Var2|Var3|Var4| Var5|Var6|Var7|Var8|Var9|Var10|Var11|Var12|Var13|\n",
      "+---+----+----+----+----+-----+----+----+----+----+-----+-----+-----+-----+\n",
      "|  0|   1|   1|   5|   0| 1382|   4|  15|   2| 181|    1|    2| null|    2|\n",
      "|  0|   2|   0|  44|   1|  102|   8|   2|   2|   4|    1|    1| null|    4|\n",
      "|  0|   2|   0|   1|  14|  767|  89|   4|   2| 245|    1|    3|    3|   45|\n",
      "|  0|null| 893|null|null| 4392|null|   0|   0|   0| null|    0| null| null|\n",
      "|  0|   3|  -1|null|   0|    2|   0|   3|   0|   0|    1|    1| null|    0|\n",
      "|  0|null|  -1|null|null|12824|null|   0|   0|   6| null|    0| null| null|\n",
      "|  0|null|   1|   2|null| 3168|null|   0|   1|   2| null|    0| null| null|\n",
      "|  1|   1|   4|   2|   0|    0|   0|   1|   0|   0|    1|    1| null|    0|\n",
      "|  0|null|  44|   4|   8|19010| 249|  28|  31| 141| null|    1| null|    8|\n",
      "|  0|null|  35|null|   1|33737|  21|   1|   2|   3| null|    1| null|    1|\n",
      "|  0|null|   2| 632|   0|56770|null|   0|   5|  65| null|    0| null|    2|\n",
      "|  0|   0|   6|   6|   6|  421| 109|   1|   7| 107|    0|    1| null|    6|\n",
      "|  1|   0|  -1|null|null| 1465|   0|  17|   0|   4|    0|    4| null| null|\n",
      "|  1|null|   2|  11|   5|10262|  34|   2|   4|   5| null|    1| null|    5|\n",
      "|  0|   0|  51|  84|   4| 3633|  26|   1|   4|   8|    0|    1| null|    4|\n",
      "|  0|null|   2|   1|  18|20255|null|   0|   1|1306| null|    0| null|   20|\n",
      "|  1|   1| 987|null|   2|  105|   2|   1|   2|   2|    1|    1| null|    2|\n",
      "|  0|   0|   1|null|   0|16597| 557|   3|   5| 123|    0|    1| null|    1|\n",
      "|  0|   0|  24|   4|   2| 2056|  12|   6|  10|  83|    0|    1| null|    2|\n",
      "|  0|   7| 102|null|   3|  780|  15|   7|  15|  15|    1|    1| null|    3|\n",
      "+---+----+----+----+----+-----+----+----+----+----+-----+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing top 20 rows of the numerical columns \n",
    "\n",
    "train_sample_EDA.select(\"CTR\", \"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\", \"Var6\", \"Var7\", \"Var8\", \"Var9\", \"Var10\", \"Var11\", \"Var12\", \"Var13\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|   Var14|   Var15|   Var16|   Var17|   Var18|   Var19|   Var20|   Var21|   Var22|   Var23|   Var24|   Var25|   Var26|   Var27|   Var28|   Var29|   Var30|   Var31|   Var32|   Var33|   Var34|   Var35|   Var36|   Var37|   Var38|   Var39|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|68fd1e64|80e26c9b|fb936136|7b4723c4|25c83c98|7e0ccccf|de7995b8|1f89b562|a73ee510|a8cd5504|b2cb9c98|37c9c164|2824a5f6|1adce6ef|8ba8b39a|891b62e7|e5ba7672|f54016b9|21ddcdc9|b1252a9d|07b5194c|    null|3a171ecb|c5c50484|e8b83407|9727dd16|\n",
      "|68fd1e64|f0cf0024|6f67f7e5|41274cd7|25c83c98|fe6b92e5|922afcc0|0b153874|a73ee510|2b53e5fb|4f1b46f3|623049e6|d7020589|b28479f6|e6c5b5cd|c92f3b61|07c540c4|b04e4670|21ddcdc9|5840adea|60f6221e|    null|3a171ecb|43f13e8b|e8b83407|731c3655|\n",
      "|287e684f|0a519c5c|02cf9876|c18be181|25c83c98|7e0ccccf|c78204a1|0b153874|a73ee510|3b08e48b|5f5e6091|8fe001f4|aa655a2f|07d13a8f|6dc710ed|36103458|8efede7f|3412118d|    null|    null|e587c466|ad3062eb|3a171ecb|3b183c5c|    null|    null|\n",
      "|68fd1e64|2c16a946|a9a87e68|2e17d6f6|25c83c98|fe6b92e5|2e8a689b|0b153874|a73ee510|efea433b|e51ddf94|a30567ca|3516f6e6|07d13a8f|18231224|52b8680f|1e88c74f|74ef3502|    null|    null|6b3a5ca6|    null|3a171ecb|9117a34a|    null|    null|\n",
      "|8cf07265|ae46a29d|c81688bb|f922efad|25c83c98|13718bbd|ad9fa255|0b153874|a73ee510|5282c137|e5d8af57|66a76a26|f06c53ac|1adce6ef|8ff4b403|01adbab4|1e88c74f|26b3c7a7|    null|    null|21c9516a|    null|32c7478e|b34f3128|    null|    null|\n",
      "|05db9164|6c9c9cf3|2730ec9c|5400db8b|43b19349|6f6d9be8|53b5f978|0b153874|a73ee510|3b08e48b|91e8fc27|be45b877|9ff13f22|07d13a8f|06969a20|9bc7fff5|776ce399|92555263|    null|    null|242bb710|8ec974f4|be7c41b4|72c78f11|    null|    null|\n",
      "|439a44a4|ad4527a2|c02372d0|d34ebbaa|43b19349|fe6b92e5|4bc6ffea|0b153874|a73ee510|3b08e48b|a4609aab|14d63538|772a00d7|07d13a8f|f9d1382e|b00d3dc9|776ce399|cdfa8259|    null|    null|20062612|    null|93bad2c0|1b256e61|    null|    null|\n",
      "|68fd1e64|2c16a946|503b9dbc|e4dbea90|f3474129|13718bbd|38eb9cf4|1f89b562|a73ee510|547c0ffe|bc8c9f21|60ab2f07|46f42a63|07d13a8f|18231224|e6b6bdc7|e5ba7672|74ef3502|    null|    null|5316a17f|    null|32c7478e|9117a34a|    null|    null|\n",
      "|05db9164|d833535f|d032c263|c18be181|25c83c98|7e0ccccf|d5b6acf2|0b153874|a73ee510|2acdcf4e|086ac2d2|dfbb09fb|41a6ae00|b28479f6|e2502ec9|84898b2a|e5ba7672|42a2edb9|    null|    null|0014c32a|    null|32c7478e|3b183c5c|    null|    null|\n",
      "|05db9164|510b40a5|d03e7c24|eb1fd928|25c83c98|    null|52283d1c|0b153874|a73ee510|015ac893|e51ddf94|951fe4a9|3516f6e6|07d13a8f|2ae4121c|8ec71479|d4bb7bd8|70d0f5f9|    null|    null|0e63fca0|    null|32c7478e|0e8fe315|    null|    null|\n",
      "|05db9164|0468d672|7ae80d0f|80d8555a|25c83c98|7e0ccccf|04277bf9|0b153874|7cc72ec2|3b08e48b|7e2c5c15|cfc86806|91a1b611|b28479f6|58251aab|146a70fd|776ce399|0b331314|21ddcdc9|5840adea|cbec39db|    null|3a171ecb|cedad179|ea9a246c|9a556cfc|\n",
      "|05db9164|9b5fd12f|    null|    null|4cf72387|    null|111121f4|0b153874|a73ee510|3b08e48b|ac9c2e8f|    null|6e2d6a15|07d13a8f|796a1a2e|    null|d4bb7bd8|8aaa5b67|    null|    null|    null|    null|32c7478e|    null|    null|    null|\n",
      "|241546e0|38a947a1|fa673455|6a14f9b9|25c83c98|fe6b92e5|1c86e0eb|1f89b562|a73ee510|e7ba2569|755e4a50|208d9687|5978055e|07d13a8f|5182f694|f8b34416|e5ba7672|e5f8f18f|    null|    null|f3ddd519|    null|32c7478e|b34f3128|    null|    null|\n",
      "|be589b51|287130e0|cd7a7a22|fb7334df|25c83c98|    null|6cdb3998|361384ce|a73ee510|3ff10fb2|5874c9c9|976cbd4c|740c210d|1adce6ef|310d155b|07eb8110|07c540c4|891589e7|18259a83|a458ea53|a0ab60ca|    null|32c7478e|a052b1ed|9b3e8820|8967c0d2|\n",
      "|5a9ed9b0|80e26c9b|97144401|5dbf0cc5|0942e0a7|13718bbd|9ce6136d|0b153874|a73ee510|2106e595|b5bb9d63|04f55317|ab04d8fe|1adce6ef|0ad47a49|2bd32e5c|3486227d|12195b22|21ddcdc9|b1252a9d|fa131867|    null|dbb486d7|8ecc176a|e8b83407|c43c3f58|\n",
      "|05db9164|bc6e3dc1|67799c69|d00d0f35|4cf72387|7e0ccccf|ca4fd8f8|64523cfa|a73ee510|3b08e48b|a0060bca|b9f28c33|22d23aac|5aebfb83|d702713a|0f655650|776ce399|3a2028fd|    null|    null|b426bc93|    null|3a171ecb|2e0a0035|    null|    null|\n",
      "|68fd1e64|38d50e09|da603082|431a5096|43b19349|7e0ccccf|3f35b640|0b153874|a73ee510|3b08e48b|3d5fb018|6aaab577|94172618|07d13a8f|ee569ce2|2f03ef40|d4bb7bd8|582152eb|21ddcdc9|b1252a9d|3b203ca1|    null|32c7478e|b21dc903|001f3601|aa5f0a15|\n",
      "|8cf07265|7cd19acc|77f2f2e5|d16679b9|4cf72387|fbad5c96|8fb24933|0b153874|a73ee510|0095a535|3617b5f5|9f32b866|428332cf|b28479f6|83ebd498|31ca40b6|e5ba7672|d0e5eb07|    null|    null|dfcfc3fa|ad3062eb|32c7478e|aee52b6f|    null|    null|\n",
      "|05db9164|f0cf0024|08b45d8b|cbb5af1b|384874ce|fbad5c96|81bb0302|37e4aa92|a73ee510|175d6c71|b7094596|1c547463|1f9d2c38|1adce6ef|55dc357b|0ca69655|e5ba7672|b04e4670|21ddcdc9|b1252a9d|f3caefdd|    null|32c7478e|4c8e5aef|ea9a246c|9593bba9|\n",
      "|3c9d8785|b0660259|3a960356|15c92ddb|4cf72387|13718bbd|00c46cd1|0b153874|a73ee510|62cfc6bd|8cffe207|656e5413|ff5626de|ad1cc976|27b1230c|fa8d05aa|e5ba7672|5edd90de|    null|    null|e12ce348|    null|c3dc6cef|49045073|    null|    null|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing top 20 rows of the categorical columns \n",
    "\n",
    "train_sample_EDA.select('Var14','Var15','Var16','Var17','Var18','Var19','Var20','Var21',\n",
    "                    'Var22','Var23','Var24','Var25','Var26','Var27','Var28','Var29','Var30',\n",
    "                   'Var31','Var32','Var33','Var34','Var35','Var36','Var37','Var38','Var39').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Brief Statistics and Coverage of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our code below, we depict brief statistics about our sampled data: \n",
    "* Our sampled data shows a click through rate of approximately 22% \n",
    "* Coverage: For the most part, most of our columns show pretty good coverage (>95%). We do see, however, a troubling number of Null (missing) values for a couple columns. \n",
    "    * Perhaps the most alarming: columns Var32, Var33, Var38, Var39, Var10, Var1, Var35, and Var12 have 50% or less of their values as non-Null. \n",
    "    \n",
    "A significant challenge for us will involve having to deal with high number of Null values in some of our columns. For numerical values, it is certainly possible to impute a number to fill in for Null values, but this challenge gets tricky with categorical variables. The handling of null values for categorical variables will be discussed in **[insert section here]**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a count of the total number of rows in our sampled data. \n",
    "total_count = train_sample_EDA.count()\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22663"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating approximate click through rate of our sampled training data \n",
    "\n",
    "train_sample_EDA.filter(train_sample_EDA.CTR==1).count()/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage_nonNull</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CTR</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var14</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var36</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var31</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var30</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var28</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var27</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var26</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var24</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var23</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var22</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var21</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var18</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var15</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var20</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var2</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var8</th>\n",
       "      <td>0.99893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var17</th>\n",
       "      <td>0.96065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var37</th>\n",
       "      <td>0.96065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var34</th>\n",
       "      <td>0.96065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var25</th>\n",
       "      <td>0.96065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var16</th>\n",
       "      <td>0.96065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var29</th>\n",
       "      <td>0.96065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var11</th>\n",
       "      <td>0.95281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var9</th>\n",
       "      <td>0.95281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var7</th>\n",
       "      <td>0.95281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var5</th>\n",
       "      <td>0.95240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var19</th>\n",
       "      <td>0.86292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var3</th>\n",
       "      <td>0.80898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var13</th>\n",
       "      <td>0.80466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var4</th>\n",
       "      <td>0.80466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var6</th>\n",
       "      <td>0.74893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var32</th>\n",
       "      <td>0.58529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var33</th>\n",
       "      <td>0.58529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var38</th>\n",
       "      <td>0.58529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var39</th>\n",
       "      <td>0.58529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var10</th>\n",
       "      <td>0.55587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var1</th>\n",
       "      <td>0.55587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var12</th>\n",
       "      <td>0.22820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var35</th>\n",
       "      <td>0.18434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Coverage_nonNull\n",
       "CTR             1.00000\n",
       "Var14           1.00000\n",
       "Var36           1.00000\n",
       "Var31           1.00000\n",
       "Var30           1.00000\n",
       "Var28           1.00000\n",
       "Var27           1.00000\n",
       "Var26           1.00000\n",
       "Var24           1.00000\n",
       "Var23           1.00000\n",
       "Var22           1.00000\n",
       "Var21           1.00000\n",
       "Var18           1.00000\n",
       "Var15           1.00000\n",
       "Var20           1.00000\n",
       "Var2            1.00000\n",
       "Var8            0.99893\n",
       "Var17           0.96065\n",
       "Var37           0.96065\n",
       "Var34           0.96065\n",
       "Var25           0.96065\n",
       "Var16           0.96065\n",
       "Var29           0.96065\n",
       "Var11           0.95281\n",
       "Var9            0.95281\n",
       "Var7            0.95281\n",
       "Var5            0.95240\n",
       "Var19           0.86292\n",
       "Var3            0.80898\n",
       "Var13           0.80466\n",
       "Var4            0.80466\n",
       "Var6            0.74893\n",
       "Var32           0.58529\n",
       "Var33           0.58529\n",
       "Var38           0.58529\n",
       "Var39           0.58529\n",
       "Var10           0.55587\n",
       "Var1            0.55587\n",
       "Var12           0.22820\n",
       "Var35           0.18434"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating coverage: \n",
    "# The column \"Coverage_nonNull\" (expressed as percentage) depicts how many non-null values are in the column. \n",
    "# This gives a sense of how well a particular variable covers our entire dataset. \n",
    "\n",
    "from pyspark.sql.functions import col, count, isnan, stddev, lit, sum\n",
    "\n",
    "coverage = train_sample_EDA.agg(*[\n",
    "    (count(c)/total_count).alias(c)    # vertical (column-wise) operations in SQL ignore NULLs\n",
    "    for c in train_sample_EDA.columns\n",
    "]).toPandas()\n",
    "\n",
    "coverage_summary = coverage.T\n",
    "coverage_summary.columns = ['Coverage_nonNull']\n",
    "coverage_summary.sort_values(by='Coverage_nonNull', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Numerical Variables\n",
    "\n",
    "### 3.2.1 Basic Statistics\n",
    "* In Table 3.0, we calculate basic statistics to summarize our numerical variables: count of observations that have the variable, the mean of the variable, the standard deviation of the variable, and the min and max of the variable. \n",
    "    * We note that the range for each variable is different; although we do not know the exact definition of what each variable stands for, we can tell that the range of each variable can vary greatly, such as 0 to 6 for Var10 but 0 to 1,741,128 for Var5. \n",
    "    * Variable 2 has a minimum value of -2, which is odd given that the Kaggle criteo data description (https://www.kaggle.com/c/criteo-display-ad-challenge/data) specifically states that the numerical variables are mostly counts but there is no specification as to whether all numbers should be non-negative. This will be addressed in **[insert section]** on deciding how negative values are to be treated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var1</th>\n",
       "      <td>55587</td>\n",
       "      <td>3.7687229028369944</td>\n",
       "      <td>10.451208916328204</td>\n",
       "      <td>0</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var2</th>\n",
       "      <td>100000</td>\n",
       "      <td>112.86373</td>\n",
       "      <td>401.522635880915</td>\n",
       "      <td>-2</td>\n",
       "      <td>18522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var3</th>\n",
       "      <td>80898</td>\n",
       "      <td>40.74491334767238</td>\n",
       "      <td>538.8188223812444</td>\n",
       "      <td>0</td>\n",
       "      <td>65535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var4</th>\n",
       "      <td>80466</td>\n",
       "      <td>8.280317152586186</td>\n",
       "      <td>10.83633586305846</td>\n",
       "      <td>0</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var5</th>\n",
       "      <td>95240</td>\n",
       "      <td>17592.59940151197</td>\n",
       "      <td>65797.8980685818</td>\n",
       "      <td>0</td>\n",
       "      <td>1741128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var6</th>\n",
       "      <td>74893</td>\n",
       "      <td>139.6850840532493</td>\n",
       "      <td>371.77609153232294</td>\n",
       "      <td>0</td>\n",
       "      <td>16290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var7</th>\n",
       "      <td>95281</td>\n",
       "      <td>15.222090448253063</td>\n",
       "      <td>65.46048661579579</td>\n",
       "      <td>0</td>\n",
       "      <td>8807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var8</th>\n",
       "      <td>99893</td>\n",
       "      <td>13.574825062817215</td>\n",
       "      <td>46.541592894521614</td>\n",
       "      <td>0</td>\n",
       "      <td>4677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var9</th>\n",
       "      <td>95281</td>\n",
       "      <td>125.29490664455663</td>\n",
       "      <td>286.41568623846194</td>\n",
       "      <td>0</td>\n",
       "      <td>12661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var10</th>\n",
       "      <td>55587</td>\n",
       "      <td>0.6201090182956447</td>\n",
       "      <td>0.6770553545626542</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var11</th>\n",
       "      <td>95281</td>\n",
       "      <td>2.4002686789601286</td>\n",
       "      <td>4.629926011637236</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var12</th>\n",
       "      <td>22820</td>\n",
       "      <td>0.9377738825591586</td>\n",
       "      <td>5.327668353607891</td>\n",
       "      <td>0</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var13</th>\n",
       "      <td>80466</td>\n",
       "      <td>11.60763552307807</td>\n",
       "      <td>52.04455598548969</td>\n",
       "      <td>0</td>\n",
       "      <td>6558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0                   1                   2    3        4\n",
       "summary   count                mean              stddev  min      max\n",
       "Var1      55587  3.7687229028369944  10.451208916328204    0      556\n",
       "Var2     100000           112.86373    401.522635880915   -2    18522\n",
       "Var3      80898   40.74491334767238   538.8188223812444    0    65535\n",
       "Var4      80466   8.280317152586186   10.83633586305846    0      417\n",
       "Var5      95240   17592.59940151197    65797.8980685818    0  1741128\n",
       "Var6      74893   139.6850840532493  371.77609153232294    0    16290\n",
       "Var7      95281  15.222090448253063   65.46048661579579    0     8807\n",
       "Var8      99893  13.574825062817215  46.541592894521614    0     4677\n",
       "Var9      95281  125.29490664455663  286.41568623846194    0    12661\n",
       "Var10     55587  0.6201090182956447  0.6770553545626542    0        6\n",
       "Var11     95281  2.4002686789601286   4.629926011637236    0      104\n",
       "Var12     22820  0.9377738825591586   5.327668353607891    0      493\n",
       "Var13     80466   11.60763552307807   52.04455598548969    0     6558"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "# Table 3.0: Basic statistics \n",
    "########\n",
    "\n",
    "numerical_cols = ['Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','Var9','Var10','Var11','Var12','Var13']\n",
    "\n",
    "numerical_stats = train_sample_EDA.describe(numerical_cols).toPandas()\n",
    "numerical_stats.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below in Table 3.1, we also calculated the mean, standard deviation, count, min, and max of each numerical variable grouped by the click through rate (CTR) target variable. \n",
    "    * From here, we made a rough guess-timate of the 95% confidence interval of the mean of the numerical variables to see if we could distinguish enough difference between observations that had a successful or unsuccessful click through on an ad. We realize that the distribution of the numerical variables were not exactly normal, but given the high number of observations, a 95% confidence interval by the form of $(\\mu - 1.96*\\frac{\\sigma}{n_{group}}, \\mu +1.96*\\frac{\\sigma}{n_{group}})$ should give a rough estimate of the group means and their standard deviations. \n",
    "    * We can see that most of numerical variables show quite some difference between their group means. The columns `lowerCI_CTR0`, `upperCI_CTR0` depict the lower and upper confidence interval bounds for the observations under CTR = 0, while the columns `lowerCI_CTR1`, `upperCI_CTR1` depict the lower and upper confidence interval bounds for the observations under CTR = 1. Most of the confidence intervals between the 2 groups do not overlap; there appears to be enough information in our numerical variables such that they should all be included into our model to distinguish the difference between CTR=0 or CTR=1 prediction. That being said, it should be noted that Var4, Var8, and Var10 have the closest confidence interval boundaries compared to the other columns, and it is possible that these 3 columns might have smaller explaining power compared to the other numerical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>avg_CTR0</th>\n",
       "      <th>avg_CTR1</th>\n",
       "      <th>std_CTR0</th>\n",
       "      <th>std_CTR1</th>\n",
       "      <th>min_CTR0</th>\n",
       "      <th>min_CTR1</th>\n",
       "      <th>max_CTR0</th>\n",
       "      <th>max_CTR1</th>\n",
       "      <th>count_CTR0</th>\n",
       "      <th>count_CTR1</th>\n",
       "      <th>lowerCI_CTR0</th>\n",
       "      <th>upperCI_CTR0</th>\n",
       "      <th>lowerCI_CTR1</th>\n",
       "      <th>upperCI_CTR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Var1</td>\n",
       "      <td>3.105930</td>\n",
       "      <td>5.472190</td>\n",
       "      <td>9.523958</td>\n",
       "      <td>12.361617</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>556</td>\n",
       "      <td>466</td>\n",
       "      <td>40017</td>\n",
       "      <td>15570</td>\n",
       "      <td>3.012615</td>\n",
       "      <td>3.199245</td>\n",
       "      <td>5.278018</td>\n",
       "      <td>5.666362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Var2</td>\n",
       "      <td>104.044106</td>\n",
       "      <td>142.960508</td>\n",
       "      <td>377.564659</td>\n",
       "      <td>473.025731</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>8180</td>\n",
       "      <td>18522</td>\n",
       "      <td>77337</td>\n",
       "      <td>22663</td>\n",
       "      <td>101.383051</td>\n",
       "      <td>106.705160</td>\n",
       "      <td>136.801906</td>\n",
       "      <td>149.119110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Var3</td>\n",
       "      <td>39.545991</td>\n",
       "      <td>45.090202</td>\n",
       "      <td>508.778938</td>\n",
       "      <td>635.908429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65535</td>\n",
       "      <td>21636</td>\n",
       "      <td>63404</td>\n",
       "      <td>17494</td>\n",
       "      <td>35.585702</td>\n",
       "      <td>43.506280</td>\n",
       "      <td>35.666836</td>\n",
       "      <td>54.513569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Var4</td>\n",
       "      <td>8.562535</td>\n",
       "      <td>7.283663</td>\n",
       "      <td>11.227450</td>\n",
       "      <td>9.256453</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>417</td>\n",
       "      <td>134</td>\n",
       "      <td>62709</td>\n",
       "      <td>17757</td>\n",
       "      <td>8.474658</td>\n",
       "      <td>8.650411</td>\n",
       "      <td>7.147513</td>\n",
       "      <td>7.419812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Var5</td>\n",
       "      <td>20439.135068</td>\n",
       "      <td>8212.370225</td>\n",
       "      <td>72249.626124</td>\n",
       "      <td>35774.517424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1741128</td>\n",
       "      <td>1300841</td>\n",
       "      <td>73067</td>\n",
       "      <td>22173</td>\n",
       "      <td>19915.256011</td>\n",
       "      <td>20963.014125</td>\n",
       "      <td>7741.482221</td>\n",
       "      <td>8683.258229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Var6</td>\n",
       "      <td>163.863788</td>\n",
       "      <td>72.758151</td>\n",
       "      <td>405.521557</td>\n",
       "      <td>243.964464</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16290</td>\n",
       "      <td>15658</td>\n",
       "      <td>55017</td>\n",
       "      <td>19876</td>\n",
       "      <td>160.475178</td>\n",
       "      <td>167.252397</td>\n",
       "      <td>69.366445</td>\n",
       "      <td>76.149856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Var7</td>\n",
       "      <td>12.155584</td>\n",
       "      <td>25.392561</td>\n",
       "      <td>62.065786</td>\n",
       "      <td>74.742871</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8807</td>\n",
       "      <td>2240</td>\n",
       "      <td>73208</td>\n",
       "      <td>22073</td>\n",
       "      <td>11.705981</td>\n",
       "      <td>12.605187</td>\n",
       "      <td>24.406520</td>\n",
       "      <td>26.378602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Var8</td>\n",
       "      <td>14.123540</td>\n",
       "      <td>11.704083</td>\n",
       "      <td>52.446823</td>\n",
       "      <td>12.989025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4677</td>\n",
       "      <td>126</td>\n",
       "      <td>77238</td>\n",
       "      <td>22655</td>\n",
       "      <td>13.753661</td>\n",
       "      <td>14.493419</td>\n",
       "      <td>11.534941</td>\n",
       "      <td>11.873225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Var9</td>\n",
       "      <td>123.191168</td>\n",
       "      <td>132.272233</td>\n",
       "      <td>284.608828</td>\n",
       "      <td>292.226579</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12661</td>\n",
       "      <td>5670</td>\n",
       "      <td>73208</td>\n",
       "      <td>22073</td>\n",
       "      <td>121.129469</td>\n",
       "      <td>125.252866</td>\n",
       "      <td>128.417049</td>\n",
       "      <td>136.127417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Var10</td>\n",
       "      <td>0.547617</td>\n",
       "      <td>0.806423</td>\n",
       "      <td>0.640019</td>\n",
       "      <td>0.731838</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>40017</td>\n",
       "      <td>15570</td>\n",
       "      <td>0.541346</td>\n",
       "      <td>0.553888</td>\n",
       "      <td>0.794927</td>\n",
       "      <td>0.817918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Var11</td>\n",
       "      <td>1.985971</td>\n",
       "      <td>3.774340</td>\n",
       "      <td>3.935803</td>\n",
       "      <td>6.220927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>97</td>\n",
       "      <td>73208</td>\n",
       "      <td>22073</td>\n",
       "      <td>1.957461</td>\n",
       "      <td>2.014482</td>\n",
       "      <td>3.692270</td>\n",
       "      <td>3.856409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Var12</td>\n",
       "      <td>0.718703</td>\n",
       "      <td>1.546795</td>\n",
       "      <td>4.888843</td>\n",
       "      <td>6.352105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>493</td>\n",
       "      <td>178</td>\n",
       "      <td>16783</td>\n",
       "      <td>6037</td>\n",
       "      <td>0.644738</td>\n",
       "      <td>0.792669</td>\n",
       "      <td>1.386558</td>\n",
       "      <td>1.707032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Var13</td>\n",
       "      <td>12.798976</td>\n",
       "      <td>7.400405</td>\n",
       "      <td>58.474210</td>\n",
       "      <td>13.285015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6558</td>\n",
       "      <td>252</td>\n",
       "      <td>62709</td>\n",
       "      <td>17757</td>\n",
       "      <td>12.341303</td>\n",
       "      <td>13.256649</td>\n",
       "      <td>7.205002</td>\n",
       "      <td>7.595809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column      avg_CTR0     avg_CTR1      std_CTR0      std_CTR1  min_CTR0  \\\n",
       "0    Var1      3.105930     5.472190      9.523958     12.361617         0   \n",
       "1    Var2    104.044106   142.960508    377.564659    473.025731        -2   \n",
       "2    Var3     39.545991    45.090202    508.778938    635.908429         0   \n",
       "3    Var4      8.562535     7.283663     11.227450      9.256453         0   \n",
       "4    Var5  20439.135068  8212.370225  72249.626124  35774.517424         0   \n",
       "5    Var6    163.863788    72.758151    405.521557    243.964464         0   \n",
       "6    Var7     12.155584    25.392561     62.065786     74.742871         0   \n",
       "7    Var8     14.123540    11.704083     52.446823     12.989025         0   \n",
       "8    Var9    123.191168   132.272233    284.608828    292.226579         0   \n",
       "9   Var10      0.547617     0.806423      0.640019      0.731838         0   \n",
       "10  Var11      1.985971     3.774340      3.935803      6.220927         0   \n",
       "11  Var12      0.718703     1.546795      4.888843      6.352105         0   \n",
       "12  Var13     12.798976     7.400405     58.474210     13.285015         0   \n",
       "\n",
       "    min_CTR1  max_CTR0  max_CTR1  count_CTR0  count_CTR1  lowerCI_CTR0  \\\n",
       "0          0       556       466       40017       15570      3.012615   \n",
       "1         -2      8180     18522       77337       22663    101.383051   \n",
       "2          0     65535     21636       63404       17494     35.585702   \n",
       "3          0       417       134       62709       17757      8.474658   \n",
       "4          0   1741128   1300841       73067       22173  19915.256011   \n",
       "5          0     16290     15658       55017       19876    160.475178   \n",
       "6          0      8807      2240       73208       22073     11.705981   \n",
       "7          0      4677       126       77238       22655     13.753661   \n",
       "8          0     12661      5670       73208       22073    121.129469   \n",
       "9          0         6         6       40017       15570      0.541346   \n",
       "10         0       104        97       73208       22073      1.957461   \n",
       "11         0       493       178       16783        6037      0.644738   \n",
       "12         0      6558       252       62709       17757     12.341303   \n",
       "\n",
       "    upperCI_CTR0  lowerCI_CTR1  upperCI_CTR1  \n",
       "0       3.199245      5.278018      5.666362  \n",
       "1     106.705160    136.801906    149.119110  \n",
       "2      43.506280     35.666836     54.513569  \n",
       "3       8.650411      7.147513      7.419812  \n",
       "4   20963.014125   7741.482221   8683.258229  \n",
       "5     167.252397     69.366445     76.149856  \n",
       "6      12.605187     24.406520     26.378602  \n",
       "7      14.493419     11.534941     11.873225  \n",
       "8     125.252866    128.417049    136.127417  \n",
       "9       0.553888      0.794927      0.817918  \n",
       "10      2.014482      3.692270      3.856409  \n",
       "11      0.792669      1.386558      1.707032  \n",
       "12     13.256649      7.205002      7.595809  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########\n",
    "# Table 3.1 Basic Statistics by Group (click through rate == 0 vs click through rate == 1) in each column. \n",
    "########## \n",
    "\n",
    "grouped_CTR0 = []\n",
    "grouped_CTR1 = []\n",
    "\n",
    "for p in range(0, len(numerical_cols)): \n",
    "    g1_avg, g0_avg=train_sample_EDA.groupBy('CTR').agg({numerical_cols[p]:\"avg\"}).collect()\n",
    "    g1_std, g0_std=train_sample_EDA.groupBy('CTR').agg({numerical_cols[p]:\"stddev\"}).collect()\n",
    "    g1_count, g0_count = train_sample_EDA.groupBy('CTR').agg({numerical_cols[p]:\"count\"}).collect()\n",
    "    g1_min, g0_min = train_sample_EDA.groupBy('CTR').agg({numerical_cols[p]:\"min\"}).collect()\n",
    "    g1_max, g0_max = train_sample_EDA.groupBy('CTR').agg({numerical_cols[p]:\"max\"}).collect()\n",
    "    grouped_CTR1.append({'Column': numerical_cols[p], 'avg_CTR1': g1_avg[1], 'std_CTR1': g1_std[1], \n",
    "                       'count_CTR1': g1_count[1], 'min_CTR1': g1_min[1], 'max_CTR1': g1_max[1]})\n",
    "    grouped_CTR0.append({'Column': numerical_cols[p], 'avg_CTR0': g0_avg[1], 'std_CTR0': g0_std[1],\n",
    "                       'count_CTR0': g0_count[1], 'min_CTR0': g0_min[1], 'max_CTR0': g0_max[1]})\n",
    "\n",
    "CTR0_df = pd.DataFrame(grouped_CTR0)\n",
    "CTR1_df = pd.DataFrame(grouped_CTR1)\n",
    "merged_group_agg = CTR0_df.merge(CTR1_df, how='left', on='Column')[['Column','avg_CTR0','avg_CTR1','std_CTR0','std_CTR1','min_CTR0','min_CTR1','max_CTR0','max_CTR1','count_CTR0','count_CTR1']]\n",
    "merged_group_agg['lowerCI_CTR0']=merged_group_agg['avg_CTR0']-((1.96)*(merged_group_agg['std_CTR0']/np.sqrt(merged_group_agg['count_CTR0'])))\n",
    "merged_group_agg['upperCI_CTR0']=merged_group_agg['avg_CTR0']+((1.96)*(merged_group_agg['std_CTR0']/np.sqrt(merged_group_agg['count_CTR0'])))\n",
    "merged_group_agg['lowerCI_CTR1']=merged_group_agg['avg_CTR1']-((1.96)*(merged_group_agg['std_CTR1']/np.sqrt(merged_group_agg['count_CTR1'])))\n",
    "merged_group_agg['upperCI_CTR1']=merged_group_agg['avg_CTR1']+((1.96)*(merged_group_agg['std_CTR1']/np.sqrt(merged_group_agg['count_CTR1'])))\n",
    "merged_group_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Histograms \n",
    "* We also plot the histograms of each of the numerical variables to gain a sense of distribution. \n",
    "* We note that pretty much all of the numerical variables are skewed right; most of the observations generally are centered around the lower range of the values and the remaining higher values are tailed off to the right. \n",
    "* As the range of all variables varies widely across all variables, we plan to standardize the numerical variables by **[insert standardization method]** such that not one variable overpowers over the other numerical variables solely due to scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 14 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########\n",
    "# Histogram of numerical variables \n",
    "########\n",
    "\n",
    "# Note that the last chart is empty as we only have 13 numerical variables. \n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "numerical_cols = ['Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','Var9','Var10','Var11','Var12','Var13']\n",
    "\n",
    "fig, axs = plt.subplots(2,7, figsize=(20,10))\n",
    "axs = axs.ravel()\n",
    "for p in range(0,len(numerical_cols)): \n",
    "    list_values = train_sample_EDA.select(numerical_cols[p]).rdd.flatMap(lambda x: x).collect()\n",
    "    cleaned_list = [x for x in list_values if x is not None]\n",
    "    axs[p].hist(cleaned_list, bins=20, density=True)\n",
    "    axs[p].set_title(numerical_cols[p])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Categorical Variables and Cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are a total of 26 categorical columns that are also included within this dataset. \n",
    "* In Table 3.2, we immediately notice the high dimensionality when we take note of the number of distinct values in each categorical column. Some (ex: Var33 and Var22) have fewer than 5 distinct values while others (ex: Var16 and Var25) have over 10,000 distinct values. We explore our methods of preprocessing and reducing dimensionality in **[insert section]**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>count_distinct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Var16</td>\n",
       "      <td>43870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Var25</td>\n",
       "      <td>41312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Var34</td>\n",
       "      <td>38618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Var29</td>\n",
       "      <td>34617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Var17</td>\n",
       "      <td>25184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Var37</td>\n",
       "      <td>12335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Var23</td>\n",
       "      <td>10997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Var39</td>\n",
       "      <td>9527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Var20</td>\n",
       "      <td>7623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Var28</td>\n",
       "      <td>5238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Var24</td>\n",
       "      <td>3799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Var26</td>\n",
       "      <td>2796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Var31</td>\n",
       "      <td>2548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Var32</td>\n",
       "      <td>1303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Var14</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Var15</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Var21</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Var18</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Var38</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Var27</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Var36</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Var19</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Var35</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Var30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Var33</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Var22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column  count_distinct\n",
       "2   Var16           43870\n",
       "11  Var25           41312\n",
       "20  Var34           38618\n",
       "15  Var29           34617\n",
       "3   Var17           25184\n",
       "23  Var37           12335\n",
       "9   Var23           10997\n",
       "25  Var39            9527\n",
       "6   Var20            7623\n",
       "14  Var28            5238\n",
       "10  Var24            3799\n",
       "12  Var26            2796\n",
       "17  Var31            2548\n",
       "18  Var32            1303\n",
       "0   Var14             541\n",
       "1   Var15             497\n",
       "7   Var21             257\n",
       "4   Var18             145\n",
       "24  Var38              51\n",
       "13  Var27              26\n",
       "22  Var36              14\n",
       "5   Var19              12\n",
       "21  Var35              11\n",
       "16  Var30              10\n",
       "19  Var33               4\n",
       "8   Var22               3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols = ['Var14','Var15','Var16','Var17','Var18','Var19','Var20','Var21','Var22',\n",
    "                    'Var23','Var24','Var25','Var26','Var27','Var28','Var29','Var30','Var31',\n",
    "                   'Var32','Var33','Var34','Var35','Var36','Var37','Var38','Var39']\n",
    "\n",
    "###########\n",
    "# Table 3.2: Count distinct values in each categorical column \n",
    "###########\n",
    "\n",
    "distinct_temp = []\n",
    "for column in categorical_cols: \n",
    "    distinct_temp.append({'Column':column, 'count_distinct': train_sample_EDA.select(column).distinct().count()})\n",
    "    \n",
    "distinct_df = pd.DataFrame(distinct_temp)\n",
    "distinct_df.sort_values(by='count_distinct', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high dimensionality of our categorical columns if we were to one hot encode so many categorical features, we wanted to know if there were certain values within the categorical variables that were more common than others, and thus would be more predictive of the final click through rate label of 1 or 0. \n",
    "* It is likely that certain values of a categorical column may be rare and therefore may not contribute much predictive power. If we were to tally up the counts of each distinct value in a categorical column (split by CTR group, or CTR == 0 vs CTR == 1) and calculate a cumulative summation of the number of observations observed per distinct value, we could distinguish just how many of the individual values within a categorical variable can cover the majority of the dataset. By further breaking down the top most frequent values in each CTR == 0 and CTR == 1 group, we can also observe which values are more telling of whether an ad will be clicked on or not. \n",
    "    * It should be noted that we defined 'majority' here as 95% of the observations within each group (CTR==0, CTR==1). \n",
    "* Furthermore, we would like to observe a sense of \"correlation\" between the labelled variable (CTR == 0, CTR==1) and our categorical features to see if there is any sort of association between the two variables. Pearson's Correlation unfortunately will not suffice, as we would be comparing a categorical variable (with no particular ordering) against a binary output variable that simply labels a successful/unsuccessful ad click. \n",
    "    * One particular workaround is the Cramer's V statistic (https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V): Based on the chi-squared statistic, Cramer's V is a measure of association between two nominal variables and gives an output number between 0 and 1 to quantify the strength of the association between two nominal variables. We therefore calculate Cramer's V statistic between successful/unsuccessful ad clicks (CTR==0, CTR==1) against the categorical values in each column to measure (if any) the strength of association between the two variables. \n",
    "\n",
    "\n",
    "* **TO CONSIDER: What to do if 'null' is one of the top values for a categorical variable?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Cumulative Summation \n",
    "# Intent: In each column, we want to know which values contribute the most in terms of frequency for CTR=1. \n",
    "##########\n",
    "\n",
    "from pyspark.sql.functions import col, desc\n",
    "from pyspark.sql import window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "count_1 = train_sample_EDA.filter(col(\"CTR\")==1).count()\n",
    "count_0 = train_sample_EDA.filter(col(\"CTR\")==0).count()\n",
    "\n",
    "# Set aside a subset \n",
    "subset_CTR1=train_sample_EDA.filter(col(\"CTR\")==1)[categorical_cols]\n",
    "subset_CTR0=train_sample_EDA.filter(col(\"CTR\")==0)[categorical_cols]\n",
    "\n",
    "win_spec = (window.Window\n",
    "                  .partitionBy()\n",
    "                  .rowsBetween(window.Window.unboundedPreceding, 0))\n",
    "\n",
    "top_cumsum_ctr1 = []\n",
    "all_cumsum_ctr1 = []\n",
    "top_cumsum_ctr0 = []\n",
    "all_cumsum_ctr0 = []\n",
    "summary_cumsum = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # --- Calculate cumulative summation for CTR==1 \n",
    "    tempdf = subset_CTR1.groupBy([col]).count().sort(desc('count')).cache()\n",
    "    tempdf = tempdf.withColumn('count_frequ', tempdf['count']/count_1).cache()\n",
    "    tempdf = tempdf.withColumn('cumsum',F.sum(tempdf.count_frequ).over(win_spec))\n",
    "    all_cumsum_ctr1.append(tempdf)\n",
    "    temp_pd_CTR1 = tempdf.where(tempdf.cumsum<=0.95).toPandas()\n",
    "    top_cumsum_ctr1.append(temp_pd_CTR1)\n",
    "    \n",
    "    # --- Calculate cumulative summation for CTR==0 \n",
    "    tempdf = subset_CTR0.groupBy([col]).count().sort(desc('count')).cache()\n",
    "    tempdf = tempdf.withColumn('count_frequ', tempdf['count']/count_0).cache()\n",
    "    tempdf = tempdf.withColumn('cumsum',F.sum(tempdf.count_frequ).over(win_spec))\n",
    "    all_cumsum_ctr0.append(tempdf)\n",
    "    temp_pd_CTR0 = tempdf.where(tempdf.cumsum<=0.95).toPandas()\n",
    "    top_cumsum_ctr0.append(temp_pd_CTR0)\n",
    "    \n",
    "    # Summary stats: \n",
    "    # For each column: count up the number of distinct values that are needed to cover 95% of the observations in each group\n",
    "    summary_cumsum.append({'Column': col, 'top_freq_CTR0_count': len(temp_pd_CTR0), 'top_freq_CTR1_count': len(temp_pd_CTR1)})\n",
    "\n",
    "summary_cumsum_df = pd.DataFrame(summary_cumsum)\n",
    "\n",
    "############\n",
    "# Save the top values for each CTR group out for each categorical column \n",
    "# DO NOT run this part of the code unless you want to save out the individual CSVs. \n",
    "# These CSVs can be used for broadcasting later on. \n",
    "############ \n",
    "\n",
    "#for i in range(0, 26): \n",
    "#    filename = 'CTR0_'+str(categorical_cols[i])+'.csv'\n",
    "#    all_cumsum_ctr0[i].toPandas().to_csv(filename, index=False)\n",
    "#    filename = 'CTR1_'+str(categorical_cols[i])+'.csv'\n",
    "#    all_cumsum_ctr1[i].toPandas().to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# Cramer's V \n",
    "###### \n",
    "\n",
    "# Cramer's Value \n",
    "# referenced from https://stackoverflow.com/questions/20892799/using-pandas-calculate-cram%C3%A9rs-coefficient-matrix\n",
    "# and https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n",
    "\n",
    "def cramers_corrected_stat(confusion_matrix): \n",
    "    \"\"\"\n",
    "    This function calculates the Cramer's coefficient matrix given a contingency table. \n",
    "    It has been discussed before that cramer's V can be over-optimistic in estimating association between between categories. \n",
    "    Therefore, a correction for bias has been implemented in this function as well. \n",
    "    \"\"\"\n",
    "    # --- get total n \n",
    "    n = np.nansum(confusion_matrix.sum()) \n",
    "    \n",
    "    # --- get r,k (shape), where r = number of rows, k = number of columns \n",
    "    r,k = confusion_matrix.shape \n",
    "    \n",
    "    # --- get chi-2 statistic\n",
    "    chi2 = 0\n",
    "    row_sums = confusion_matrix.sum(axis=1) \n",
    "    col_sums = confusion_matrix.sum(axis=0) \n",
    "    for index, row in confusion_matrix.iterrows(): \n",
    "        # index will denote the row number \n",
    "        for col in confusion_matrix.columns: # iterate across rows \n",
    "            chi2+=((row[col]-(row_sums[index]*col_sums[col]/n))**2)/(row_sums[index]*col_sums[col]/n)\n",
    "    # --- get phi2 \n",
    "    phi2 = chi2/n \n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1)) \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "\n",
    "all_categorical_columns = ['Var14','Var15','Var16','Var17','Var18','Var19','Var20','Var21','Var22','Var23','Var24','Var25','Var26','Var27','Var28','Var29','Var30','Var31','Var32','Var33','Var34','Var35','Var36','Var37','Var38','Var39']\n",
    "\n",
    "def calculate_adj_cramersV_allcolumns(columns, rdd_obj): \n",
    "    \"\"\"\n",
    "    This function will apply Cramer's value function (cramers_corrected_stat) onto the values of categorical columns in the RDD. \n",
    "    \"\"\"\n",
    "    cramers_df = []\n",
    "    for c in columns: \n",
    "        # --- make contingency table \n",
    "        contingency = rdd_obj.map(lambda x: ((x['CTR'],x[c]),1)) \\\n",
    "            .reduceByKey(lambda x,y: x+y) \\\n",
    "            .map(lambda x: ((x[0][0],x[0][1]),x[1])).collect()\n",
    "        \n",
    "        # --- Unwrap contingency mapped output \n",
    "        unwrapped_df = pd.DataFrame([[contingency[i][0][0], contingency[i][0][1], contingency[i][1]] for i in range(0, len(contingency))], columns = ['ctr','category','count']).sort_values(by=['category','ctr']).reset_index(drop=True)\n",
    "        \n",
    "        # --- Make contingency table \n",
    "        matr_obj = pd.DataFrame(0, columns = unwrapped_df['category'].drop_duplicates().values, index = unwrapped_df['ctr'].drop_duplicates().values)\n",
    "        for index, row in unwrapped_df.iterrows(): \n",
    "            matr_obj.at[row['ctr'], row['category']]=row['count']\n",
    "        \n",
    "        # --- Calculate Cramer's V (adjusted) \n",
    "        cramersV_value = cramers_corrected_stat(matr_obj)\n",
    "        cramers_df.append({'column': c, 'cramersvalue': cramersV_value})\n",
    "    return(cramers_df)\n",
    "\n",
    "data_rdd = train_sample_EDA.rdd\n",
    "\n",
    "all_categorical_columns = ['Var14','Var15','Var16','Var17','Var18','Var19','Var20',\n",
    "                           'Var21','Var22','Var23','Var24','Var25','Var26','Var27',\n",
    "                           'Var28','Var29','Var30','Var31','Var32','Var33','Var34',\n",
    "                           'Var35','Var36','Var37','Var38','Var39']\n",
    "\n",
    "cramers_results = pd.DataFrame(calculate_adj_cramersV_allcolumns(all_categorical_columns, data_rdd)).reset_index(drop=True)\n",
    "cramers_results.columns=['Column','cramersvalue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore combine all of our summary statistics together. In table 3.3, for each categorical variable, we provide the \n",
    "* number of distinct values (`count_distinct`)\n",
    "* number of categorical values that cover 95% of the observations in each labelled group (CTR==0 in `top_freq_CTR0_count` and CTR==1 in `top_freq_CTR1_count`). \n",
    "* percentage of observations in the categorical column that are not null (`Coverage_nonNull`). \n",
    "* Cramer's Value between the categorical column and labelled CTR variable (`Cramersvalue`). \n",
    "\n",
    "When sorted, we note a few things: \n",
    "* In general, it looks like columns with a lower number of distinct values tend to also have lower Cramer's coefficient with our labelled CTR variable. On the opposite end, it seems like columns with a higher number of distinct values tend to have more association with the labelled CTR variable. \n",
    "* For almost all of the categorical columns, we can see that there are fewer features that cover the majority of observations for CTR = 1 than CTR = 0. \n",
    "    * For example: For column Var25, we observe that there are 10844 categorical values (26% of all distinct values in Var25) that cover 95% or more of the observations that have a label of CTR = 1. In contrast, we see that there are 28315 categorical values (68% of all distinct values in Var25) that cover 95% or more of the observations that have a label of CTR = 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# Combine summary statistics together for all categorical data \n",
    "####### \n",
    "\n",
    "# (1) Merge distinct counts of values for each column against the cumulative summations \n",
    "ver1 = distinct_df.merge(summary_cumsum_df, how='left', on='Column')\n",
    "\n",
    "# (2) Merge output from (1) against percentage of non-null values for each categorical column. \n",
    "coverage_tomerge = pd.DataFrame([list(coverage_summary.index.values), list(coverage_summary['Coverage_nonNull'].values)]).T\n",
    "coverage_tomerge.columns=['Column','Coverage_nonNull']\n",
    "ver2 = ver1.merge(coverage_tomerge, how='left', on='Column')\n",
    "\n",
    "# (3) Merge output from (2) against the Cramer's coefficient results \n",
    "categorical_combined_df = ver2.merge(cramers_results, how='left', on='Column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>count_distinct</th>\n",
       "      <th>top_freq_CTR0_count</th>\n",
       "      <th>top_freq_CTR1_count</th>\n",
       "      <th>Coverage_nonNull</th>\n",
       "      <th>cramersvalue</th>\n",
       "      <th>pct_topfreq_CTR0</th>\n",
       "      <th>pct_topfreq_CTR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Var29</td>\n",
       "      <td>34617</td>\n",
       "      <td>23274</td>\n",
       "      <td>9441</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.315933</td>\n",
       "      <td>0.672329</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Var25</td>\n",
       "      <td>41312</td>\n",
       "      <td>28315</td>\n",
       "      <td>10844</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.312090</td>\n",
       "      <td>0.685394</td>\n",
       "      <td>0.262490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Var34</td>\n",
       "      <td>38618</td>\n",
       "      <td>26336</td>\n",
       "      <td>10208</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.310233</td>\n",
       "      <td>0.681962</td>\n",
       "      <td>0.264333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Var16</td>\n",
       "      <td>43870</td>\n",
       "      <td>30314</td>\n",
       "      <td>11370</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.307947</td>\n",
       "      <td>0.690996</td>\n",
       "      <td>0.259175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Var17</td>\n",
       "      <td>25184</td>\n",
       "      <td>16671</td>\n",
       "      <td>6950</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.299285</td>\n",
       "      <td>0.661968</td>\n",
       "      <td>0.275969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Var28</td>\n",
       "      <td>5238</td>\n",
       "      <td>2135</td>\n",
       "      <td>1826</td>\n",
       "      <td>1</td>\n",
       "      <td>0.287163</td>\n",
       "      <td>0.407598</td>\n",
       "      <td>0.348606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Var20</td>\n",
       "      <td>7623</td>\n",
       "      <td>4350</td>\n",
       "      <td>3125</td>\n",
       "      <td>1</td>\n",
       "      <td>0.270162</td>\n",
       "      <td>0.570641</td>\n",
       "      <td>0.409944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Var37</td>\n",
       "      <td>12335</td>\n",
       "      <td>6635</td>\n",
       "      <td>3449</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.269548</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.279611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Var31</td>\n",
       "      <td>2548</td>\n",
       "      <td>795</td>\n",
       "      <td>811</td>\n",
       "      <td>1</td>\n",
       "      <td>0.259051</td>\n",
       "      <td>0.312009</td>\n",
       "      <td>0.318289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Var24</td>\n",
       "      <td>3799</td>\n",
       "      <td>2000</td>\n",
       "      <td>1534</td>\n",
       "      <td>1</td>\n",
       "      <td>0.247067</td>\n",
       "      <td>0.526454</td>\n",
       "      <td>0.403790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Var26</td>\n",
       "      <td>2796</td>\n",
       "      <td>1494</td>\n",
       "      <td>1191</td>\n",
       "      <td>1</td>\n",
       "      <td>0.239850</td>\n",
       "      <td>0.534335</td>\n",
       "      <td>0.425966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Var39</td>\n",
       "      <td>9527</td>\n",
       "      <td>4198</td>\n",
       "      <td>2478</td>\n",
       "      <td>0.58529</td>\n",
       "      <td>0.232717</td>\n",
       "      <td>0.440642</td>\n",
       "      <td>0.260103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Var23</td>\n",
       "      <td>10997</td>\n",
       "      <td>5866</td>\n",
       "      <td>3702</td>\n",
       "      <td>1</td>\n",
       "      <td>0.208153</td>\n",
       "      <td>0.533418</td>\n",
       "      <td>0.336637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Var15</td>\n",
       "      <td>497</td>\n",
       "      <td>185</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>0.201925</td>\n",
       "      <td>0.372233</td>\n",
       "      <td>0.358149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Var30</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.169796</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Var36</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.124470</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Var27</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.117776</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Var32</td>\n",
       "      <td>1303</td>\n",
       "      <td>188</td>\n",
       "      <td>206</td>\n",
       "      <td>0.58529</td>\n",
       "      <td>0.104115</td>\n",
       "      <td>0.144282</td>\n",
       "      <td>0.158097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Var22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Var38</td>\n",
       "      <td>51</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0.58529</td>\n",
       "      <td>0.086958</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Var19</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.86292</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Var33</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.58529</td>\n",
       "      <td>0.045960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Var35</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18434</td>\n",
       "      <td>0.029709</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Var14</td>\n",
       "      <td>541</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033272</td>\n",
       "      <td>0.033272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Var18</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Var21</td>\n",
       "      <td>257</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031128</td>\n",
       "      <td>0.035019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column  count_distinct  top_freq_CTR0_count  top_freq_CTR1_count  \\\n",
       "15  Var29           34617                23274                 9441   \n",
       "11  Var25           41312                28315                10844   \n",
       "20  Var34           38618                26336                10208   \n",
       "2   Var16           43870                30314                11370   \n",
       "3   Var17           25184                16671                 6950   \n",
       "14  Var28            5238                 2135                 1826   \n",
       "6   Var20            7623                 4350                 3125   \n",
       "23  Var37           12335                 6635                 3449   \n",
       "17  Var31            2548                  795                  811   \n",
       "10  Var24            3799                 2000                 1534   \n",
       "12  Var26            2796                 1494                 1191   \n",
       "25  Var39            9527                 4198                 2478   \n",
       "9   Var23           10997                 5866                 3702   \n",
       "1   Var15             497                  185                  178   \n",
       "16  Var30              10                    7                    6   \n",
       "22  Var36              14                    6                    6   \n",
       "13  Var27              26                    6                    5   \n",
       "18  Var32            1303                  188                  206   \n",
       "8   Var22               3                    1                    1   \n",
       "24  Var38              51                   11                   12   \n",
       "5   Var19              12                    4                    5   \n",
       "19  Var33               4                    3                    3   \n",
       "21  Var35              11                    2                    2   \n",
       "0   Var14             541                   18                   18   \n",
       "4   Var18             145                    5                    5   \n",
       "7   Var21             257                    8                    9   \n",
       "\n",
       "   Coverage_nonNull  cramersvalue  pct_topfreq_CTR0  pct_topfreq_CTR1  \n",
       "15          0.96065      0.315933          0.672329          0.272727  \n",
       "11          0.96065      0.312090          0.685394          0.262490  \n",
       "20          0.96065      0.310233          0.681962          0.264333  \n",
       "2           0.96065      0.307947          0.690996          0.259175  \n",
       "3           0.96065      0.299285          0.661968          0.275969  \n",
       "14                1      0.287163          0.407598          0.348606  \n",
       "6                 1      0.270162          0.570641          0.409944  \n",
       "23          0.96065      0.269548          0.537900          0.279611  \n",
       "17                1      0.259051          0.312009          0.318289  \n",
       "10                1      0.247067          0.526454          0.403790  \n",
       "12                1      0.239850          0.534335          0.425966  \n",
       "25          0.58529      0.232717          0.440642          0.260103  \n",
       "9                 1      0.208153          0.533418          0.336637  \n",
       "1                 1      0.201925          0.372233          0.358149  \n",
       "16                1      0.169796          0.700000          0.600000  \n",
       "22                1      0.124470          0.428571          0.428571  \n",
       "13                1      0.117776          0.230769          0.192308  \n",
       "18          0.58529      0.104115          0.144282          0.158097  \n",
       "8                 1      0.098506          0.333333          0.333333  \n",
       "24          0.58529      0.086958          0.215686          0.235294  \n",
       "5           0.86292      0.054795          0.333333          0.416667  \n",
       "19          0.58529      0.045960          0.750000          0.750000  \n",
       "21          0.18434      0.029709          0.181818          0.181818  \n",
       "0                 1      0.000000          0.033272          0.033272  \n",
       "4                 1      0.000000          0.034483          0.034483  \n",
       "7                 1      0.000000          0.031128          0.035019  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "# Table 3.3\n",
    "########\n",
    "\n",
    "categorical_combined_df['pct_topfreq_CTR0'] = categorical_combined_df['top_freq_CTR0_count']/categorical_combined_df['count_distinct']\n",
    "categorical_combined_df['pct_topfreq_CTR1'] = categorical_combined_df['top_freq_CTR1_count']/categorical_combined_df['count_distinct']\n",
    "\n",
    "categorical_combined_df.sort_values(by=['cramersvalue','Coverage_nonNull'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Clean-up and transforming variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple options: (TBD...)\n",
    "\n",
    "# For numerical: we see 0's and also -2. \n",
    "# Option 1: Log will require non zero and non negative. We can perhaps do log(x+1) in which advantages are \n",
    "# (1) relatively easy; (2) x = 0 will map to a value of 0 even after transformation \n",
    "# Option 2: log(x)^2 only if the value itself > 2 (NTU paper here https://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf) \n",
    "\n",
    "# For categorical: \n",
    "# Things to consider: (A) Cramer's value (pick only features that hit X threshold of Cramer's value)? \n",
    "# Option 1: Take the features that cover 95% of class CTR==1 only, code everything else + Null as a \"lump all\" category. \n",
    "# Cons of Option 1: Can we treat 'null' and 'everything else' equally? \n",
    "# Option 2: If count of a specific distinct value is < 10, we ignore that value. \n",
    "# Cons of Option 2: Doesn't exactly reduce dimensionality, especially when we have distinct values that skyrocket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sample training data and convert to dataframe\n",
    "train_sample = sc.textFile('data/sample_training.txt')\\\n",
    "                 .map(lambda x: x.split('\\t'))\\\n",
    "                 .toDF().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numeric columns from string to double\n",
    "convert_cols = ['_1','_2','_3','_4','_5','_6','_7','_8','_9','_10','_11','_12','_13','_14']\n",
    "\n",
    "for col in convert_cols:\n",
    "    train_sample = train_sample.withColumn(col, train_sample[col].cast(\"double\"))\n",
    "train_sample = train_sample.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate train data for homegrown solution - select only 10000 rows and only numerical features + target \n",
    "train_sample_red = train_sample.select(convert_cols).limit(10000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----+----+------+----+----+---+-----+---+---+----+----+\n",
      "| _1| _2| _3|  _4|  _5|    _6|  _7|  _8| _9|  _10|_11|_12| _13| _14|\n",
      "+---+---+---+----+----+------+----+----+---+-----+---+---+----+----+\n",
      "|0.0|1.0|1.0| 5.0| 0.0|1382.0| 4.0|15.0|2.0|181.0|1.0|2.0|null| 2.0|\n",
      "|0.0|2.0|0.0|44.0| 1.0| 102.0| 8.0| 2.0|2.0|  4.0|1.0|1.0|null| 4.0|\n",
      "|0.0|2.0|0.0| 1.0|14.0| 767.0|89.0| 4.0|2.0|245.0|1.0|3.0| 3.0|45.0|\n",
      "+---+---+---+----+----+------+----+----+---+-----+---+---+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sample_red.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values with averages\n",
    "from pyspark.sql.functions import avg\n",
    "for col in train_sample_red.columns:\n",
    "    train_sample_red = train_sample_red.na.fill(round(train_sample_red.na.drop().agg(avg(col)).first()[0],1), [col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----+----+------+----+----+---+-----+---+---+---+----+\n",
      "| _1| _2| _3|  _4|  _5|    _6|  _7|  _8| _9|  _10|_11|_12|_13| _14|\n",
      "+---+---+---+----+----+------+----+----+---+-----+---+---+---+----+\n",
      "|0.0|1.0|1.0| 5.0| 0.0|1382.0| 4.0|15.0|2.0|181.0|1.0|2.0|1.0| 2.0|\n",
      "|0.0|2.0|0.0|44.0| 1.0| 102.0| 8.0| 2.0|2.0|  4.0|1.0|1.0|1.0| 4.0|\n",
      "|0.0|2.0|0.0| 1.0|14.0| 767.0|89.0| 4.0|2.0|245.0|1.0|3.0|3.0|45.0|\n",
      "+---+---+---+----+----+------+----+----+---+-----+---+---+---+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sample_red.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache into rdd\n",
    "train_sample_red_RDD = train_sample_red.rdd.map(lambda x: (x[0], np.array(x[1:]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data round mean of each feature.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "    Returns:\n",
    "        normedRDD - records are tuples of (y, features_array)\n",
    "    \"\"\"\n",
    "    featureMeans = dataRDD.map(lambda x: x[1]).mean()\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[1]).variance())\n",
    "\n",
    "    normedRDD = dataRDD.map(lambda x: (x[0], (x[1] - featureMeans)/featureStdev))\n",
    "\n",
    "    return normedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogLoss(dataRDD, W, regType = None, regParam=0.05):\n",
    "    \"\"\"\n",
    "    Compute log loss function.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient defaults to 0.1\n",
    "    Returns:\n",
    "        loss - (float) the regularized loss\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[1]), x[0])).cache()\n",
    "\n",
    "    # add regularization term\n",
    "    reg_term = 0\n",
    "    if regType == 'ridge':\n",
    "        reg_term = regParam*np.linalg.norm(W[1:])\n",
    "    elif regType == 'lasso':\n",
    "        reg_term = regParam*np.sum(np.abs(W[1:]))\n",
    "        \n",
    "    #broadcast model\n",
    "    #W = sc.broadcast(W) #uncomment this line when deploying it on the cloud\n",
    "    \n",
    "    # compute loss\n",
    "    loss = augmentedData.map(lambda x: x[1]*np.log(1 + np.exp(-np.dot(x[0], W))) + \\\n",
    "                             (1 - x[1])*(np.dot(x[0], W) + np.log(1 + np.exp(-np.dot(x[0], W))))).sum()\\\n",
    "                            /augmentedData.count()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def LogLoss_grad(dataRDD, W, regType = None, regParam=0.05):\n",
    "    \"\"\"\n",
    "    Compute log loss function inside gradient descent.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient defaults to 0.1\n",
    "    Returns:\n",
    "        loss - (float) the regularized loss\n",
    "    \"\"\"\n",
    "\n",
    "    # add regularization term\n",
    "    reg_term = 0\n",
    "    if regType == 'ridge':\n",
    "        reg_term = regParam*np.linalg.norm(W[1:])\n",
    "    elif regType == 'lasso':\n",
    "        reg_term = regParam*np.sum(np.abs(W[1:]))\n",
    "    \n",
    "    # compute loss\n",
    "    loss = dataRDD.map(lambda x: x[1]*np.log(1 + np.exp(-np.dot(x[0], W))) + \\\n",
    "                             (1 - x[1])*(np.dot(x[0], W) + np.log(1 + np.exp(-np.dot(x[0], W))))).sum()\\\n",
    "                            /dataRDD.count()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDUpdate(trainRDD, testRDD, W, nSteps = 20, regType = None, regParam=0.05, learningRate = 0.05, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps of regularized gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient defaults to 0.1\n",
    "        learningRate - (float) defaults to 0.1\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "        training_loss (float) training loss for new_model\n",
    "        test_loss (float) test loss for new_model\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedTrainData = trainRDD.map(lambda x: (np.append([1.0], x[1]), x[0])).cache()\n",
    "    augmentedTestData = testRDD.map(lambda x: (np.append([1.0], x[1]), x[0])).cache()\n",
    "    \n",
    "    # compute size of training sample\n",
    "    sizeTrainSample = augmentedTrainData.count()\n",
    "    \n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = W\n",
    "    #broadcast model\n",
    "    #model = sc.broadcast(W) #uncomment this line when deploying it on the cloud\n",
    "    for idx in range(nSteps):\n",
    "        # add regularization term\n",
    "        reg_term = np.zeros(len(model))\n",
    "        if regType == 'ridge':\n",
    "            reg_term = np.append(0,2*regParam*model[1:])\n",
    "        elif regType == 'lasso':\n",
    "            reg_term = np.append(0,regParam*np.sign(model[1:]))\n",
    "    \n",
    "        # compute gradient\n",
    "        grad = augmentedTrainData.map(lambda x: ((1/(1 + np.exp(-np.dot(x[0], model))) - x[1])*x[0]))\\\n",
    "               .sum()/sizeTrainSample + reg_term\n",
    "    \n",
    "        #update model parameters\n",
    "        new_model = model - learningRate*grad\n",
    "        #new_model = sc.broadcast(new_model) #uncomment this line when deploying it on the cloud\n",
    "        training_loss = LogLoss_grad(augmentedTrainData, new_model, regType=regType, regParam=regParam)\n",
    "        test_loss = LogLoss_grad(augmentedTestData, new_model, regType=regType, regParam=regParam)\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(new_model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[k for k in new_model]}\")\n",
    "        \n",
    "        model = new_model\n",
    "        #broadcast model\n",
    "        #model = sc.broadcast(new_model) #uncomment this line when deploying it on the cloud\n",
    "   \n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrediction(dataRDD, W, treshProb=0.5):\n",
    "    \"\"\"\n",
    "    Make predictions of target and compute number of: true positives, true negatives, \n",
    "    false positive, false negatives .\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        treshProb- (float) threshold probability for imputation of positive labels\n",
    "    Returns:\n",
    "        pred - (rdd) predicted targets\n",
    "        ntp - (integer) number of true positives\n",
    "        ntn - (integer) number of true negatives\n",
    "        nfp - (integer) number of false positives\n",
    "        nfn - (integer) number of false negatives\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[1]), x[0])).cache()\n",
    "    \n",
    "    # compute prediction\n",
    "    pred = augmentedData.map(lambda x: (int((1/(1 + np.exp(-np.dot(x[0], W))))>treshProb), x[1] )).cache()\n",
    "    \n",
    "    ntp = pred.map(lambda x: int((x[0]*x[1]) == 1)).sum()\n",
    "    ntn = pred.map(lambda x: int((x[0]+x[1]) == 0)).sum()\n",
    "    nfp = pred.map(lambda x: int((x[0] == 1) * (x[1] == 0))).sum()\n",
    "    nfn = pred.map(lambda x: int((x[0] == 0) * (x[1] == 1))).sum()\n",
    "   \n",
    "    return pred, ntp, ntn, nfp, nfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "normedRDD = normalize(train_sample_red_RDD).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train, validation and test sets\n",
    "train, validation, test = normedRDD.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of features\n",
    "numb_features = len(train_sample_red.columns) - 1\n",
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nSteps = 150\n",
    "regType = 'ridge'\n",
    "regParam = 0.02\n",
    "learningRate = 0.1\n",
    "\n",
    "# run gradient descent\n",
    "train_loss, test_loss, model = GDUpdate(train, validation, BASELINE, nSteps, regType=regType, \n",
    "                                        regParam=regParam, learningRate=learningRate, verbose = False)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.781360201511335\n",
      "Precision is:  0.2765957446808511\n",
      "Recall is:  0.031476997578692496\n",
      "F1 score is:  0.05652173913043479\n",
      "False positive rate is:  0.021628498727735368\n",
      "True positive rate is:  0.031476997578692496\n"
     ]
    }
   ],
   "source": [
    "# make predictions and compute metrics for treshProb = 0.5\n",
    "pred, ntp, ntn, nfp, nfn = makePrediction(validation, model[-1], 0.5)\n",
    "\n",
    "acc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'True positive rate')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate ROC curve\n",
    "tprSave = []\n",
    "fprSave = []\n",
    "for i in np.arange(0.0, 0.6, 0.1):\n",
    "    pred, ntp, ntn, nfp, nfn = makePrediction(validation, model[-1], i)\n",
    "    fprSave.append(nfp/(ntn+nfp))\n",
    "    tprSave.append(ntp/(ntp+nfn))\n",
    "\n",
    "plt.plot(fprSave, tprSave)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression will take care of the linear terms, now to account for the interaction term, we expand to include 2nd Degree polynomial features. The below equation represents the formulation.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} +  \\sum_{i=1}^n \\sum_{j=i+1}^n w_{ij} x_{i} x_{j}\n",
    "\\end{align}\n",
    "\n",
    "The challenge with solving the above equation is that the time complexity is $O(n^2)$\n",
    "\n",
    "In order to work with this, we use a matrix factorization technique for the interaction terms, inspired by Matrix factorization. We introduce a hyperpameter K which represent the latent factors for factorizing the weight vector $w_{ij}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\sum_{i=1}^{n} \\sum_{j=i+1}^n \\langle \\textbf{v}_i , \\textbf{v}_{j} \\rangle x_i x_{j}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Using the computation specified in Stephen Rendles paper, we can simplify the interaction term to the below equation.\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j}\n",
    "&= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j} - \\frac{1}{2} \\sum_{i=1}^n \\langle \\textbf{v}_i , \\textbf{v}_{i} \\rangle x_{i} x_{i}  \\\\\n",
    "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j} \\right)\\frac{1}{2}\\left( \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
    "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j}  -  \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
    "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left(\\sum_{i=1}^n v_{i,f}x_{i} \\right) \\left( \\sum_{j=1}^n v_{j,f}x_{j} \\right) - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right) \\\\\n",
    "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align}\n",
    "\n",
    "So, we can rewrite the equation to compute in $O(n)$ as\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align}\n",
    "\n",
    "#### Gradient\n",
    "\n",
    "For our classification problem, we can define the gradients as :\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial\\theta}\\hat{y}(\\textbf{x}) =\n",
    "\\begin{cases}\n",
    "1,  & \\text{if $\\theta$ is $w_0$} \\\\\n",
    "x_i, & \\text{if $\\theta$ is $w_i$} \\\\\n",
    "x_i\\sum_{j=1}^{n} v_{j,f}x_j - v_{i,f}x_{i}^2 & \\text{if $\\theta$ is $v_{i,f}$}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "#### For labels -1&1\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\hat{y} }(\\textbf{L}) =\n",
    "\\frac{d}{d \\hat{y}}\\left[ -y \\hat{y} + \\ln \\big(e^{y \\hat{y}} + 1 \\big) \\right] \n",
    "&= \\frac{1}{e^{y \\hat{y}} + 1} \\cdot  \\frac{d}{dx}\\left[e^{y \\hat{y}} + 1 \\right] - y \\\\\n",
    "&= \\frac{ye^{y \\hat{y}}}{e^{y \\hat{y}} + 1} - y\\\\\n",
    "&= \\frac{-y}{e^{y \\hat{y}} + 1}\n",
    "\\end{align}\n",
    "\n",
    "#### For labels 0,1\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\hat{y} }(\\textbf{L}) =\n",
    "\\frac{d}{d \\hat{y}}\\left[ -y \\hat{y} + \\ln \\big(e^{ \\hat{y}} + 1 \\big) \\right] \n",
    "&= \\frac{1}{e^{\\hat{y}} + 1} \\cdot  \\frac{d}{dx}\\left[e^{ \\hat{y}} + 1 \\right] - y \\\\\n",
    "&= \\frac{e^{ \\hat{y}}}{e^{\\hat{y}} + 1} - y\\\\\n",
    "&= \\frac{-y-e^{\\hat{y}}(y-1)}{e^{ \\hat{y}} + 1}\n",
    "\\end{align}\n",
    "\n",
    "The gradient of loss is defined by (by using chain rule):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta }(\\textbf{L}) = \n",
    "\\frac{\\partial}{\\partial \\hat{y} }(\\textbf{L}) \\centerdot \\frac{\\partial}{\\partial \\theta}\\hat{y}(\\textbf{x}) \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmLoss(dataRDD, w, w1,w0) :\n",
    "    \"\"\"\n",
    "    Computes the logloss given the data and model W\n",
    "    dataRDD - array of features, label\n",
    "    \"\"\"\n",
    "    w_bc = sc.broadcast(w)\n",
    "    w1_bc = sc.broadcast(w1)\n",
    "    w0_bc = sc.broadcast(w0)\n",
    "    def probability_value(x,W,W1,W0): \n",
    "        xa = np.array([x])\n",
    "        V =  xa.dot(W)\n",
    "        V_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(V*V - V_square).sum() + xa.dot(W1.T) + W0\n",
    "        return 1.0/(1.0 + np.exp(-phi))\n",
    "    \n",
    "    loss = dataRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x:  (probability_value(x[1],w_bc.value, w1_bc.value, w0_bc.value), x[0])) \\\n",
    "        .map(lambda x: (1 - 1e-12, x[1]) if x[0] == 1 else ((1e-12, x[1]) if x[0] == 0  else (x[0],x[1]))) \\\n",
    "        .map(lambda x: -(x[1] * np.log(x[0]) + (1-x[1])*np.log(1-x[0]))).mean()\n",
    "    \n",
    "    \n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmGradUpdate_v1(dataRDD, w, w1, w0, alpha, regParam, regParam1, regParam0):\n",
    "    \"\"\"\n",
    "    Computes the gradient and updates the model\n",
    "    \"\"\"\n",
    "    \n",
    "    w_bc = sc.broadcast(w)\n",
    "    w1_bc = sc.broadcast(w1)\n",
    "    w0_bc = sc.broadcast(w0)\n",
    "    rp_bc = sc.broadcast(regParam)\n",
    "    rp1_bc = sc.broadcast(regParam1)\n",
    "    rp0_bc = sc.broadcast(regParam0)\n",
    "    \n",
    "    #Gradient for interaction term\n",
    "    \n",
    "    def row_grad(x, y, W, W1, W0, regParam, regParam1, regParam0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        expnyt = np.exp(y*phi) \n",
    "        grad_loss = (-y/(1+expnyt))*(xa.T.dot(xa).dot(W) - np.diag(np.square(x)).dot(W))\n",
    "        return 2*regParam*W + grad_loss\n",
    "    \n",
    "    #Gradient for Linear term\n",
    "    def row_grad1(x, y, W, W1, W0, regParam, regParam1, regParam0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        expnyt = np.exp(y*phi)\n",
    "        grad_loss1 = (-y/(1+expnyt))*xa\n",
    "        return 2*regParam1*W1 + grad_loss1\n",
    "    \n",
    "    #Gradient for bias term\n",
    "    def row_grad0(x, y, W, W1, W0, regParam, regParam1, regParam0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        expnyt = np.exp(y*phi)\n",
    "        grad_loss0 = (-y/(1+expnyt))*1\n",
    "        return 2*regParam0*W0 +grad_loss0\n",
    "    \n",
    "   \n",
    "    \n",
    "    batchRDD = dataRDD.sample(False, 0.01, 2019)  \n",
    "    grad = batchRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x: (1, row_grad(x[1], x[0], w_bc.value, w1_bc.value, w0_bc.value, rp_bc.value,rp1_bc.value,rp0_bc.value))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    model = w - alpha * grad.values().collect()[0] \n",
    "    \n",
    "    grad1 = batchRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x: (1, row_grad1(x[1], x[0], w_bc.value, w1_bc.value, w0_bc.value, rp_bc.value,rp1_bc.value,rp0_bc.value))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    model1 = w1 - alpha * grad1.values().collect()[0]\n",
    "    \n",
    "    grad0 = batchRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x: (1, row_grad0(x[1], x[0], w_bc.value, w1_bc.value, w0_bc.value, rp_bc.value,rp1_bc.value,rp0_bc.value))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    model0 = w0 - alpha * grad0.values().collect()[0]\n",
    "    \n",
    "    return model, model1 ,model0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(trainRDD, testRDD, model, model1, model0, nSteps = 20, \n",
    "                    learningRate = 0.01, regParam = 0.01,regParam1 = 0.01,regParam0 = 0.01, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of OLS gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history, model1_history, model0_history = [], [], [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    model1 = wInit1\n",
    "    model0 = wInit0\n",
    "    for idx in range(nSteps): \n",
    "        \n",
    "        ############## YOUR CODE HERE #############\n",
    "        \n",
    "        model, model1, model0 = fmGradUpdate_v1(trainRDD, model, model1, model0, learningRate, regParam, regParam1, regParam0)\n",
    "        training_loss = fmLoss(trainRDD, model, model1, model0) \n",
    "        test_loss = fmLoss(testRDD, model, model1, model0) \n",
    "        ############## (END) YOUR CODE #############\n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        model1_history.append(model1)\n",
    "        model0_history.append(model0)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            #print(f\"Model: {[k for k in model]}\")\n",
    "   \n",
    "    return train_history, test_history, model_history, model1_history, model0_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wInitialization(dataRDD, factor):\n",
    "    nrFeat = len(dataRDD.first()[1])\n",
    "    np.random.seed(int(time.time())) \n",
    "    w =  np.random.ranf((nrFeat, factor))\n",
    "    w = w / np.sqrt((w*w).sum())\n",
    "    \n",
    "    w1 =  np.random.ranf(nrFeat)\n",
    "    w1 = w1 / np.sqrt((w1*w1).sum())\n",
    "    \n",
    "    w0 =  np.random.ranf(1)\n",
    "    \n",
    "    return w, w1, w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n",
      "training loss: 1.7613677632878466\n",
      "test loss: 1.8473588378092802\n",
      "----------\n",
      "STEP: 2\n",
      "training loss: 1.6925499864940252\n",
      "test loss: 1.77991321700028\n",
      "----------\n",
      "STEP: 3\n",
      "training loss: 1.6269341758288614\n",
      "test loss: 1.7162818108534752\n",
      "----------\n",
      "STEP: 4\n",
      "training loss: 1.5643604963061055\n",
      "test loss: 1.6557467834136312\n",
      "----------\n",
      "STEP: 5\n",
      "training loss: 1.5046773101948088\n",
      "test loss: 1.5978907116204593\n",
      "----------\n",
      "STEP: 6\n",
      "training loss: 1.4477248797385507\n",
      "test loss: 1.5424103620787777\n",
      "----------\n",
      "STEP: 7\n",
      "training loss: 1.3933928309760386\n",
      "test loss: 1.4899135792486287\n",
      "----------\n",
      "STEP: 8\n",
      "training loss: 1.3415195179925146\n",
      "test loss: 1.4398597956765486\n",
      "----------\n",
      "STEP: 9\n",
      "training loss: 1.2919984315180826\n",
      "test loss: 1.392117537489453\n",
      "----------\n",
      "STEP: 10\n",
      "training loss: 1.2446850360432156\n",
      "test loss: 1.3465727752222836\n",
      "----------\n",
      "STEP: 11\n",
      "training loss: 1.1994796214689498\n",
      "test loss: 1.3027988995213067\n",
      "----------\n",
      "STEP: 12\n",
      "training loss: 1.156252724176136\n",
      "test loss: 1.2612719919347266\n",
      "----------\n",
      "STEP: 13\n",
      "training loss: 1.114912903263196\n",
      "test loss: 1.2215946493010346\n",
      "----------\n",
      "STEP: 14\n",
      "training loss: 1.0753574090744558\n",
      "test loss: 1.1836634950873965\n",
      "----------\n",
      "STEP: 15\n",
      "training loss: 1.0374768566549053\n",
      "test loss: 1.1473791306044483\n",
      "----------\n",
      "STEP: 16\n",
      "training loss: 1.0011935162909722\n",
      "test loss: 1.1126532761440147\n",
      "----------\n",
      "STEP: 17\n",
      "training loss: 0.9664095252417222\n",
      "test loss: 1.0793985580260697\n",
      "----------\n",
      "STEP: 18\n",
      "training loss: 0.9330559502021979\n",
      "test loss: 1.0475345938000729\n",
      "----------\n",
      "STEP: 19\n",
      "training loss: 0.9010561573909133\n",
      "test loss: 1.0169855733924187\n",
      "----------\n",
      "STEP: 20\n",
      "training loss: 0.8703276393322495\n",
      "test loss: 0.9876796940421267\n",
      "----------\n",
      "STEP: 21\n",
      "training loss: 0.8408137132315278\n",
      "test loss: 0.9595497862269918\n",
      "----------\n",
      "STEP: 22\n",
      "training loss: 0.8124405497160797\n",
      "test loss: 0.9325331659719441\n",
      "----------\n",
      "STEP: 23\n",
      "training loss: 0.7851587326229817\n",
      "test loss: 0.9065700378485789\n",
      "----------\n",
      "STEP: 24\n",
      "training loss: 0.7589070858541418\n",
      "test loss: 0.8813787456575974\n",
      "----------\n",
      "STEP: 25\n",
      "training loss: 0.7336328907041012\n",
      "test loss: 0.8573600690025575\n",
      "----------\n",
      "STEP: 26\n",
      "training loss: 0.7092866695639239\n",
      "test loss: 0.8342383127360893\n",
      "----------\n",
      "STEP: 27\n",
      "training loss: 0.6858217525385962\n",
      "test loss: 0.8119678015500521\n",
      "----------\n",
      "STEP: 28\n",
      "training loss: 0.6631944369987495\n",
      "test loss: 0.7905051166280188\n",
      "----------\n",
      "STEP: 29\n",
      "training loss: 0.6413679899980731\n",
      "test loss: 0.7698095630754261\n",
      "----------\n",
      "STEP: 30\n",
      "training loss: 0.6202945027854759\n",
      "test loss: 0.749843589166555\n",
      "----------\n",
      "STEP: 31\n",
      "training loss: 0.5999420158256975\n",
      "test loss: 0.73057081903669\n",
      "----------\n",
      "STEP: 32\n",
      "training loss: 0.5802806657187833\n",
      "test loss: 0.7119577659020299\n",
      "----------\n",
      "STEP: 33\n",
      "training loss: 0.5612690107572316\n",
      "test loss: 0.693973100118417\n",
      "----------\n",
      "STEP: 34\n",
      "training loss: 0.5428853992292193\n",
      "test loss: 0.6765863780212182\n",
      "----------\n",
      "STEP: 35\n",
      "training loss: 0.5250921424216365\n",
      "test loss: 0.6597697206360559\n",
      "----------\n",
      "STEP: 36\n",
      "training loss: 0.5078708170792863\n",
      "test loss: 0.6434969590304194\n",
      "----------\n",
      "STEP: 37\n",
      "training loss: 0.49118694245703887\n",
      "test loss: 0.6277428096847687\n",
      "----------\n",
      "STEP: 38\n",
      "training loss: 0.4750206624913649\n",
      "test loss: 0.6124839221703032\n",
      "----------\n",
      "STEP: 39\n",
      "training loss: 0.4593532574457228\n",
      "test loss: 0.5975130557588642\n",
      "----------\n",
      "STEP: 40\n",
      "training loss: 0.4441544858227491\n",
      "test loss: 0.5831787526973317\n",
      "----------\n",
      "STEP: 41\n",
      "training loss: 0.4294122960417926\n",
      "test loss: 0.5692768570654239\n",
      "----------\n",
      "STEP: 42\n",
      "training loss: 0.4150988722172166\n",
      "test loss: 0.5557884996497782\n",
      "----------\n",
      "STEP: 43\n",
      "training loss: 0.40120009887350444\n",
      "test loss: 0.5426957952717504\n",
      "----------\n",
      "STEP: 44\n",
      "training loss: 0.387702639045\n",
      "test loss: 0.5299826428904569\n",
      "----------\n",
      "STEP: 45\n",
      "training loss: 0.3745817789970872\n",
      "test loss: 0.5176319175167161\n",
      "----------\n",
      "STEP: 46\n",
      "training loss: 0.361826010378941\n",
      "test loss: 0.5054729279356376\n",
      "----------\n",
      "STEP: 47\n",
      "training loss: 0.3494205441398616\n",
      "test loss: 0.4938043515117738\n",
      "----------\n",
      "STEP: 48\n",
      "training loss: 0.3373512660269507\n",
      "test loss: 0.48245591878317406\n",
      "----------\n",
      "STEP: 49\n",
      "training loss: 0.32560482505751165\n",
      "test loss: 0.4714150936466405\n",
      "----------\n",
      "STEP: 50\n",
      "training loss: 0.31416855125577753\n",
      "test loss: 0.4606692833974744\n",
      "----------\n",
      "STEP: 51\n",
      "training loss: 0.30303031713931866\n",
      "test loss: 0.45007201066834646\n",
      "----------\n",
      "STEP: 52\n",
      "training loss: 0.2921786637757295\n",
      "test loss: 0.43988280013677405\n",
      "----------\n",
      "STEP: 53\n",
      "training loss: 0.28160265116257516\n",
      "test loss: 0.42995638655087604\n",
      "----------\n",
      "STEP: 54\n",
      "training loss: 0.27129191621185783\n",
      "test loss: 0.4202815987038367\n",
      "----------\n",
      "STEP: 55\n",
      "training loss: 0.2612329300616033\n",
      "test loss: 0.4107311926987562\n",
      "----------\n",
      "STEP: 56\n",
      "training loss: 0.2514234456468267\n",
      "test loss: 0.40153288032616274\n",
      "----------\n",
      "STEP: 57\n",
      "training loss: 0.24185078435466686\n",
      "test loss: 0.39256137816569303\n",
      "----------\n",
      "STEP: 58\n",
      "training loss: 0.23250298960421453\n",
      "test loss: 0.38369943140513474\n",
      "----------\n",
      "STEP: 59\n",
      "training loss: 0.22337875560803502\n",
      "test loss: 0.37515269481790325\n",
      "----------\n",
      "STEP: 60\n",
      "training loss: 0.21446336109531267\n",
      "test loss: 0.36680884507967176\n",
      "----------\n",
      "STEP: 61\n",
      "training loss: 0.20575283252604268\n",
      "test loss: 0.3585628600969228\n",
      "----------\n",
      "STEP: 62\n",
      "training loss: 0.19724320301492293\n",
      "test loss: 0.35060227228674873\n",
      "----------\n",
      "STEP: 63\n",
      "training loss: 0.1889210955802917\n",
      "test loss: 0.34273429306992664\n",
      "----------\n",
      "STEP: 64\n",
      "training loss: 0.18078332040875117\n",
      "test loss: 0.3351291088176238\n",
      "----------\n",
      "STEP: 65\n",
      "training loss: 0.1728235265883392\n",
      "test loss: 0.3276122604932829\n",
      "----------\n",
      "STEP: 66\n",
      "training loss: 0.16503571602879277\n",
      "test loss: 0.3203402817670411\n",
      "----------\n",
      "STEP: 67\n",
      "training loss: 0.15741128878046842\n",
      "test loss: 0.31315131490923476\n",
      "----------\n",
      "STEP: 68\n",
      "training loss: 0.14995057075707535\n",
      "test loss: 0.30618892155050575\n",
      "----------\n",
      "STEP: 69\n",
      "training loss: 0.14264250937648482\n",
      "test loss: 0.2993040981760843\n",
      "----------\n",
      "STEP: 70\n",
      "training loss: 0.13548772245356838\n",
      "test loss: 0.29256760487196964\n",
      "----------\n",
      "STEP: 71\n",
      "training loss: 0.12847580186415525\n",
      "test loss: 0.28603301585190094\n",
      "----------\n",
      "STEP: 72\n",
      "training loss: 0.12160487240030689\n",
      "test loss: 0.2795700975805941\n",
      "----------\n",
      "STEP: 73\n",
      "training loss: 0.11487302958308003\n",
      "test loss: 0.2732407625048869\n",
      "----------\n",
      "STEP: 74\n",
      "training loss: 0.10826830003886315\n",
      "test loss: 0.2670376935536036\n",
      "----------\n",
      "STEP: 75\n",
      "training loss: 0.10179422670825321\n",
      "test loss: 0.26095919616827346\n",
      "----------\n",
      "STEP: 76\n",
      "training loss: 0.09544423139724025\n",
      "test loss: 0.2550491285905365\n",
      "----------\n",
      "STEP: 77\n",
      "training loss: 0.08921227884794503\n",
      "test loss: 0.2492023253433966\n",
      "----------\n",
      "STEP: 78\n",
      "training loss: 0.08309930449586732\n",
      "test loss: 0.24346814833301528\n",
      "----------\n",
      "STEP: 79\n",
      "training loss: 0.07709723008963176\n",
      "test loss: 0.2377971530720324\n",
      "----------\n",
      "STEP: 80\n",
      "training loss: 0.0712049619160266\n",
      "test loss: 0.23227657300184879\n",
      "----------\n",
      "STEP: 81\n",
      "training loss: 0.06541926812513932\n",
      "test loss: 0.2268566036771531\n",
      "----------\n",
      "STEP: 82\n",
      "training loss: 0.05973697054229603\n",
      "test loss: 0.2215353883503331\n",
      "----------\n",
      "STEP: 83\n",
      "training loss: 0.0541530308863673\n",
      "test loss: 0.21630710889146507\n",
      "----------\n",
      "STEP: 84\n",
      "training loss: 0.04866665164254738\n",
      "test loss: 0.21113849570609788\n",
      "----------\n",
      "STEP: 85\n",
      "training loss: 0.043275011933711874\n",
      "test loss: 0.20609256212610783\n",
      "----------\n",
      "STEP: 86\n",
      "training loss: 0.03797538485384991\n",
      "test loss: 0.20110246347507757\n",
      "----------\n",
      "STEP: 87\n",
      "training loss: 0.03276514764305595\n",
      "test loss: 0.1961981970961257\n",
      "----------\n",
      "STEP: 88\n",
      "training loss: 0.02764004928128419\n",
      "test loss: 0.19137697399228815\n",
      "----------\n",
      "STEP: 89\n",
      "training loss: 0.022601145261704144\n",
      "test loss: 0.18666234964989606\n",
      "----------\n",
      "STEP: 90\n",
      "training loss: 0.017641092911304643\n",
      "test loss: 0.18199911972465493\n",
      "----------\n",
      "STEP: 91\n",
      "training loss: 0.012762600248739816\n",
      "test loss: 0.1773887325332454\n",
      "----------\n",
      "STEP: 92\n",
      "training loss: 0.007960308991784204\n",
      "test loss: 0.1728750957971716\n",
      "----------\n",
      "STEP: 93\n",
      "training loss: 0.0032351687957627223\n",
      "test loss: 0.16843349548510553\n",
      "----------\n",
      "STEP: 94\n",
      "training loss: -0.0014192798697959005\n",
      "test loss: 0.16404004977302522\n",
      "----------\n",
      "STEP: 95\n",
      "training loss: -0.006000601988087776\n",
      "test loss: 0.15973455317505594\n",
      "----------\n",
      "STEP: 96\n",
      "training loss: -0.01051352286337814\n",
      "test loss: 0.15547513380939784\n",
      "----------\n",
      "STEP: 97\n",
      "training loss: -0.01495974847431328\n",
      "test loss: 0.1512813187315707\n",
      "----------\n",
      "STEP: 98\n",
      "training loss: -0.019338436850572356\n",
      "test loss: 0.1471334777066501\n",
      "----------\n",
      "STEP: 99\n",
      "training loss: -0.023653861131141967\n",
      "test loss: 0.14306346588513552\n",
      "----------\n",
      "STEP: 100\n",
      "training loss: -0.027907588099394576\n",
      "test loss: 0.13903741473149556\n",
      "\n",
      "... trained 100 iterations in 89.27388191223145 seconds\n",
      "[[ 0.08464565  0.1808374 ]\n",
      " [ 0.15520454  0.06017157]\n",
      " [ 0.1808739   0.10894884]\n",
      " [ 0.05662551  0.1027227 ]\n",
      " [ 0.4185091   0.41209689]\n",
      " [ 0.24415324  0.31864337]\n",
      " [-0.05632459 -0.03349816]\n",
      " [ 0.14687468  0.15063156]\n",
      " [ 0.21471271  0.06264041]\n",
      " [ 0.3418956   0.37199985]\n",
      " [ 0.20063693  0.21090103]\n",
      " [ 0.18942591  0.26770573]\n",
      " [-0.04896147  0.12433106]]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "wInit, wInit1, wInit0 = wInitialization(train, 2)\n",
    "logerr_train, logerr_test, models, model1s, model0s = GradientDescent(train, test, wInit, wInit1, wInit0, nSteps = 100,\n",
    "                                                    learningRate = 0.002, regParam = 0.01, regParam1 = 0.01, regParam0 = 0.01, verbose = True)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")\n",
    "print(models[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated model is: [[ 0.08464565  0.1808374 ]\n",
      " [ 0.15520454  0.06017157]\n",
      " [ 0.1808739   0.10894884]\n",
      " [ 0.05662551  0.1027227 ]\n",
      " [ 0.4185091   0.41209689]\n",
      " [ 0.24415324  0.31864337]\n",
      " [-0.05632459 -0.03349816]\n",
      " [ 0.14687468  0.15063156]\n",
      " [ 0.21471271  0.06264041]\n",
      " [ 0.3418956   0.37199985]\n",
      " [ 0.20063693  0.21090103]\n",
      " [ 0.18942591  0.26770573]\n",
      " [-0.04896147  0.12433106]]\n",
      "The loss of the estimated model is: -0.027907588099394576\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"The estimated model is: {models[-1]}\")\n",
    "print(f\"The loss of the estimated model is: {logerr_train[-1]}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(logerr_train)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmMakePrediction(dataRDD, w, w1, w0):\n",
    "    \"\"\"\n",
    "    Perform one regularized gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        pred - (rdd) predicted targets\n",
    "    \"\"\"\n",
    "    w_bc = sc.broadcast(w)\n",
    "    w1_bc = sc.broadcast(w1)\n",
    "    w0_bc = sc.broadcast(w0)\n",
    "    def predict_fm(x, W, W1, W0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        return phi\n",
    "    \n",
    "    # compute prediction\n",
    "    pred = dataRDD.map(lambda x: int(predict_fm(x[1],w_bc.value, w1_bc.value, w0_bc.value)>0.5) )\n",
    "   \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of true positives is 2182.0\n",
      "number of predicted positives is 840\n"
     ]
    }
   ],
   "source": [
    "print(f'number of true positives is {normedRDD.map(lambda x: x[0]).sum()}')\n",
    "res = fmMakePrediction(normedRDD, models[-1], model1s[-1], model0s[-1]).cache()\n",
    "print(f'number of predicted positives is {res.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Adding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The earlier section showed a logistic regression model with numerical variables only. In this section, we incrementally add categorical variables and redo the logistic regression model on a small data set. Categorical variables present a challenge because each of them can have a million different values thereby creating millions of dimensions. To get around \"the curse of dimensionality\", we looked at quite a few methods to reduce the dimensions to a manageable level. This includes frequency based dimensionality reduction and hashing techniques at a feature level and at a collection of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"The curse of dimensionality\" with categorical variables\n",
    "Below, we take a look at the number of dimensions in the train_sample dataset for each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column _15 has 541 unique categorical values\n",
      "Column _16 has 497 unique categorical values\n",
      "Column _17 has 43870 unique categorical values\n",
      "Column _18 has 25184 unique categorical values\n",
      "Column _19 has 145 unique categorical values\n",
      "Column _20 has 12 unique categorical values\n",
      "Column _21 has 7623 unique categorical values\n",
      "Column _22 has 257 unique categorical values\n",
      "Column _23 has 3 unique categorical values\n",
      "Column _24 has 10997 unique categorical values\n",
      "Column _25 has 3799 unique categorical values\n",
      "Column _26 has 41312 unique categorical values\n",
      "Column _27 has 2796 unique categorical values\n",
      "Column _28 has 26 unique categorical values\n",
      "Column _29 has 5238 unique categorical values\n",
      "Column _30 has 34617 unique categorical values\n",
      "Column _31 has 10 unique categorical values\n",
      "Column _32 has 2548 unique categorical values\n",
      "Column _33 has 1303 unique categorical values\n",
      "Column _34 has 4 unique categorical values\n",
      "Column _35 has 38618 unique categorical values\n",
      "Column _36 has 11 unique categorical values\n",
      "Column _37 has 14 unique categorical values\n",
      "Column _38 has 12335 unique categorical values\n",
      "Column _39 has 51 unique categorical values\n",
      "Column _40 has 9527 unique categorical values\n",
      "Total number of distinct categorical variables in train_sample is: 241338\n"
     ]
    }
   ],
   "source": [
    "# number of unique categorical values\n",
    "from pyspark.sql.functions import col\n",
    "distCatVarCnt = 0\n",
    "for col in train_sample.columns[14:]:\n",
    "    cnt = train_sample.select(col).distinct().count()\n",
    "    print('Column ' + col + ' has ' + str(cnt) \\\n",
    "          + ' unique categorical values')\n",
    "    distCatVarCnt += cnt\n",
    "print(\"Total number of distinct categorical variables in train_sample is:\", distCatVarCnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the number of unique categorical variables is close to 3 million just for the train_sample dataset. We need to look into reducing the dimensionality without losing too much of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Per column frequency based dimensionality reduction\n",
    "\n",
    "One possible methold is to figure out if there are any frequent values in these categorical variables and choose the top 15 of them. If they make up nearly 100% of the values, then we can lump the rest under \"Other\" and come up with 16 column bins. This is a compute intensive operation as we try to figure out the top 15 values for each column and their contribution to the total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col                                       top15_values  top15_pct_contribution\n",
      "0   14  [05db9164, 68fd1e64, 5a9ed9b0, 8cf07265, be589...                  93.788\n",
      "1   15  [38a947a1, 09e68b86, 80e26c9b, 38d50e09, 28713...                  51.167\n",
      "2   16  [, d032c263, b00d1501, 02cf9876, aa8c1539, 77f...                  20.675\n",
      "3   17  [c18be181, d16679b9, , 85dd697c, 13508380, f92...                  27.756\n",
      "4   18  [25c83c98, 4cf72387, 43b19349, 384874ce, 30903...                  98.839\n",
      "5   19  [7e0ccccf, fbad5c96, , fe6b92e5, 13718bbd, 6f6...                 100.000\n",
      "6   20  [3f4ec687, 49b74ebc, 7195046d, 970f01b2, 88002...                   9.373\n",
      "7   21  [0b153874, 5b392875, 1f89b562, 37e4aa92, 062b5...                  97.351\n",
      "8   22                     [a73ee510, 7cc72ec2, a18233ea]                 100.000\n",
      "9   23  [3b08e48b, fbbf2c95, efea433b, 6c47047a, 0e9ea...                  34.346\n",
      "10  24  [c4adf918, 7f8ffe57, e51ddf94, 4d8549da, f25fe...                  13.919\n",
      "11  25  [, dfbb09fb, e0d76380, 8fe001f4, d8c29807, 9f3...                  21.235\n",
      "12  26  [85dbe138, 46f42a63, 3516f6e6, 51b97b8f, 80467...                  16.194\n",
      "13  27  [07d13a8f, b28479f6, 1adce6ef, 64c94865, cfef1...                  99.644\n",
      "14  28  [36721ddc, 52baadf5, 42b3012c, 10040656, dbc5e...                  13.778\n",
      "15  29  [, 84898b2a, 1203a270, 36103458, 31ca40b6, c64...                  21.940\n",
      "16  30  [e5ba7672, 07c540c4, d4bb7bd8, 3486227d, 776ce...                 100.000\n",
      "17  31  [5aed7436, 891589e7, e88ffc9d, 582152eb, 7ef5a...                  29.133\n",
      "18  32  [, 21ddcdc9, 55dd3565, cf99e5de, 9437f62f, 1d1...                  84.109\n",
      "19  33                   [, 5840adea, a458ea53, b1252a9d]                 100.000\n",
      "20  34  [, 0014c32a, 73d06dde, e587c466, 5f957280, dfc...                  21.631\n",
      "21  35  [, ad3062eb, c9d4222a, 8ec974f4, 78e2e389, c00...                 100.000\n",
      "22  36  [32c7478e, 3a171ecb, 423fab69, be7c41b4, bcdee...                 100.000\n",
      "23  37  [1793a828, 3fdb382b, 3b183c5c, aee52b6f, , b34...                  41.412\n",
      "24  38  [, e8b83407, 001f3601, ea9a246c, 010f6491, cb0...                  97.321\n",
      "25  39  [, 49d68486, c84c4aec, 2fede552, 984e0db0, b7d...                  59.652\n"
     ]
    }
   ],
   "source": [
    "train_sample_rdd = sc.textFile('data/sample_training.txt')\n",
    "testRdd = sc.parallelize(train_sample_rdd.take(100000))\n",
    "\n",
    "def getTop15FreqCols (rdd):\n",
    "    \n",
    "    #First get a count of rdd length\n",
    "    rddLen = rdd.count()\n",
    "\n",
    "    #Create pandas dataframe to store the top 15 values\n",
    "    top15df = pd.DataFrame(columns=['col', 'top15_values', 'top15_pct_contribution'])\n",
    "\n",
    "    for col in range(14,40):\n",
    "        catRdd = rdd.map(lambda x: x.split('\\t')[col]) \\\n",
    "                    .map(lambda x: (x,1)) \\\n",
    "                    .reduceByKey(lambda x,y: x + y)\n",
    "    \n",
    "        freqRecord = catRdd.takeOrdered(15, key=lambda x: -x[1])\n",
    "        freqList = []\n",
    "        freqCnt = 0\n",
    "        for (k,v) in freqRecord:\n",
    "            freqCnt += v\n",
    "            freqList.append(str(k))\n",
    "        top15df = top15df.append({'col': col, 'top15_values': freqList, 'top15_pct_contribution': 100*freqCnt/rddLen}, ignore_index=True)\n",
    "    \n",
    "    return(top15df)\n",
    "\n",
    "top15df = getTop15FreqCols(train_sample_rdd)\n",
    "\n",
    "#top15df = top15df.set_index('col')\n",
    "    \n",
    "print(top15df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, we can see that the categorical variables in some of the columns account for almost 100% of all values. These include columns 14, 18, 19, 21, 22, 27, 30, 32, 33, 35, 36 and 38. For these 11 columns, it would make sense to keep the top 15 values and lump everything else under an \"Other\" column.\n",
    "\n",
    "For the rest of the columns that don't exhibit this behavior, it might make sense to look at other strategies such as hashing to reduce dimensionality. Before we take this approach of using both the frequency related information and hashing, let's first take a look at hashing all columns next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Per column Feature Hashing\n",
    "There are 26 categorical features in this model represented by strings that are 8 bytes long. So, theoretically, each string can take $2^{64} -1$ different values and lead to that many dimensions. We need to have far fewer dimensions so that we can make the problem computationally achievable and as well lead to a generalized algorithm as well. One way of achieving this is through what is popularly called the \"hashing trick\" (provide references). \n",
    "\n",
    "A simple way to reduce dimensionality is to hash the 8 byte long strings into, say 16 or 32 groups. We used the murmurHash3 hashing which is generally the preferred way of hashing strings (provide references and more details). Hashing leads to collisions as many strings could end up hashing to the same hash value. However, it has been proven (references) that even with collisions, hashing leads to very generalized models.\n",
    "\n",
    "One the categorical variables are hashed down to, say 16 values, they are then 1-hot encoded and fed into the logistic regression model. This section presents the results with the inclusion of 27 categorical variables, each individually hashed to 16 values, in addition to the numerical variables in the earlier section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: murmurhash3 in /opt/anaconda/lib/python3.6/site-packages (2.3.5)\n",
      "\u001b[31mtwisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install murmurhash3 if needed\n",
    "!pip install murmurhash3\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the complete dataset\n",
    "train_sample_rdd = sc.textFile('data/sample_training.txt')\n",
    "\n",
    "#Get the top 1000 rows only (as before for numerical variables)\n",
    "testRdd = sc.parallelize(train_sample_rdd.take(10000),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashTrans (rdd):\n",
    "    \n",
    "\n",
    "    def createHash (elem,hlen):\n",
    "        import mmh3\n",
    "        hashStr = []\n",
    "        for str in elem:\n",
    "            hashStr.append(mmh3.hash(str) % int(hlen))\n",
    "        return(hashStr)\n",
    "\n",
    "    def create1Hot(elem, hlen):\n",
    "        oneHotStr = []\n",
    "        #for hashStr in elem:\n",
    "        for i in range (hlen):\n",
    "            if (i == elem):\n",
    "                oneHotStr.append(1)\n",
    "            else:\n",
    "                oneHotStr.append(0)\n",
    "        return(oneHotStr)\n",
    "\n",
    "    def createCatArray(elem):\n",
    "        catArray = []\n",
    "        for array in elem:\n",
    "            for x in array:\n",
    "                catArray.append(x)\n",
    "        return(np.array(catArray))\n",
    "\n",
    "    #Define murmurHash level for 1-hot encoding\n",
    "    HASHLEN = 16\n",
    "\n",
    "    #testRdd.map(lambda x : x.split('\\t')[14:40]).map(lambda x: [mmh3.hash(xn)%16 for xn in x]).take(5)\n",
    "    categoricalRdd = rdd.map(lambda x : x.split('\\t')[14:40]) \\\n",
    "                        .map(lambda x: createHash(x,HASHLEN)) \\\n",
    "                        .map(lambda x: [create1Hot(xn, HASHLEN) for xn in x]) \\\n",
    "                        .map(createCatArray)\n",
    "    return(categoricalRdd)\n",
    "\n",
    "categoricalRdd = hashTrans(testRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, array([-0.45879823, -0.24774127, -0.04629014, -0.85019922, -0.24482546,\n",
       "         -0.40934022, -0.02096037, -0.63143545,  0.18286248,  0.7486028 ,\n",
       "         -0.10449796,  0.01324891, -0.49600354,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ]))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now include this into the logistic regression model\n",
    "#Zip numerical and categorical variables\n",
    "\n",
    "def mergeNumPlusCatRdds(elem):\n",
    "    x, y = elem\n",
    "    xkey , xval = x\n",
    "    merge =  (xkey, np.hstack((xval,y)))\n",
    "    return(merge)\n",
    "\n",
    "numPlusCatRdd = normedRDD.zip(categoricalRdd) \\\n",
    "                         .map(mergeNumPlusCatRdds)\n",
    "numPlusCatRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n"
     ]
    }
   ],
   "source": [
    "#compute the number of features\n",
    "numb_features = np.size(numPlusCatRdd.first()[1])\n",
    "print(numb_features)\n",
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train, validation and test sets\n",
    "train, validation, test = numPlusCatRdd.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nSteps = 150\n",
    "regType = 'ridge'\n",
    "regParam = 0.1\n",
    "learningRate = 0.1\n",
    "\n",
    "# run gradient descent\n",
    "train_loss, test_loss, model = GDUpdate(train, validation, BASELINE, nSteps, regType=regType, \n",
    "                                        regParam=regParam, learningRate=learningRate, verbose = False)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2fc28f4192e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# make predictions and compute metrics for treshProb = 0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakePrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mntp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mntn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mntn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnfp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mntp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# make predictions and compute metrics for treshProb = 0.5\n",
    "pred, ntp, ntn, nfp, nfn = makePrediction(validation, model[-1], 0.5)\n",
    "\n",
    "acc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n",
      "training loss: 2.321231475285203\n",
      "test loss: 2.389359169703438\n",
      "----------\n",
      "STEP: 2\n",
      "training loss: 1.8230748504689207\n",
      "test loss: 1.8742586446728218\n",
      "----------\n",
      "STEP: 3\n",
      "training loss: 1.4650617121411433\n",
      "test loss: 1.5039468464130261\n",
      "----------\n",
      "STEP: 4\n",
      "training loss: 1.1929069936879537\n",
      "test loss: 1.2223084946474805\n",
      "----------\n",
      "STEP: 5\n",
      "training loss: 0.9782992272573738\n",
      "test loss: 1.0000743202485511\n",
      "----------\n",
      "STEP: 6\n",
      "training loss: 0.804743779455009\n",
      "test loss: 0.8202008832485425\n",
      "----------\n",
      "STEP: 7\n",
      "training loss: 0.66174245110325\n",
      "test loss: 0.671849163628504\n",
      "----------\n",
      "STEP: 8\n",
      "training loss: 0.5421756056450527\n",
      "test loss: 0.5476738486068338\n",
      "----------\n",
      "STEP: 9\n",
      "training loss: 0.4409950870530131\n",
      "test loss: 0.442469774456338\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "wInit, wInit1, wInit0 = wInitialization(train, 2)\n",
    "#print(wInit)\n",
    "logerr_train, logerr_test, models_cat, model1s_cat, model0s_cat = GradientDescent(train, test, wInit,wInit1, wInit0, nSteps = 100,\n",
    "                                                    learningRate = 0.001, regParam = 0.01, regParam1 = 0.01, regParam0 = 0.01, verbose = True)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"The estimated model is: {models[-1]}\")\n",
    "print(f\"The loss of the estimated model is: {logerr_train[-1]}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(logerr_train)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of true positives is {numPlusCatRdd.map(lambda x: x[0]).sum()}')\n",
    "res = fmMakePrediction(numPlusCatRdd, models_cat[-1], model1s_cat[-1], model0s_cat[-1]).cache()\n",
    "print(f'number of predicted positives is {res.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Multi-column Feature Hashing\n",
    "Hashing each column independently resulted in a dimensionality reduction from several million vectors to 416 for the categorical variables. We could further reduce dimensionality through multi-column Feature Hashing with a likely tradeoff being the loss of accuracy (TBD). \n",
    "\n",
    "We used the multi-column FeatureHashing functionality in Apache Spark MLLib to look into how dimensionality reduction to fewer vectors for logistic regression. Multi-column FeatureHashing would be very handy when we start looking at adding in quadratic terms for logistic regression. Instead of a (416,416) quadratic feature matrix, we could deal with a smaller (32,32) or (64,64) features, leading to faster computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark MLLib FeatureHasher example\n",
    "The FeatureHasher in spark takes multiple columns of categorical (and numerical) variables and hashes them down to fewer features. It is possible to specify the number of output features so that we can restrict the dimensions to a more manageable number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature hashing example\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "dataset = spark.createDataFrame([\n",
    "    (2.2, True, \"1\", \"foo\"),\n",
    "    (3.3, False, \"2\", \"bar\"),\n",
    "    (4.4, False, \"3\", \"baz\"),\n",
    "    (5.5, False, \"4\", \"foo\")\n",
    "], [\"real\", \"bool\", \"stringNum\", \"string\"])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-column Feature Hashing with test set\n",
    "We applied Multi-column Feature Hashing to get to a dimensionality of 64 vectors using FeatureHasher. The results of the logistic regression following this hashing are given below (need to add time, accuracy etc...How do you compare otherwise?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sample training data and convert to dataframe\n",
    "train_sample1 = sc.textFile('data/sample_training.txt')\\\n",
    "                 .map(lambda x: x.split('\\t'))\\\n",
    "                 .toDF().limit(10000).cache()\n",
    "train_sample1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose categorical columns to Hash\n",
    "hashInpList = []\n",
    "for c in range(14,41):\n",
    "    col = \"_\"+str(c)\n",
    "    hashInpList.append(col)\n",
    "print (hashInpList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Multi-column hashing with 32 output features\n",
    "hasher = FeatureHasher(numFeatures=256, inputCols=hashInpList,outputCol=\"features\")\n",
    "\n",
    "featurized = hasher.transform(train_sample1)\n",
    "featurized.select(\"features\").show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized.show(3)\n",
    "featurized.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into a dense vector format to feed logistic regression model\n",
    "def extractVec (elem):\n",
    "    return(np.array(tuple(elem.features.toArray().tolist())))\n",
    "    \n",
    "multiHashCatRdd = featurized.select(\"features\").rdd.map(extractVec)\n",
    "#map(extractVec)\n",
    "multiHashCatRdd.take(3)\n",
    "multiHashCatRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now include this into the logistic regression model\n",
    "#Zip numerical and categorical variables\n",
    "\n",
    "def mergeNumPlusCatRdds(elem):\n",
    "    x, y = elem\n",
    "    xkey , xval = x\n",
    "    merge =  (xkey, np.hstack((xval,y)))\n",
    "    return(merge)\n",
    "\n",
    "numPlusCatRdd2 = normedRDD.zip(multiHashCatRdd) \\\n",
    "                         .map(mergeNumPlusCatRdds)\n",
    "numPlusCatRdd2.take(1)\n",
    "numPlusCatRdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of features\n",
    "numb_features = np.size(numPlusCatRdd2.first()[1])\n",
    "print(numb_features)\n",
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train, validation and test sets\n",
    "train, validation, test = numPlusCatRdd2.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nSteps = 150\n",
    "regType = 'ridge'\n",
    "regParam = 0.1\n",
    "learningRate = 0.1\n",
    "\n",
    "# run gradient descent\n",
    "train_loss, test_loss, model = GDUpdate(train, validation, BASELINE, nSteps, regType=regType, \n",
    "                                        regParam=regParam, learningRate=learningRate, verbose = False)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.8019559902200489\n",
      "Precision is:  0.5\n",
      "Recall is:  0.009876543209876543\n",
      "F1 score is:  0.01937046004842615\n",
      "False positive rate is:  0.0024390243902439024\n",
      "True positive rate is:  0.009876543209876543\n"
     ]
    }
   ],
   "source": [
    "# make predictions and compute metrics for treshProb = 0.5\n",
    "pred, ntp, ntn, nfp, nfn = makePrediction(validation, model[-1], 0.5)\n",
    "\n",
    "acc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "wInit, wInit1, wInit0 = wInitialization(train, 2)\n",
    "#print(wInit)\n",
    "logerr_train, logerr_test, models_cat2, model1s_cat2, model0s_cat2 = GradientDescent(train, validation, wInit,wInit1, wInit0, nSteps = 100,\n",
    "                                                    learningRate = 0.001, regParam = 0.01, regParam1 = 0.01, regParam0 = 0.01, verbose = True)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The estimated model is: {models[-1]}\")\n",
    "print(f\"The loss of the estimated model is: {logerr_train[-1]}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(logerr_train)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of true positives is {numPlusCatRdd2.map(lambda x: x[0]).sum()}')\n",
    "res = fmMakePrediction(numPlusCatRdd2, models_cat2[-1], model1s_cat2[-1], model0s_cat2[-1]).cache()\n",
    "print(f'number of predicted positives is {res.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Combining Frequency and Hash Based Dimensionality Reduction\n",
    "For some categorical features, it makes sense to use a frequency based approach, while hashing is useful in other cases. 12 of the columns were amenable to frequency based approaches since 90% or more of the values were accounted for by the top 15 frequent values. The hybrid approach uses frequencies to encode these columns, while using the hash only for the other 14 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hybridFreqHashTrans (top15df, rdd):\n",
    "\n",
    "    #First create Broadcast variable\n",
    "    top15dfB = sc.broadcast(top15df)\n",
    "    \n",
    "    #set FreqThr at 90% for using frequency to bin the categories\n",
    "    TOP15_FREQ_THR = 90\n",
    "\n",
    "    #Define murmurHash level for 1-hot encoding\n",
    "    HASHLEN = 16\n",
    "    \n",
    "    #top15df = pd.DataFrame(columns=['col', 'top15_values', 'top15_pct_contribution'])\n",
    "    \n",
    "    def transformRow (row, top15df, hlen):\n",
    "        col = 0\n",
    "        transformedRow = []\n",
    "        \n",
    "        for x in row:\n",
    "            #Get frequency based map\n",
    "            if (top15df.loc[col]['top15_pct_contribution'] > 90):\n",
    "                xtrans = 15\n",
    "                for (ind,topElem) in enumerate(top15df.loc[col]['top15_values']):\n",
    "                    if (x == topElem):\n",
    "                        xtrans = ind\n",
    "            #else hash\n",
    "            else:\n",
    "                xtrans = mmh3.hash(x+str(col)) % int(hlen)\n",
    "        \n",
    "            transformedRow.append(xtrans)\n",
    "            \n",
    "            #Do this for all elements\n",
    "            col += 1\n",
    "        return (transformedRow)\n",
    "\n",
    "    def create1Hot(elem, hlen):\n",
    "        oneHotStr = []\n",
    "        #for hashStr in elem:\n",
    "        for i in range (hlen):\n",
    "            if (i == elem):\n",
    "                oneHotStr.append(1)\n",
    "            else:\n",
    "                oneHotStr.append(0)\n",
    "        return(oneHotStr)\n",
    "\n",
    "    def createCatArray(elem):\n",
    "        catArray = []\n",
    "        for array in elem:\n",
    "            for x in array:\n",
    "                catArray.append(x)\n",
    "        return(np.array(catArray))\n",
    "\n",
    "\n",
    "\n",
    "    categoricalRdd = rdd.map(lambda x : x.split('\\t')[14:40]) \\\n",
    "                        .map(lambda x: transformRow(x, top15dfB.value, HASHLEN)) \\\n",
    "                        .map(lambda x: [create1Hot(xn, HASHLEN) for xn in x]) \\\n",
    "                        .map(createCatArray)\n",
    "    return(categoricalRdd)\n",
    "\n",
    "#Read in the complete dataset\n",
    "train_sample_rdd = sc.textFile('data/sample_training.txt')\n",
    "\n",
    "#Get the top 1000 rows only (as before for numerical variables)\n",
    "testRdd = sc.parallelize(train_sample_rdd.take(10000),1)\n",
    "\n",
    "hybridCatRdd = hybridFreqHashTrans(top15df, testRdd)\n",
    "\n",
    "hybridCatRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, array([-0.45879823, -0.24774127, -0.04629014, -0.85019922, -0.24482546,\n",
       "         -0.40934022, -0.02096037, -0.63143545,  0.18286248,  0.7486028 ,\n",
       "         -0.10449796,  0.01324891, -0.49600354,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ]))]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now include this into the logistic regression model\n",
    "#Zip numerical and categorical variables\n",
    "\n",
    "def mergeNumPlusCatRdds(elem):\n",
    "    x, y = elem\n",
    "    xkey , xval = x\n",
    "    merge =  (xkey, np.hstack((xval,y)))\n",
    "    return(merge)\n",
    "\n",
    "numPlusCatRdd3 = normedRDD.zip(hybridCatRdd) \\\n",
    "                         .map(mergeNumPlusCatRdds)\n",
    "numPlusCatRdd3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n"
     ]
    }
   ],
   "source": [
    "#compute the number of features\n",
    "numb_features = np.size(numPlusCatRdd3.first()[1])\n",
    "print(numb_features)\n",
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train, validation and test sets\n",
    "train, validation, test = numPlusCatRdd3.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nSteps = 150\n",
    "regType = 'ridge'\n",
    "regParam = 0.1\n",
    "learningRate = 0.1\n",
    "\n",
    "# run gradient descent\n",
    "train_loss, test_loss, model = GDUpdate(train, validation, BASELINE, nSteps, regType=regType, \n",
    "                                        regParam=regParam, learningRate=learningRate, verbose = False)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.7901541521631029\n",
      "Precision is:  0.47619047619047616\n",
      "Recall is:  0.023752969121140142\n",
      "F1 score is:  0.04524886877828054\n",
      "False positive rate is:  0.006918238993710692\n",
      "True positive rate is:  0.023752969121140142\n"
     ]
    }
   ],
   "source": [
    "# make predictions and compute metrics for treshProb = 0.5\n",
    "pred, ntp, ntn, nfp, nfn = makePrediction(validation, model[-1], 0.5)\n",
    "\n",
    "acc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOY EXAMPLE - comparison with ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOY EXAMPLE STARTS HERE\n",
    "numb_features = 2\n",
    "\n",
    "N = 100\n",
    "D = 3\n",
    "\n",
    "X_toy = np.random.randn(N,D)\n",
    "\n",
    "# center the first 50 points at (-1,-1)\n",
    "X_toy[:50,:] = X_toy[:50,:] - 1*np.ones((50,D))\n",
    "\n",
    "# center the last 50 points at (2, 2)\n",
    "X_toy[50:,:] = X_toy[50:,:] + 2*np.ones((50,D))\n",
    "\n",
    "X_toy[:50,0] = 0\n",
    "X_toy[50:,0] = 1\n",
    "\n",
    "rdd1 = sc.parallelize(X_toy)\n",
    "rdd1 = rdd1.map(lambda x: [float(i) for i in x])\n",
    "toy_sample_red = rdd1.toDF([\"_1\", \"_2\", \"_3\"])\n",
    "toy_sample_red_RDD = toy_sample_red.rdd.map(lambda x: (x[0], x[1:])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSteps = 150\n",
    "learningRate = 0.5\n",
    "regType = 'ridge'\n",
    "regParam = 0.05\n",
    "\n",
    "Loss_save = []\n",
    "Model_norm = []\n",
    "#broadcast model\n",
    "model = BASELINE\n",
    "for idx in range(nSteps):\n",
    "    #print(\"----------\")\n",
    "    #print(f\"STEP: {idx+1}\")\n",
    "    \n",
    "    # compute loss\n",
    "    loss = LogLoss(toy_sample_red_RDD, model, regType=regType, regParam=regParam)\n",
    "    # update model parameters\n",
    "    model = GDUpdate(toy_sample_red_RDD, model, regType=regType, regParam=regParam, learningRate=learningRate)\n",
    "    \n",
    "    #store results\n",
    "    Loss_save.append(loss)\n",
    "    Model_norm.append(np.linalg.norm(model))\n",
    "\n",
    "print(f\"The estimated model is: {model}\")\n",
    "print(f\"The loss of the estimated model is: {loss}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(Loss_save)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.plot(Model_norm)\n",
    "plt.title('Norm of vector of parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual simulated value is 50\n",
    "res = makePrediction(toy_sample_red_RDD, model).cache()\n",
    "res.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = X_toy[:,1:],X_toy[:,0]\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',fit_intercept=True).fit(X, y)\n",
    "print(clf.coef_, clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_toy[:,1], X_toy[:,2], c=X_toy[:,0], s=100, alpha=0.5)\n",
    "x_axis = np.linspace(-6, 6, 100)\n",
    "y_axis = -(model[0] + x_axis*model[1]) / model[2]\n",
    "plt.plot(x_axis, y_axis)\n",
    "y_axis = -(clf.intercept_+x_axis*clf.coef_[0][0]) / clf.coef_[0][1]\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_cols = ['_1','_2','_3','_4','_5','_6','_7','_8','_9','_10','_11','_12','_13','_14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate train data for homegrown solution - select only 1000 rows and only numerical features + one categorical variable +target \n",
    "train_sample_red = train_sample.select(convert_cols + ['_23']).limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_red.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values with averages\n",
    "from pyspark.sql.functions import avg\n",
    "for col in convert_cols:\n",
    "    train_sample_red = train_sample_red.na.fill(round(train_sample_red.na.drop().agg(avg(col)).first()[0],1), [col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "df = train_sample_red.withColumn(\"_23\", split(col(\"_23\"),\" \"))\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cat_Vectorizer = CountVectorizer(inputCol=\"_23\", outputCol=\"_23_array\", vocabSize=4, minDF=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catVectorizer_model = cat_Vectorizer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe = catVectorizer_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StringIndexer(inputCols=[\"gender\"], outputCols=[\"gender_numeric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cardinality of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique categorical values\n",
    "from pyspark.sql.functions import col\n",
    "for col in train_sample.columns[14:]:\n",
    "    print('Column ' + col + ' has ' + str(train_sample.select(col).distinct().count()) \\\n",
    "          + ' unique categorical values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plots of selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of target variable\n",
    "hist_c1 = train_sample.select('_1').rdd.flatMap(lambda x: x).histogram(2)\n",
    "pd.DataFrame(list(zip(*hist_c1))).set_index(0).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 20\n",
    "hist_c20 = train_sample.groupBy('_20').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c20))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 23\n",
    "hist_c23 = train_sample.groupBy('_23').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c23))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 28\n",
    "hist_c28 = train_sample.groupBy('_28').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c28))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 31\n",
    "hist_c31 = train_sample.groupBy('_31').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c31))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 34\n",
    "hist_c34 = train_sample.groupBy('_34').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c34))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 36\n",
    "hist_c36 = train_sample.groupBy('_36').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c36))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimates of ctr based on category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 20\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_20').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_20').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 23\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_23').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_23').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 28\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_28').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_28').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 31\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_31').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_31').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 34\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_34').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_34').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 36\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_36').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_36').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
