{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team 24   \n",
    "Vivian Lu, Siddhartha Jakkamreddy, Venky Nagapudi, Luca Garre   \n",
    "Summer 2019, sections 4 and 5   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Introduction__\n",
    "Online ad is a multibillion dollar industry fueled by large investments and ever increasing performance goals. Targeted advertisement based on users' browsing industry and demographic, ad features such as overall appearance, employed colors and text, and website features such as ad's relative placement in the webpage, sizes, etc., is receiving more and more interest due to its potential for revenue generation. In this context, machine learning is proving resourceful in the understanding of the features that mostly affect users' Click-Through Rates (CTR) and, based on this understanding, in informing the design of ads that maximize performance metrics such as click and convertion rates. Further, machine learning solutions can easily be deployed in a data pipeline enviroment in order to select and offer, on a user-specific basis, the ad which expectedly maximizes the user's interest. \n",
    "\n",
    "...\n",
    "\n",
    "## __Goal of the analysis__\n",
    "The purpose of the present analysis is to estimate whether a given ad will be clicked based on a set of features describing the ad. \n",
    "\n",
    "...\n",
    "\n",
    "## __Description of the dataset__\n",
    "The dataset is provided by __[put_reference_to_CriteoLabs]__ and is composed of three files, a `readme.txt`, a `train.txt` and a `test.txt` file, respectively. The readme file contains a brief description of the data. The `train.txt` and `test.txt` files contain the train and test data. Both files are formatted as tab separated value tables, and amount to 45840617 and 6042135 rows for the train and test data, respectively. Following the description of the data, each row represents an ad and contains the following fields (see commands below, these expect the data to be contained in a data folder inside the current working directory):\n",
    "\n",
    "- 1 binary field indicating whether the ad has been clicked (1) or not (0). This field is available only for the train data;\n",
    "- 13 fields containing integer features representing counts;\n",
    "- 26 categorical features. These are hashed as 32 bits keys for anonymization purposes;\n",
    "\n",
    "From a printout of the first rows of the data files it appears that the data contain no headers. This implies that, with the sole exception of the first binary field, it is not possible to characterize the various fields in terms of the features these represent. It is also noted that rows in the data can have missing values. This is again noticed when looking at the printed lines, as these have a number of entries which is lower than the number of fields specified in the `readme.txt` file. \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows in the train data\n",
    "!wc -l data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows in the test data\n",
    "!wc -l data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row of the train data\n",
    "!head -1 data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row of the test data\n",
    "!head -1 data/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression belongs to the family of so-called generalized linear models and is by far one of the most known and applied algorithms for the prediction of a target variable $Y$, which represents the possible occurrence of an event of interest $e$. This variable is binary, and usually is encoded such that $Y=1$ represents the occurrence of $e$. More specifically, given a set of explanatory features $X_i$, $i = 1,2, \\dots, n$, logistic regression characterizes the probability of occurrence of $e$, $\\pi[e] \\equiv \\pi$, as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi = \\frac{1}{1 + \\exp^{-z} }\n",
    "\\end{equation}\n",
    "\n",
    "where $z = \\beta_0 + \\sum_{i=1}^n \\beta_i X_i$ and $\\beta_i$ are model parameters. As can be seen from the equation above, and in compliance with probability rules, $\\pi \\in (0, 1)$ for any $\\beta_i$ and $X_i$, owing to the fact that the exponential function is strictly positive, and considering that the denominator is always higher than the numerator. After some algebraic manipulations an equivalent, and more compact, formulation of the above equation can be obtained as:\n",
    "\n",
    "\\begin{equation}\n",
    "log\\left( \\frac{\\pi}{1-\\pi} \\right) = \\beta_0 + \\sum_{i=1}^n \\beta_i X_i\n",
    "\\end{equation}\n",
    "\n",
    "where the left side is usually referred to as the logit function, $logit(\\pi)$, while the right side makes the linear nature of this model explicit. This becomes clearer when considering the decision boundary, i.e., the hypersurface that segments the feature space in positive versus vegative regions. For logistic regression, such boundary is associated with the locus of points in the feature space where $\\pi=0.5$, i.e., the model has no preference as to whether a point in this locus should be assigned to the positive or the negative class. Casting $\\pi=0.5$ in the left side of the equation above renders a linear equation of the decision boundary in the feature space, in compliance with the linear nature of this model.  \n",
    "\n",
    "## Log-loss function and parameter estimation\n",
    "\n",
    "In accordance with established practices in the fields of statistics and machine learning, the parameters $\\beta_i$ of the logistic regression model are estimated via maximization of the log-likelihood function. In essence, for a sample of $m$ data points $(x_{ij}, y_j)$, $i = 1,2,\\dots,n$, $j = 1,2,\\dots,m$, where $x_{ij}$ is the $j$-th record of the $i$-th feature, and $y_j$ is the $j$-th record of the target binary variable $Y$, the parameters $\\beta_i$ are estimated such that the log-likelihood function:\n",
    "\n",
    "\\begin{equation}\n",
    "log\\left[ L(\\beta_i|y_j) \\right] = \\frac{1}{m} log\\left( \\prod_{j = 1}^{m} \\pi_j^{y_j} \\left( 1-\\pi_j \\right)^{1-y_j} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "is maximized. The term in the multiplication corresponds to the likelihood function of the Bernoulli distribution for the (degenerate) case of one single trial and number of successes $y_j = 1$ and $y_j = 0$ for success and failure, respectively.\n",
    "\n",
    "Operationally, the above maximization is usually achieved taking the negative of the log-likelihood function and computing the parameters $\\beta_i$ as the argmin of the negated log-likelihood which, after some manipulations, can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{L} = -log\\left[ L(\\beta_i|y_j) \\right] = - \\frac{1}{m}\\sum_{j=1}^{m} \\left[ y_j \\cdot log(\\pi_j) + (1-y_j) \\cdot log(1-\\pi_j) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "The right term of the equation, also called Cross-Entropy or log-loss, being a function $\\pi$, is ultimately a function of the parameters $\\beta_i$ and the features $X_i$ through the logistic regression relationship. The log-loss gives some insights as to the role of this function during estimation of the parameters. Let us assume that for a certain data point, $(x_{ij}, y_j)$, the target variable is equal to $1$. For this given data point, the right term of the equation simplifies to $-log(\\pi_j)$. Since this term needs to be minimized, the parameters $\\beta_i$ of the model need to be chosen such that $\\pi_j$ approaches $1$ as closely as possible. Conversely for an observation $y_j = 0$, minimization of the log-loss, $-log(1 - \\pi_j)$, requires $\\pi_j$ to approach $0$. This dual role of the log-loss function makes such that likelihood maximization in logistic regression aims to find the set of model parameters which best separate positive from negative observations in the space of the explanatory features $X_i$, in the sense of mapping as closely as possible positive targets to $\\pi = 1$ and negative targets to $\\pi = 0$. Another appealing property, which turns out to the be of paramount importance for the strategy outlined below, is that this log-loss function is convex, i.e., one and only one point of minimum exists in the space of parameters $\\beta_i$.\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Finding the optimum set of parameters requires a suitable optimization framework. Among various approaches, gradient descent of $\\hat{L}$ is a well-established approach for functions. For a certain point of the $n$-th dimensional space of parameters $\\beta_i$, the gradient of the log-loss function, $\\nabla \\hat{L}$ is computed, and thereafter a translation is performed in the parameter space along the gradient direction (the steepest descent).\n",
    "\n",
    "Gradient descent requires the computation of the gradient. In order to derive its formulation, it is convenient to consider the $i$-th component of $\\nabla \\hat{L}$, i.e.:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_i \\hat{L} = \\frac{\\partial}{\\partial \\beta_i} \\hat{L}\n",
    "\\end{equation}\n",
    "\n",
    "Taking the derivative inside the summation and operating on the logarithm yields:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_i \\hat{L} = - \\frac{1}{m} \\sum_{j=1}^{m} \\left( \\frac{y_j}{\\pi_j} - \\frac{1-y_j}{1-\\pi_j} \\right) \\frac{\\partial \\pi_j}{\\partial \\beta_i}\n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the probability with respect to the parameter equates to (refer to the initial logistic regression formulation):\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\pi_j}{\\partial \\beta_i} = \\frac{\\exp^{-z_j}}{(1+\\exp^{-z_j})^2} \\frac{\\partial z_j} {\\partial \\beta_i} = \\frac{\\exp^{-z_j}}{1+\\exp^{-z_j}} \\frac{1}{1+\\exp^{-z_j}} \\frac{\\partial z_j} {\\partial \\beta_i} = (1-\\pi_j) \\pi_j \\frac{\\partial z_j} {\\partial \\beta_i}\n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the linear combination term yields:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial z_j} {\\partial \\beta_i} = x_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "Putting it all together, one finally obtains: \n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_i \\hat{L} = - \\frac{1}{m} \\sum_{j=1}^{m} \\left[ y_j (1-\\pi_j) - (1-y_j) \\pi_j \\right] x_{ij} = \\frac{1}{m}\\sum_{j=1}^{m} (\\pi_j-y_j) x_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "for $i = 1,2,\\dots,n$. \n",
    "\n",
    "Assuming a certain starting point in the space of parameters, $\\beta_i^0$, gradient descent first computes the gradient $\\nabla \\hat{L}$ at this starting point, and shifts the point along the direction of this gradient by computing a new point $\\beta_i^1 = \\beta_i^0 - \\alpha \\cdot \\nabla \\hat{L}$, where $\\alpha$ is a learning rate. This is done iteratively until suitable stopping criteria are met.\n",
    "\n",
    "## Algorithm for scalable implementation of logistic regression\n",
    "\n",
    "- Assume starting values for logistic parameters $\\beta_i^0$\n",
    "- Set learning parameter $\\alpha$\n",
    "- For each iteration $k$:\n",
    "- Broadcast parameters $\\beta_i^{k}$ to all worker nodes\n",
    "- Map: emit key-value pairs. Key: index $j$, values: target variable $y_j$ and array of explanatory features $x_{ij}$, for $j = 1,2,\\dots,n$\n",
    "- Map: for every $j = 1,2,\\dots,n$ compute probability $\\pi_j$ and $\\left[ y_j (1-\\pi_j) - (1-y_j) \\pi_j \\right] x_{ij}$\n",
    "- Reduce: sum over $j$ and divide by $m$, for $i = 1,2,\\dots,n$\n",
    "- Update $\\beta_i^{k}$\n",
    "- Run next iteration\n",
    "\n",
    "__References:__\n",
    "\n",
    "Bilder, C.R. and Loughin, T.M. (2015). Analysis of Categorical Data with R. CRC Press. \n",
    "\n",
    "Kremonic, Z. (2017). Maximum likelihood and gradient descent demonstration. Blog post. Accessed on July 2019 at https://zlatankr.github.io/posts/2017/03/06/mle-gradient-descent.\n",
    "\n",
    "\n",
    "\n",
    "Additional resources:\n",
    "\n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "\n",
    "https://ttic.uchicago.edu/~suriya/website-intromlss2018/course_material/Day3b.pdf \n",
    "\n",
    "http://www.holehouse.org/mlclass/06_Logistic_Regression.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"final_project\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sample training data and convert to dataframe\n",
    "train_sample = sc.textFile('data/sample_training.txt')\\\n",
    "                 .map(lambda x: x.split('\\t'))\\\n",
    "                 .toDF().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+-----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+--------+--------+--------+--------+---+---+\n",
      "| _1| _2| _3| _4| _5|   _6| _7| _8| _9|_10|_11|_12|_13|_14|     _15|     _16|     _17|     _18|     _19|     _20|     _21|     _22|     _23|     _24|     _25|     _26|     _27|     _28|     _29|     _30|     _31|     _32|_33|_34|     _35|     _36|     _37|     _38|_39|_40|\n",
      "+---+---+---+---+---+-----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+--------+--------+--------+--------+---+---+\n",
      "|  0|  0|  1|   |  0|16597|557|  3|  5|123|  0|  1|   |  1|8cf07265|7cd19acc|77f2f2e5|d16679b9|4cf72387|fbad5c96|8fb24933|0b153874|a73ee510|0095a535|3617b5f5|9f32b866|428332cf|b28479f6|83ebd498|31ca40b6|e5ba7672|d0e5eb07|   |   |dfcfc3fa|ad3062eb|32c7478e|aee52b6f|   |   |\n",
      "|  0|  1|  0|  1|   | 1427|  3| 16| 11| 50|  0|  2|  1|   |05db9164|26a88120|615e3e4e|2788fed8|4cf72387|7e0ccccf|3f4ec687|0b153874|a73ee510|0e9ead52|c4adf918|f5d19c1c|85dbe138|07d13a8f|24ff9452|1034ac0d|3486227d|b486119d|   |   |63580fba|        |32c7478e|2a90c749|   |   |\n",
      "|  0|   |  1|   |   |23255|   |  0|  1| 73|   |  0|   |   |7e5c2ff4|d833535f|b00d1501|d16679b9|25c83c98|7e0ccccf|65c53f25|1f89b562|a73ee510|3b08e48b|ad2bc6f4|e0d76380|39ccb769|b28479f6|a733d362|1203a270|776ce399|281769c2|   |   |73d06dde|        |32c7478e|aee52b6f|   |   |\n",
      "+---+---+---+---+---+-----+---+---+---+---+---+---+---+---+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+--------+--------+--------+--------+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# head first three rows\n",
    "train_sample.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numeric columns from string to double\n",
    "convert_cols = ['_1','_2','_3','_4','_5','_6','_7','_8','_9','_10','_11','_12','_13','_14']\n",
    "\n",
    "for col in convert_cols:\n",
    "    train_sample = train_sample.withColumn(col, train_sample[col].cast(\"double\"))\n",
    "train_sample = train_sample.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----+----+-------+-----+----+----+-----+----+---+----+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+--------+--------+--------+--------+---+---+\n",
      "| _1|  _2| _3|  _4|  _5|     _6|   _7|  _8|  _9|  _10| _11|_12| _13| _14|     _15|     _16|     _17|     _18|     _19|     _20|     _21|     _22|     _23|     _24|     _25|     _26|     _27|     _28|     _29|     _30|     _31|     _32|_33|_34|     _35|     _36|     _37|     _38|_39|_40|\n",
      "+---+----+---+----+----+-------+-----+----+----+-----+----+---+----+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+--------+--------+--------+--------+---+---+\n",
      "|0.0| 0.0|1.0|null| 0.0|16597.0|557.0| 3.0| 5.0|123.0| 0.0|1.0|null| 1.0|8cf07265|7cd19acc|77f2f2e5|d16679b9|4cf72387|fbad5c96|8fb24933|0b153874|a73ee510|0095a535|3617b5f5|9f32b866|428332cf|b28479f6|83ebd498|31ca40b6|e5ba7672|d0e5eb07|   |   |dfcfc3fa|ad3062eb|32c7478e|aee52b6f|   |   |\n",
      "|0.0| 1.0|0.0| 1.0|null| 1427.0|  3.0|16.0|11.0| 50.0| 0.0|2.0| 1.0|null|05db9164|26a88120|615e3e4e|2788fed8|4cf72387|7e0ccccf|3f4ec687|0b153874|a73ee510|0e9ead52|c4adf918|f5d19c1c|85dbe138|07d13a8f|24ff9452|1034ac0d|3486227d|b486119d|   |   |63580fba|        |32c7478e|2a90c749|   |   |\n",
      "|0.0|null|1.0|null|null|23255.0| null| 0.0| 1.0| 73.0|null|0.0|null|null|7e5c2ff4|d833535f|b00d1501|d16679b9|25c83c98|7e0ccccf|65c53f25|1f89b562|a73ee510|3b08e48b|ad2bc6f4|e0d76380|39ccb769|b28479f6|a733d362|1203a270|776ce399|281769c2|   |   |73d06dde|        |32c7478e|aee52b6f|   |   |\n",
      "+---+----+---+----+----+-------+-----+----+----+-----+----+---+----+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+---+---+--------+--------+--------+--------+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# head first three rows\n",
    "train_sample.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate click-through rate\n",
    "num_ct = train_sample.groupBy().sum('_1').collect()[0][0]\n",
    "num_ct/train_sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add plots, discussions, etc. for EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate train data for homegrown solution - select only 100,000 rows and only numerical features + target \n",
    "train_sample_red = train_sample.select(convert_cols).limit(10000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----+----+-------+-----+----+----+-----+----+---+----+----+\n",
      "| _1|  _2| _3|  _4|  _5|     _6|   _7|  _8|  _9|  _10| _11|_12| _13| _14|\n",
      "+---+----+---+----+----+-------+-----+----+----+-----+----+---+----+----+\n",
      "|0.0| 0.0|1.0|null| 0.0|16597.0|557.0| 3.0| 5.0|123.0| 0.0|1.0|null| 1.0|\n",
      "|0.0| 1.0|0.0| 1.0|null| 1427.0|  3.0|16.0|11.0| 50.0| 0.0|2.0| 1.0|null|\n",
      "|0.0|null|1.0|null|null|23255.0| null| 0.0| 1.0| 73.0|null|0.0|null|null|\n",
      "+---+----+---+----+----+-------+-----+----+----+-----+----+---+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sample_red.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values with averages\n",
    "from pyspark.sql.functions import avg\n",
    "for col in train_sample_red.columns:\n",
    "    train_sample_red = train_sample_red.na.fill(round(train_sample_red.na.drop().agg(avg(col)).first()[0],1), [col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----+---+-------+-----+----+----+-----+---+---+---+---+\n",
      "| _1| _2| _3|  _4| _5|     _6|   _7|  _8|  _9|  _10|_11|_12|_13|_14|\n",
      "+---+---+---+----+---+-------+-----+----+----+-----+---+---+---+---+\n",
      "|0.0|0.0|1.0|21.9|0.0|16597.0|557.0| 3.0| 5.0|123.0|0.0|1.0|0.9|1.0|\n",
      "|0.0|1.0|0.0| 1.0|9.2| 1427.0|  3.0|16.0|11.0| 50.0|0.0|2.0|1.0|9.8|\n",
      "|0.0|6.7|1.0|21.9|9.2|23255.0| 86.0| 0.0| 1.0| 73.0|0.7|0.0|0.9|9.8|\n",
      "+---+---+---+----+---+-------+-----+----+----+-----+---+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sample_red.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache into rdd\n",
    "train_sample_red_RDD = train_sample_red.rdd.map(lambda x: (x[0], np.array(x[1:]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data round mean of each feature.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "    Returns:\n",
    "        normedRDD - records are tuples of (y, features_array)\n",
    "    \"\"\"\n",
    "    featureMeans = dataRDD.map(lambda x: x[1]).mean()\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[1]).variance())\n",
    "\n",
    "    normedRDD = dataRDD.map(lambda x: (x[0], (x[1] - featureMeans)/featureStdev))\n",
    "\n",
    "    return normedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d - write function to compute loss (FILL IN MISSING CODE BELOW)\n",
    "def LogLoss(dataRDD, W, regType = None, regParam=0.1):\n",
    "    \"\"\"\n",
    "    Compute log loss function.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient defaults to 0.1\n",
    "    Returns:\n",
    "        loss - (float) the regularized loss\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[1]), x[0])).cache()\n",
    "    \n",
    "    # add regularization term\n",
    "    reg_term = 0\n",
    "    if regType == 'ridge':\n",
    "        reg_term = regParam*np.linalg.norm(W[1:])\n",
    "    elif regType == 'lasso':\n",
    "        reg_term = regParam*np.sum(np.abs(W[1:]))\n",
    "    \n",
    "    # compute loss\n",
    "    loss = augmentedData.map(lambda x: x[1]*np.log(1 + np.exp(-np.dot(x[0], W))) + \\\n",
    "                             (1 - x[1])*(np.dot(x[0], W) + np.log(1 + np.exp(-np.dot(x[0], W))))).sum()\\\n",
    "                            /augmentedData.count()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDUpdate(dataRDD, W, regType = None, regParam=0.1, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one regularized gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient defaults to 0.1\n",
    "        learningRate - (float) defaults to 0.1\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[1]), x[0]))\n",
    "    \n",
    "    # add regularization term\n",
    "    reg_term = np.zeros(len(W))\n",
    "    if regType == 'ridge':\n",
    "        reg_term = np.append(0,2*regParam*W[1:])\n",
    "    elif regType == 'lasso':\n",
    "        reg_term = np.append(0,regParam*np.sign(W[1:]))\n",
    "    \n",
    "    # compute gradient\n",
    "    grad = augmentedData.map(lambda x: ((1/(1 + np.exp(-np.dot(x[0], W))) - x[1])*x[0])).sum()/augmentedData.count() + reg_term\n",
    "    \n",
    "    #update model parameters\n",
    "    new_model = W - learningRate*grad\n",
    "   \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrediction(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Perform one regularized gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        pred - (rdd) predicted targets\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[1]), x[0])).cache()\n",
    "    \n",
    "    # compute prediction\n",
    "    pred = augmentedData.map(lambda x: int((1/(1 + np.exp(-np.dot(x[0], W))))>0.5) )\n",
    "   \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "normedRDD = normalize(train_sample_red_RDD).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of features\n",
    "numb_features = len(train_sample_red.columns) - 1\n",
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSteps = 150\n",
    "learningRate = 0.5\n",
    "regType = 'lasso'\n",
    "regParam = 0.02\n",
    "\n",
    "Loss_save = []\n",
    "Model_norm = []\n",
    "#broadcast model\n",
    "model = BASELINE #model = sc.broadcast(BASELINE) #substitute this line with the comment when deploying it on the cloud\n",
    "for idx in range(nSteps):\n",
    "    #print(\"----------\")\n",
    "    #print(f\"STEP: {idx+1}\")\n",
    "    \n",
    "    # compute loss\n",
    "    loss = LogLoss(normedRDD, model, regType=regType, regParam=regParam)\n",
    "    # update model parameters\n",
    "    model = GDUpdate(normedRDD, model, regType=regType, regParam=regParam, learningRate=learningRate)\n",
    "    \n",
    "    #store results\n",
    "    Loss_save.append(loss)\n",
    "    Model_norm.append(np.linalg.norm(model))\n",
    "    \n",
    "    #broadcast model\n",
    "    #model = sc.broadcast(model) #uncomment this line when deploying it on the cloud\n",
    "\n",
    "print(f\"The estimated model is: {model}\")\n",
    "print(f\"The loss of the estimated model is: {loss}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(Loss_save)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.plot(Model_norm)\n",
    "plt.title('Norm of vector of parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of true positives is {normedRDD.map(lambda x: x[0]).sum()}')\n",
    "res = makePrediction(normedRDD, model).cache()\n",
    "print(f'number of predicted positives is {res.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression will take care of the linear terms, now to account for the interaction term, we expand to include 2nd Degree polynomial features. The below equation represents the formulation.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} +  \\sum_{i=1}^n \\sum_{j=i+1}^n w_{ij} x_{i} x_{j}\n",
    "\\end{align}\n",
    "\n",
    "The challenge with solving the above equation is that the time complexity is $O(n^2)$\n",
    "\n",
    "In order to work with this, we use a matrix factorization technique for the interaction terms, inspired by Matrix factorization. We introduce a hyperpameter K which represent the latent factors for factorizing the weight vector $w_{ij}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\sum_{i=1}^{n} \\sum_{j=i+1}^n \\langle \\textbf{v}_i , \\textbf{v}_{j} \\rangle x_i x_{j}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Using the computation specified in Stephen Rendles paper, we can simplify the interaction term to the below equation.\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j}\n",
    "&= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j} - \\frac{1}{2} \\sum_{i=1}^n \\langle \\textbf{v}_i , \\textbf{v}_{i} \\rangle x_{i} x_{i}  \\\\\n",
    "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j} \\right)\\frac{1}{2}\\left( \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
    "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j}  -  \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
    "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left(\\sum_{i=1}^n v_{i,f}x_{i} \\right) \\left( \\sum_{j=1}^n v_{j,f}x_{j} \\right) - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right) \\\\\n",
    "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align}\n",
    "\n",
    "So, we can rewrite the equation to compute in $O(n)$ as\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align}\n",
    "\n",
    "#### Gradient\n",
    "\n",
    "For our classification problem, we can define the gradients as :\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial\\theta}\\hat{y}(\\textbf{x}) =\n",
    "\\begin{cases}\n",
    "1,  & \\text{if $\\theta$ is $w_0$} \\\\\n",
    "x_i, & \\text{if $\\theta$ is $w_i$} \\\\\n",
    "x_i\\sum_{j=1}^{n} v_{j,f}x_j - v_{i,f}x_{i}^2 & \\text{if $\\theta$ is $v_{i,f}$}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\hat{y} }(\\textbf{L}) =\n",
    "\\frac{d}{d \\hat{y}}\\left[ \\ln \\big(e^{-y \\hat{y}} + 1 \\big) \\right] \n",
    "&= \\frac{1}{e^{-y \\hat{y}} + 1} \\cdot  \\frac{d}{dx}\\left[e^{-y \\hat{y}} + 1 \\right] \\\\\n",
    "&= -\\frac{y}{e^{y \\hat{y}} + 1}\n",
    "\\end{align}\n",
    "\n",
    "The gradient of loss is defined by (by using chain rule):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta }(\\textbf{L}) = \n",
    "\\frac{\\partial}{\\partial \\hat{y} }(\\textbf{L}) \\centerdot \\frac{\\partial}{\\partial \\theta}\\hat{y}(\\textbf{x}) \n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmLoss(dataRDD, w) :\n",
    "    \"\"\"\n",
    "    Computes the logloss given the data and model W\n",
    "    dataRDD - array of features, label\n",
    "    \"\"\"\n",
    "    w_bc = sc.broadcast(w)\n",
    "    def probability_value(x,W): \n",
    "        xa = np.array([x])\n",
    "        V =  xa.dot(W)\n",
    "        V_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(V*V - V_square).sum()\n",
    "        return 1.0/(1.0 + np.exp(-phi))\n",
    "    \n",
    "    loss = dataRDD.map(lambda x:  (probability_value(x[1],w_bc.value), x[0])) \\\n",
    "        .map(lambda x: (1 - 1e-12, x[1]) if x[0] == 1 else ((1e-12, x[1]) if x[0] == 0  else (x[0],x[1]))) \\\n",
    "        .map(lambda x: -(x[1] * np.log(x[0]) - (1-x[1])*np.log(1-x[0]))).mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmGradUpdate_v1(dataRDD, w, alpha, regParam):\n",
    "    \"\"\"\n",
    "    Computes the gradient and updates the model\n",
    "    \"\"\"\n",
    "    \n",
    "    G = np.zeros(np.shape(w))\n",
    "    w_bc = sc.broadcast(w)\n",
    "    a_bc = sc.broadcast(alpha)\n",
    "    def row_grad(x, y, W, regParam):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum()\n",
    "        expnyt = np.exp(y*phi)\n",
    "        grad_loss = (-y)/(1+expnyt)*(xa.T.dot(xa).dot(W) - np.diag(np.square(x)).dot(W))\n",
    "        \n",
    "        return regParam*W + grad_loss\n",
    "      \n",
    "    grad = dataRDD.map(lambda x: (1, row_grad(x[1], x[0], w_bc.value, regParam))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    model = w - alpha * grad.values().collect()[0] \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(trainRDD, testRDD, wInit, nSteps = 20, \n",
    "                    learningRate = 0.01, regParam = 0.01, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of OLS gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    for idx in range(nSteps): \n",
    "        \n",
    "        ############## YOUR CODE HERE #############\n",
    "        model = fmGradUpdate_v1(trainRDD, model, learningRate, regParam)\n",
    "        training_loss = fmLoss(trainRDD, model) \n",
    "        test_loss = fmLoss(testRDD, model) \n",
    "        ############## (END) YOUR CODE #############\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[k for k in model]}\")\n",
    "   \n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wInitialization(dataRDD, factor):\n",
    "    nrFeat = len(dataRDD.first()[1])\n",
    "    np.random.seed(int(time.time())) \n",
    "    w =  np.random.ranf((nrFeat, factor))\n",
    "    w = w / np.sqrt((w*w).sum())\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "wInit = wInitialization(normedRDD, 2)\n",
    "#print(wInit)\n",
    "logerr_train, logerr_test, models = GradientDescent(normedRDD, normedRDD, wInit, nSteps = 200,\n",
    "                                                    learningRate = 0.001, regParam = 0.01, verbose = False)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")\n",
    "print(models[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The estimated model is: {models[-1]}\")\n",
    "print(f\"The loss of the estimated model is: {logerr_train[-1]}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(logerr_train)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmMakePrediction(dataRDD, w):\n",
    "    \"\"\"\n",
    "    Perform one regularized gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        pred - (rdd) predicted targets\n",
    "    \"\"\"\n",
    "    w_bc = sc.broadcast(w)\n",
    "    def predict_fm(x, W):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum()\n",
    "        return phi\n",
    "    \n",
    "    # compute prediction\n",
    "    pred = dataRDD.map(lambda x: int(predict_fm(x[1],w_bc.value)>0.5) )\n",
    "   \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of true positives is {normedRDD.map(lambda x: x[0]).sum()}')\n",
    "res = fmMakePrediction(normedRDD, models[-1]).cache()\n",
    "print(f'number of predicted positives is {res.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Adding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The earlier section showed a logistic regression model with numerical variables only. In this section, we incrementally add categorical variables and redo the logistic regression model on a small data set. Categorical variables present a challenge because each of them can have a million different values thereby creating millions of dimensions. To get around \"the curse of dimensionality\", we looked at quite a few methods to reduce the dimensions to a manageable level. This includes frequency based dimensionality reduction and hashing techniques at a feature level and at a collection of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"The curse of dimensionality\" with categorical variables\n",
    "Below, we take a look at the number of dimensions in the train_sample dataset for each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique categorical values\n",
    "from pyspark.sql.functions import col\n",
    "distCatVarCnt = 0\n",
    "for col in train_sample.columns[14:]:\n",
    "    cnt = train_sample.select(col).distinct().count()\n",
    "    print('Column ' + col + ' has ' + str(cnt) \\\n",
    "          + ' unique categorical values')\n",
    "    distCatVarCnt += cnt\n",
    "print(\"Total number of distinct categorical variables in train_sample is:\", distCatVarCnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the number of unique categorical variables is close to 3 million just for the train_sample dataset. We need to look into reducing the dimensionality without losing too much of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Per column frequency based dimensionality reduction\n",
    "\n",
    "One possible methold is to figure out if there are any frequent values in these categorical variables and choose the top 15 of them. If they make up nearly 100% of the values, then we can lump the rest under \"Other\" and come up with 16 column bins. This is a compute intensive operation as we try to figure out the top 15 values for each column and their contribution to the total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_rdd = sc.textFile('data/sample_training.txt')\n",
    "testRdd = sc.parallelize(train_sample_rdd.take(100000))\n",
    "\n",
    "#rddLen = testRdd.count()\n",
    "rddLen = train_sample_rdd.count()\n",
    "\n",
    "top15FreqList = dict()\n",
    "top15FreqPercent = dict()\n",
    "\n",
    "top15df = pd.DataFrame(columns=['col', 'top15_values', 'top15_pct_contribution'])\n",
    "\n",
    "for col in range(14,40):\n",
    "    #col = \"_\"+str(c)\n",
    "    catRdd = train_sample_rdd.map(lambda x: x.split('\\t')[col]) \\\n",
    "                             .map(lambda x: (x,1)) \\\n",
    "                             .reduceByKey(lambda x,y: x + y)\n",
    "    freqRecord = catRdd.takeOrdered(15, key=lambda x: -x[1])\n",
    "    freqList = []\n",
    "    freqCnt = 0\n",
    "    for (k,v) in freqRecord:\n",
    "        freqCnt += v\n",
    "        freqList.append(str(k))\n",
    "    #top15FreqList[col] = freqList\n",
    "    #top15FreqPercent[col] = 100* freqCnt / rddLen\n",
    "    top15df = top15df.append({'col': col, 'top15_values': freqList, 'top15_pct_contribution': 100*freqCnt/rddLen}, ignore_index=True)\n",
    "    #cnt = catRdd.count()\n",
    "    #print (\"Top 15 values for column\", col, \"namely: \", top15FreqList[col], \"make up \", top15FreqPercent[col], \" of the values\")\n",
    "    \n",
    "print(top15df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, we can see that the categorical variables in some of the columns account for almost 100% of all values. These include columns 14, 18, 19, 21, 22, 27, 30, 32, 33, 35, 36 and 38. For these 11 columns, it would make sense to keep the top 15 values and lump everything else under an \"Other\" column.\n",
    "\n",
    "For the rest of the columns that don't exhibit this behavior, it might make sense to look at other strategies such as hashing to reduce dimensionality. Before we take this approach of using both the frequency related information and hashing, let's first take a look at hashing all columns next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Per column Feature Hashing\n",
    "There are 26 categorical features in this model represented by strings that are 8 bytes long. So, theoretically, each string can take $2^{64} -1$ different values and lead to that many dimensions. We need to have far fewer dimensions so that we can make the problem computationally achievable and as well lead to a generalized algorithm as well. One way of achieving this is through what is popularly called the \"hashing trick\" (provide references). \n",
    "\n",
    "A simple way to reduce dimensionality is to hash the 8 byte long strings into, say 16 or 32 groups. We used the murmurHash3 hashing which is generally the preferred way of hashing strings (provide references and more details). Hashing leads to collisions as many strings could end up hashing to the same hash value. However, it has been proven (references) that even with collisions, hashing leads to very generalized models.\n",
    "\n",
    "One the categorical variables are hashed down to, say 16 values, they are then 1-hot encoded and fed into the logistic regression model. This section presents the results with the inclusion of 27 categorical variables, each individually hashed to 16 values, in addition to the numerical variables in the earlier section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install murmurhash3 if needed\n",
    "!pip install murmurhash3\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the complete dataset\n",
    "train_sample_rdd = sc.textFile('data/sample_training.txt')\n",
    "\n",
    "#Get the top 1000 rows only (as before for numerical variables)\n",
    "testRdd = sc.parallelize(train_sample_rdd.take(10000),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createHash (elem,hlen):\n",
    "    import mmh3\n",
    "    hashStr = []\n",
    "    for str in elem:\n",
    "        hashStr.append(mmh3.hash(str) % int(hlen))\n",
    "    return(hashStr)\n",
    "\n",
    "def create1Hot(elem, hlen):\n",
    "    oneHotStr = []\n",
    "    #for hashStr in elem:\n",
    "    for i in range (hlen):\n",
    "        if (i == elem):\n",
    "            oneHotStr.append(1)\n",
    "        else:\n",
    "            oneHotStr.append(0)\n",
    "    return(oneHotStr)\n",
    "\n",
    "def createCatArray(elem):\n",
    "    catArray = []\n",
    "    for array in elem:\n",
    "        for x in array:\n",
    "            catArray.append(x)\n",
    "    return(np.array(catArray))\n",
    "\n",
    "#Define murmurHash level for 1-hot encoding\n",
    "HASHLEN = 16\n",
    "\n",
    "#testRdd.map(lambda x : x.split('\\t')[14:40]).map(lambda x: [mmh3.hash(xn)%16 for xn in x]).take(5)\n",
    "categoricalRdd = testRdd.map(lambda x : x.split('\\t')[14:40]) \\\n",
    "                        .map(lambda x: createHash(x,HASHLEN)) \\\n",
    "                        .map(lambda x: [create1Hot(xn, HASHLEN) for xn in x]) \\\n",
    "                        .map(createCatArray)\n",
    "categoricalRdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, array([-0.47547949, -0.28723426, -0.03746799, -0.92965574, -0.01272051,\n",
       "          1.24738994, -0.19755653, -0.15479084, -0.00440928, -1.27487322,\n",
       "         -0.34456582, -0.00283255, -0.26716893,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ]))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now include this into the logistic regression model\n",
    "#Zip numerical and categorical variables\n",
    "\n",
    "def mergeNumPlusCatRdds(elem):\n",
    "    x, y = elem\n",
    "    xkey , xval = x\n",
    "    merge =  (xkey, np.hstack((xval,y)))\n",
    "    return(merge)\n",
    "\n",
    "numPlusCatRdd = normedRDD.zip(categoricalRdd) \\\n",
    "                         .map(mergeNumPlusCatRdds)\n",
    "numPlusCatRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n"
     ]
    }
   ],
   "source": [
    "#compute the number of features\n",
    "numb_features = np.size(numPlusCatRdd.first()[1])\n",
    "print(numb_features)\n",
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train, validation and test sets\n",
    "train, validation, test = numPlusCatRdd.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n",
      "----------\n",
      "STEP: 2\n",
      "----------\n",
      "STEP: 3\n",
      "----------\n",
      "STEP: 4\n",
      "----------\n",
      "STEP: 5\n",
      "----------\n",
      "STEP: 6\n",
      "----------\n",
      "STEP: 7\n",
      "----------\n",
      "STEP: 8\n",
      "----------\n",
      "STEP: 9\n",
      "----------\n",
      "STEP: 10\n",
      "----------\n",
      "STEP: 11\n",
      "----------\n",
      "STEP: 12\n",
      "----------\n",
      "STEP: 13\n",
      "----------\n",
      "STEP: 14\n",
      "----------\n",
      "STEP: 15\n",
      "----------\n",
      "STEP: 16\n",
      "----------\n",
      "STEP: 17\n",
      "----------\n",
      "STEP: 18\n",
      "----------\n",
      "STEP: 19\n",
      "----------\n",
      "STEP: 20\n",
      "----------\n",
      "STEP: 21\n",
      "----------\n",
      "STEP: 22\n",
      "----------\n",
      "STEP: 23\n",
      "----------\n",
      "STEP: 24\n",
      "----------\n",
      "STEP: 25\n",
      "----------\n",
      "STEP: 26\n",
      "----------\n",
      "STEP: 27\n",
      "----------\n",
      "STEP: 28\n",
      "----------\n",
      "STEP: 29\n",
      "----------\n",
      "STEP: 30\n",
      "----------\n",
      "STEP: 31\n",
      "----------\n",
      "STEP: 32\n",
      "----------\n",
      "STEP: 33\n",
      "----------\n",
      "STEP: 34\n",
      "----------\n",
      "STEP: 35\n",
      "----------\n",
      "STEP: 36\n",
      "----------\n",
      "STEP: 37\n",
      "----------\n",
      "STEP: 38\n",
      "----------\n",
      "STEP: 39\n",
      "----------\n",
      "STEP: 40\n",
      "----------\n",
      "STEP: 41\n",
      "----------\n",
      "STEP: 42\n",
      "----------\n",
      "STEP: 43\n",
      "----------\n",
      "STEP: 44\n",
      "----------\n",
      "STEP: 45\n",
      "----------\n",
      "STEP: 46\n",
      "----------\n",
      "STEP: 47\n",
      "----------\n",
      "STEP: 48\n",
      "----------\n",
      "STEP: 49\n",
      "----------\n",
      "STEP: 50\n",
      "----------\n",
      "STEP: 51\n",
      "----------\n",
      "STEP: 52\n",
      "----------\n",
      "STEP: 53\n",
      "----------\n",
      "STEP: 54\n",
      "----------\n",
      "STEP: 55\n",
      "----------\n",
      "STEP: 56\n",
      "----------\n",
      "STEP: 57\n",
      "----------\n",
      "STEP: 58\n",
      "----------\n",
      "STEP: 59\n",
      "----------\n",
      "STEP: 60\n",
      "----------\n",
      "STEP: 61\n",
      "----------\n",
      "STEP: 62\n",
      "----------\n",
      "STEP: 63\n",
      "----------\n",
      "STEP: 64\n",
      "----------\n",
      "STEP: 65\n",
      "----------\n",
      "STEP: 66\n",
      "----------\n",
      "STEP: 67\n",
      "----------\n",
      "STEP: 68\n",
      "----------\n",
      "STEP: 69\n",
      "----------\n",
      "STEP: 70\n",
      "----------\n",
      "STEP: 71\n",
      "----------\n",
      "STEP: 72\n",
      "----------\n",
      "STEP: 73\n",
      "----------\n",
      "STEP: 74\n",
      "----------\n",
      "STEP: 75\n",
      "----------\n",
      "STEP: 76\n",
      "----------\n",
      "STEP: 77\n",
      "----------\n",
      "STEP: 78\n",
      "----------\n",
      "STEP: 79\n",
      "----------\n",
      "STEP: 80\n",
      "----------\n",
      "STEP: 81\n",
      "----------\n",
      "STEP: 82\n",
      "----------\n",
      "STEP: 83\n",
      "----------\n",
      "STEP: 84\n",
      "----------\n",
      "STEP: 85\n",
      "----------\n",
      "STEP: 86\n",
      "----------\n",
      "STEP: 87\n",
      "----------\n",
      "STEP: 88\n",
      "----------\n",
      "STEP: 89\n",
      "----------\n",
      "STEP: 90\n",
      "----------\n",
      "STEP: 91\n",
      "----------\n",
      "STEP: 92\n",
      "----------\n",
      "STEP: 93\n",
      "----------\n",
      "STEP: 94\n",
      "----------\n",
      "STEP: 95\n",
      "----------\n",
      "STEP: 96\n",
      "----------\n",
      "STEP: 97\n",
      "----------\n",
      "STEP: 98\n",
      "----------\n",
      "STEP: 99\n",
      "----------\n",
      "STEP: 100\n",
      "----------\n",
      "STEP: 101\n",
      "----------\n",
      "STEP: 102\n",
      "----------\n",
      "STEP: 103\n",
      "----------\n",
      "STEP: 104\n",
      "----------\n",
      "STEP: 105\n",
      "----------\n",
      "STEP: 106\n",
      "----------\n",
      "STEP: 107\n",
      "----------\n",
      "STEP: 108\n",
      "----------\n",
      "STEP: 109\n",
      "----------\n",
      "STEP: 110\n",
      "----------\n",
      "STEP: 111\n",
      "----------\n",
      "STEP: 112\n",
      "----------\n",
      "STEP: 113\n",
      "----------\n",
      "STEP: 114\n",
      "----------\n",
      "STEP: 115\n",
      "----------\n",
      "STEP: 116\n",
      "----------\n",
      "STEP: 117\n",
      "----------\n",
      "STEP: 118\n",
      "----------\n",
      "STEP: 119\n",
      "----------\n",
      "STEP: 120\n",
      "----------\n",
      "STEP: 121\n",
      "----------\n",
      "STEP: 122\n",
      "----------\n",
      "STEP: 123\n",
      "----------\n",
      "STEP: 124\n",
      "----------\n",
      "STEP: 125\n",
      "----------\n",
      "STEP: 126\n",
      "----------\n",
      "STEP: 127\n",
      "----------\n",
      "STEP: 128\n",
      "----------\n",
      "STEP: 129\n",
      "----------\n",
      "STEP: 130\n",
      "----------\n",
      "STEP: 131\n",
      "----------\n",
      "STEP: 132\n",
      "----------\n",
      "STEP: 133\n",
      "----------\n",
      "STEP: 134\n",
      "----------\n",
      "STEP: 135\n",
      "----------\n",
      "STEP: 136\n",
      "----------\n",
      "STEP: 137\n",
      "----------\n",
      "STEP: 138\n",
      "----------\n",
      "STEP: 139\n",
      "----------\n",
      "STEP: 140\n",
      "----------\n",
      "STEP: 141\n",
      "----------\n",
      "STEP: 142\n",
      "----------\n",
      "STEP: 143\n",
      "----------\n",
      "STEP: 144\n",
      "----------\n",
      "STEP: 145\n",
      "----------\n",
      "STEP: 146\n",
      "----------\n",
      "STEP: 147\n",
      "----------\n",
      "STEP: 148\n",
      "----------\n",
      "STEP: 149\n",
      "----------\n",
      "STEP: 150\n",
      "The estimated model is: [-1.07661191e+00  1.63763627e-03  5.33505824e-03 -6.12988016e-03\n",
      " -1.44896658e-02 -8.11383907e-02 -1.05581121e-01 -9.48290396e-03\n",
      "  3.58267238e-03 -1.01615320e-02  1.12911659e-01  2.46945702e-01\n",
      "  5.20688959e-04 -1.54641668e-03 -6.22308344e-03  1.19302297e-03\n",
      "  8.51785765e-04 -3.52381631e-01 -6.84236512e-03 -5.67843208e-01\n",
      " -5.19713339e-03  4.05221014e-03 -6.25445608e-03  7.68997564e-04\n",
      " -2.61761201e-01 -4.29940312e-03 -6.08276740e-03  3.18867461e-03\n",
      " -5.66594565e-03 -5.77984678e-03  2.33769039e-03 -1.13973882e-02\n",
      "  8.83870561e-03  2.61876625e-03  7.47221197e-03 -2.71019085e-03\n",
      " -4.83663338e-03  9.39338453e-04 -3.68646164e-03  4.21545163e-03\n",
      " -8.96208498e-03 -9.57489718e-03 -6.41163032e-03 -2.01655221e-03\n",
      " -1.22426910e-03  8.32802951e-03  2.01163099e-04 -3.73708035e-03\n",
      " -7.15019013e-03  5.21379741e-03 -8.53257219e-03 -2.61205530e-03\n",
      "  9.65979194e-03  1.83181492e-03  2.79660046e-03 -6.87584182e-03\n",
      " -9.95355881e-03  8.59733494e-03  7.71194471e-03 -4.45143211e-03\n",
      " -7.41836201e-03  5.47687822e-03 -1.89522060e-03 -9.32440354e-03\n",
      "  1.52254856e-01  5.55676154e-03 -2.73917755e-03 -3.05144334e-05\n",
      "  5.93011875e-04 -5.54059515e-03 -5.17112706e-03 -8.90458309e-03\n",
      " -7.52715085e-03 -6.78225391e-03 -7.74681902e-03  5.96942090e-03\n",
      " -5.69500217e-04 -5.14043723e-03  5.81575913e-05  6.52519360e-03\n",
      "  3.55548097e-01 -7.71255098e-03  7.10329225e-03 -3.15221132e-03\n",
      "  2.54465695e-03  5.81702829e-03  1.15622126e-01 -3.75926593e-03\n",
      "  2.94291797e-03  1.66562265e-03 -9.29491028e-03  9.62477728e-03\n",
      " -5.72353188e-03 -3.83645241e-03 -9.45005090e-03 -4.17587858e-03\n",
      "  6.85260650e-03  1.78966021e-03 -7.46488047e-03 -4.09539748e-03\n",
      "  3.05494902e-03 -4.33509438e-03  1.81982677e-01 -5.68301866e-03\n",
      " -3.69273170e-03  1.04358259e-02 -5.63415044e-03 -8.20042321e-03\n",
      " -1.55457559e+00  3.03363191e-03 -1.43006647e-01  2.51827690e-04\n",
      " -8.73904107e-03 -9.13500280e-04 -9.17602202e-03 -3.99313793e-03\n",
      "  4.99977050e-04 -7.27492274e-03 -7.85116484e-03 -7.31802363e-03\n",
      " -1.90500324e-03 -4.23673193e-03 -8.88708680e-02  3.49726440e-03\n",
      "  5.11329497e-03  7.36616990e-03 -5.45923710e-03  3.88601052e-03\n",
      " -4.12942356e-01  6.13551017e-03 -2.25878570e-03  8.15752208e-04\n",
      "  6.70446328e-04 -8.55155997e-03 -1.77524132e-03 -2.02120698e-03\n",
      "  5.28614152e-03 -7.29392269e-03 -4.98764006e-03  1.75019716e-03\n",
      "  1.02035111e-03 -3.13043247e-01 -7.86920644e-03 -8.42000806e-03\n",
      "  9.26510533e-03  1.84906056e-03  2.07420871e-03  7.90276116e-03\n",
      "  2.36946862e-03 -9.28252841e-03  1.83121099e-03  2.99718165e-03\n",
      "  2.99917579e-05 -2.19559365e-03  6.80590524e-03  9.27987432e-03\n",
      " -9.79403522e-03  5.48743689e-03  3.84013904e-03 -1.50328384e-03\n",
      " -7.24198859e-04  6.56236627e-03  5.23860225e-03 -4.18733179e-03\n",
      "  3.77596679e-03 -6.79549893e-03 -1.51550112e-01 -7.54369564e-03\n",
      "  9.70745696e-03 -5.39195383e-03 -1.69982052e-03  9.62099122e-02\n",
      " -6.69458419e-03 -7.69909825e-03 -3.47809344e-04  2.22930257e-03\n",
      "  4.96585847e-03 -2.29053299e-03 -4.84660932e-03 -1.01182171e-02\n",
      "  2.84882853e-03 -1.72172326e-03 -6.01971674e-01  4.15905590e-03\n",
      " -4.29297274e-03 -1.84184552e-03 -4.54283267e-03 -6.32643334e-03\n",
      " -1.45213657e-03  3.42604725e-03  6.05606228e-03  8.04372523e-03\n",
      "  6.22228493e-03  9.82256395e-02  5.73174551e-03  3.49515876e-04\n",
      "  6.10008466e-03  3.67037765e-03  8.33514817e-03 -8.55797237e-03\n",
      "  7.08732508e-03  8.45003156e-03  4.99196413e-04  4.98236851e-03\n",
      " -1.07829992e-02 -4.79440440e-03 -4.05967986e-03  5.54765709e-03\n",
      "  5.06977112e-03 -7.76822651e-03 -1.87766543e-03 -6.10797942e-04\n",
      " -5.36007395e-03 -2.72768738e-03  8.81253335e-03 -9.05207824e-03\n",
      " -3.21918028e-03 -8.66164525e-03  1.63962929e-03  1.85672160e-04\n",
      " -5.72159634e-03 -2.79885126e-03 -8.81068674e-03  7.42362671e-04\n",
      "  9.82870905e-02  9.54488686e-03  8.32059725e-02  7.84085321e-03\n",
      "  6.55218862e-05  8.51620449e-04 -2.01753047e-03  8.81554701e-02\n",
      " -5.09677070e-04  3.41462723e-03 -3.47499164e-03 -6.17266126e-03\n",
      " -6.18835158e-03 -4.62446804e-03 -2.07649851e-03 -7.98312237e-03\n",
      " -6.87045550e-03 -9.18445304e-03 -6.04241803e-04  9.49548556e-03\n",
      " -8.18951782e-03 -1.02571025e-02  8.03425231e-03 -2.66462851e-03\n",
      " -8.40931840e-03 -6.69014340e-03  1.00056949e-02 -6.53149957e-03\n",
      "  2.53454517e-03 -8.72476312e-03 -1.88506579e-03 -1.40564272e-01\n",
      " -7.95224888e-03  4.70235926e-03 -7.52471343e-03 -2.53763652e-03\n",
      " -2.37646142e-01 -8.09170979e-03  6.46677689e-03 -6.34256443e-03\n",
      "  6.04587276e-03  5.72101940e-03 -7.39486189e-03 -1.34096716e-01\n",
      " -5.07211305e-03 -6.00618280e-03  3.60644055e-03  5.27387822e-03\n",
      "  2.30331761e-04 -4.64467587e-03 -4.00301099e-03 -2.90921253e-03\n",
      " -5.50359936e-03 -2.20575208e-03  7.89686821e-03  7.36792743e-03\n",
      " -6.10142892e-03  6.62816712e-03  9.99431684e-03  4.60545471e-03\n",
      "  9.57055258e-04  6.09163023e-02  3.88551418e-03  8.15170322e-03\n",
      " -2.96558352e-03 -5.02981540e-03 -8.55812795e-02  3.14203873e-03\n",
      " -1.14990178e-03 -7.51358640e-03 -3.66857843e-03 -1.43252942e-01\n",
      " -6.74720546e-03  2.87915352e-02  1.75253824e-03  4.87620693e-03\n",
      "  1.02323645e-02  7.63833078e-03  7.84972570e-03 -6.56189190e-03\n",
      " -5.58922969e-01  3.12227831e-01  7.23926328e-03  4.63255637e-04\n",
      " -4.00331753e-03  7.64513735e-03 -8.45089576e-03  1.63133327e-03\n",
      " -5.07174484e-03  9.62517804e-03 -1.02023005e-02 -4.83059234e-03\n",
      "  7.48612459e-03 -6.38633535e-01 -5.48564825e-03 -4.09518218e-03\n",
      " -5.01576007e-03  3.84147988e-03  9.90812795e-03 -5.85672299e-01\n",
      "  9.66039746e-03  1.97759294e-01 -2.87123211e-04  7.29659593e-03\n",
      " -6.87504732e-03  5.30779720e-03  3.35393383e-03 -3.54373038e-03\n",
      " -6.93270618e-03 -4.81373363e-03  5.49927465e-03 -2.96906772e-03\n",
      " -2.19591156e-03  1.39900913e-03 -8.21595152e-03 -4.74549025e-04\n",
      "  4.27766343e-03 -6.14850136e-03 -3.06125188e-03  5.38120950e-03\n",
      "  3.62003740e-04 -1.07461311e-02 -8.31333791e-03 -5.86695792e-03\n",
      " -5.30592720e-03  5.74252651e-03  6.28056188e-03 -4.21766284e-03\n",
      " -1.64867607e-03  9.70761012e-03 -8.74966650e-03  1.38941781e-03\n",
      "  9.62317730e-03  3.05181655e-03 -1.33076413e-03  3.53323513e-03\n",
      "  9.51916397e-03 -6.83162768e-03  7.25636277e-03  5.35881677e-03\n",
      "  9.83307174e-03  7.85568351e-03 -9.88567890e-03 -5.13878514e-03\n",
      " -5.16269733e-03 -1.77586815e-03 -5.30975694e-03  1.93347924e-03\n",
      " -1.08309282e-03 -2.82004341e-04 -8.77652454e-03 -5.64766436e-03\n",
      "  2.49392732e-03 -2.37964245e-01  7.69172265e-03 -1.48111995e-03\n",
      "  2.39605626e-03 -3.46130708e-03 -6.97614109e-03 -9.39719262e-03\n",
      " -8.71078875e-03  6.20510360e-03  1.39419721e-04  2.23736438e-03\n",
      "  7.90419175e-03 -5.17367764e-04 -5.53332726e-03  7.20531437e-03\n",
      " -2.28950539e-03 -8.82603067e-03  9.97040493e-03 -8.67377499e-03\n",
      "  4.59896696e-03  1.20886939e-01  2.32536165e-03  2.89348310e-03\n",
      " -7.26813447e-03 -4.39629671e-03 -6.21111865e-03  2.26697780e-03\n",
      "  6.60336132e-03  7.95189120e-03 -7.13411525e-03 -7.24821475e-03\n",
      "  3.93011646e-03  2.09145074e-01 -2.37446102e-01 -1.08118473e-02\n",
      " -1.86714362e-04 -2.93231548e-04 -9.62176500e-03  2.45954695e-03\n",
      " -8.78252004e-03  3.33863780e-04 -3.19838609e-03 -2.57645733e-03\n",
      "  6.15492075e-03 -7.13611120e-03 -3.92352077e-03  3.89120415e-05\n",
      " -4.04430077e-01 -8.71400423e-03  7.81468611e-03 -4.87688717e-03\n",
      "  2.06432697e-03  8.69175027e-03]\n",
      "The loss of the estimated model is: 0.531271015615906\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Norm of vector of parameters')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEFCAYAAAAYKqc0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XlAVdXax/HvYUZGQQ/ghOEsCuKsqCgiDogiZGlpb2SZXdNrehus7uRVy27DTbs5W3br1ZxN0TJxwCmcxTFHFBUOyCgzHNb7h9fzRuKEwIHD8/mLs/c+Zz8u5Mdm7bXX0iilFEIIIUyKmbELEEIIUfEk3IUQwgRJuAshhAmScBdCCBMk4S6EECZIwl0IIUyQhLuoUpcvXyYsLAw/Pz+++eYbY5dTKfLz85kwYQKdOnVi8uTJxi5H1FIS7jVcYGAgPXv2JDc317Bt9erVjB071ohV3d+SJUvo2rUrx44d44UXXqiSc44dO5bVq1dXybkAfvzxR27dukVsbCxz586tsvNWhapuS1F+Eu4mQK/XV8hVsFKKkpKSCqjo/m7evEmLFi0q9RwVqTxtcvPmTZo2bYqFhUUlVfX/iouLK/0cFUmv1xu7hFpDwt0EjBs3jmXLlpGVlVXm/qNHjxIREUGnTp2IiIjg6NGjhn1jx47ls88+Y9SoUfj6+pKQkFBqm5+fHxMmTCA9PZ1p06bRsWNHIiIiuH79+n3riY6OJiQkhM6dOzN27FguXboEwAsvvEBsbCwzZszAz8+PK1eulHpfVFQU4eHhpbZ9/fXXTJgwAYDCwkLmzJlD37596dmzJ3/5y1/Iz883HLt9+3aGDx9Ox44dCQoKIiYmhs8++4zDhw8bzjljxoxytcnvXbp0ibFjx9K5c2dCQkKIjo4GYO7cuXz55Zds3boVPz+/Mq9y582bx+TJk5kyZQp+fn6MGDGCc+fOGfYvWrSIoKAg/Pz8GDJkCD///LNh37p16xg1ahSzZ8+ma9euzJs3j2vXrvHCCy/QrVs3unXrxrRp00r9XwgMDGTJkiWEhobSoUMH3n33XW7dusXLL7+Mn58fL774IpmZmYbjjx8/zqhRo+jcuTPDhg0jNjYW4L5teenSJSIjI+natSsDBw5ky5Yths965513+Otf/8orr7xChw4diI2NZffu3QwZMgQ/Pz969+7N0qVL72kjUQGUqNH69eun9u3bpyZOnKg+/fRTpZRSq1atUmPGjFFKKZWenq46d+6s1q9fr4qKitSmTZtU586dVVpamlJKqTFjxqiAgAB1/vx5VVRUpAoLC9WYMWNUUFCQunr1qsrKylKDBw9WwcHBat++faqoqEi9+eab6p133imznsuXLytfX1+1d+9eVVhYqBYtWqSCgoJUQUGB4XyrVq0q8725ubmqQ4cO6sqVK4Zt4eHhavPmzUoppWbOnKleffVVlZ6erm7fvq1effVV9fHHHyullDpx4oTq2LGj2rt3r9Lr9SopKUldvHixzHOWp01+q7CwUAUFBan58+ergoICtX//ftWhQwd16dIlpZRSc+fOVdOmTbvv92zu3Lmqbdu2auvWraqwsFAtWbJE9evXz3CeLVu2qKSkJKXX61VUVJTy9fVVOp1OKaXU2rVrVZs2bdQ333yjioqKVF5enoqPj1d79+5VBQUFKjU1VT333HNq5syZpf6PjBw5UqWkpKikpCTVvXt3FRYWpk6fPq0KCgrU2LFj1bx585RSSiUlJamuXbuqXbt2Kb1er/bu3au6du2qUlNTy2zLnJwc1adPH7VmzRpVVFSkTp06pbp27arOnz+vlFLq7bffVh07dlSHDx9Wer1e5efnK39/f3Xo0CGllFIZGRnq1KlT920rUX5y5W4iJk+ezLfffktaWlqp7bt27cLT05OwsDAsLCwYOnQoXl5e7Ny503DMiBEjaNGiBRYWFlhaWgIQHh5OkyZNcHBwoE+fPjRu3JiePXtiYWHBoEGDOHPmTJl1bNmyhYCAAPz9/bG0tGTcuHHk5+dz7Nixh/4bbG1t6d+/P5s3bwYgPj6ey5cvExgYiFKK1atX8+677+Ls7Iy9vT2vvvoqUVFRAKxZs4aIiAj8/f0xMzPDzc2NZs2alXme8rbJXSdOnCA3N5fx48djZWVFjx496Nevn6GWR+Ht7c2gQYOwtLQkMjKSwsJCTpw4AcDgwYNxc3PDzMyMIUOG4OnpSVxcnOG9Wq2WsWPHYmFhgY2NDZ6envj7+2NlZYWLiwuRkZEcOnSo1PnGjBlDvXr1cHNzo3Pnzvj4+NC2bVusrKwYMGCA4fu5ceNG+vTpQ0BAAGZmZvj7+9OuXTt2795937Zs2LAhERERWFhY4O3tzcCBA/npp58Mx/Tv359OnTphZmaGtbU1FhYWXLx4kezsbJycnPD29n7kdhOPrvI7BUWVaNmyJX379mXRokWlQi05OZkGDRqUOrZBgwbodDrDaw8Pj3s+r169eoavra2tS722sbEpdQP3t35/PjMzMzw8PEqd70FCQ0P58MMPef3119m8eTNBQUHY2tqSmppKXl5eqW4b9Zv+8MTERAICAh7pHOVtk9++393dHTOz/782+v37H8bd3d3w9d1fRsnJyQBs2LCBr776ihs3bgCQm5tLenp6me8FSE1NZebMmRw+fJicnByUUjg6OpY65lG/nzdv3uTHH38s9YuuuLiYbt26lfnvuHHjBnFxcXTu3NmwTa/XM2zYMMPr37fl3LlzmT9/Pp988gmtWrVi2rRp+Pn5lfn5ovwk3E3I5MmTGTFiBC+99JJhm1ar5ebNm6WOS0xMpHfv3obXGo2mwmrQarWcP3/e8FopRWJiIm5ubo/0fn9/f9LT0zl79iybN29m+vTpANStWxcbGxuioqLK/CwPDw+uXbv2yDU+SZtotVqSkpIoKSkxBHxiYiJNmzZ9pPMDJCUlGb4uKSlBp9Oh1Wq5ceMG77//Pl9//TV+fn6Ym5szfPjwUu/9fW2ffPIJGo2GH374gbp167J9+3ZDf/jj8vDwYPjw4cycOfORj+/SpQtfffXVI5/Dx8eH+fPnU1RUxHfffceUKVPu+5eBKD/pljEhnp6eDBkyhP/85z+GbQEBAcTHx7Np0yaKi4vZsmULFy9epG/fvpVSw+DBg9m9ezcHDhygqKiIZcuWYWVl9chXZhYWFgwcOJCPPvqIzMxM/P39gTtXtyNHjmT27NmkpqYCoNPp2LNnDwBPP/0069at48CBA4awvHsjt169eqVuij5pm/j4+GBra8uSJUsoKioiNjaWHTt2MGTIkEdtJk6fPs22bdsoLi5m+fLlWFlZ4evrS15eHhqNBhcXFwDWrl3LhQsXHvhZOTk51KlTB0dHR3Q6HUuWLHnkOn5v2LBh7Ny5kz179qDX6ykoKCA2Ntbwy+j3bdm3b1/i4+PZsGEDRUVFFBUVERcXZ2j73yssLOSHH37g9u3bWFpaYmdnh7m5ebnrFfcn4W5iJk6cWKrLpG7duixYsICvvvqKbt26sWTJEhYsWGAIj4rm5eXFP//5T/7xj3/QvXt3du7cyYIFC7CysnrkzwgNDWX//v0MGjSo1HDCN998E09PT5555hk6duzIiy++aBhx4+PjwwcffMDs2bPp1KkTY8aMMVydv/DCC/z000906dKFmTNnPnGbWFlZMX/+fGJiYujevTt///vf+eijj+7bx1+W/v37s2XLFrp06cLGjRuZN28elpaWNG/enJdeeolRo0bRs2dPzp8/T8eOHR/4Wa+//jpnzpyhc+fOjB8/nuDg4Eeu4/c8PDz48ssvWbhwIT169CAgIIClS5caur9+35b29vYsXbqULVu20Lt3b3r16sXHH39MYWHhfc+xceNGAgMD6dixIytXruSjjz4qd73i/jRKyWIdQlSlefPmcfXqVT7++GNjlyJMmFy5CyGECZJwF0IIEyTdMkIIYYLkyl0IIUyQhLsQQpigavEQU0rKbWOXIIQQNU79+g733SdX7kIIYYIk3IUQwgRJuAshhAl6aLgnJiYyduxYBg8eTEhICMuXLwcgIyODyMhIgoODiYyMLDXZ/2+tX7+e4OBggoODWb9+fcVWL4QQokwPHeeenJxMSkoK3t7eZGdnExERwb///W/WrVuHs7Mz48ePZ9GiRWRmZvLmm2+Wem9GRgYRERGsXbsWjUZDeHg469atw8nJqdRxckNVCCEe3xPdUNVqtYbJ9O3t7fHy8kKn0xEdHU1YWBgAYWFhbN++/Z737t27F39/f5ydnXFycsLf398wi58QQojK81h97tevX+fs2bP4+vqSmpqKVqsF7vwC+P0KQHBnStbfLizg5ub2WAsaCCGEKJ9HDvecnBwmT57Mu+++i729/SO9p6wen4pcGEIIIUTZHinci4qKmDx5MqGhoYa5ol1dXQ3LgiUnJ5c5F7a7u3upFWfurjZTUQ5eTSdi2SGuppW95JsQQtRWDw13pRTvvfceXl5eREZGGrYHBgayYcMG4M6aj/3797/nvb169WLv3r1kZmaSmZnJ3r176dWrV4UV36SuLbfzi5my/hTpufdfHEAIIWqbh46WOXz4MM8//zwtW7Y0rBc5depUfHx8mDJlComJiXh4ePD555/j7OzMyZMnWblyJbNmzQLurEq/cOFCACZMmEBERMQ953iS0TInb2bx2uo4Wmnt+XKkD9YWMnRfCFE7PGi0TLWY8vdJh0JGn0/hnU1nCWpZn1lDW2Mm/fpCiFrA5OeW6d+yPpP7PMX28yl8uTfe2OUIIYTRVYtZISvCmM6NuJGZz/KDCTR0smGEj4exSxJCCKMxmXDXaDT8KbA5iVn5zNl+AXdHa3o0fbTV7IUQwtSYRLfMXRZmGmYPbYNXPTumbzrLr8nZxi5JCCGMwqTCHcDOyoLPRrTDzsqcP647xc3MfGOXJIQQVc7kwh3AzcGaeU+3p0hfwqS1J8nILTJ2SUIIUaVMMtwBvFzt+DTMG93tAt7YcIq8Ir2xSxJCiCpjsuEO4NvQiZlDWnMm6TbTN52lWF9i7JKEEKJKmHS4A/RtUY+3+zdn35U0Zv98oczJzIQQwtSYzFDIBwn3bcCtnEIWH7hGfXsrXuv1lLFLEkKISlUrwh3glR6epGQXsiw2AVc7a57xa2DskoQQotLUmnDXaDS8HdSCtNwiPt5xEVc7S/q3rG/ssoQQolKYfJ/7b1mYaZgV0pr2DRz585ZzHEnIMHZJQghRKWpVuAPYWJrzaZg3jZxtmbbhNGd1sji3EML01LpwB3CyteSLiPY42lgwee0p4lNlJSchhGmpleEOoHWw5ounfTDTwOtrT5KUJdMUCCFMR60Nd7izTN/ciPZkFxTz+pqTslSfEMJk1OpwB2iltedfI9qRdLuAP647RXZBsbFLEkKIJ1brwx2gQyMn5oS25XxKDn/aeJqCYpmmQAhRsz10DdXp06eza9cuXF1d2bx5MwBTpkzhypUrANy+fRsHBwc2btx4z3sDAwOxs7PDzMwMc3Nz1q1bV+Y5nnQN1Yry49lk/rLlHL2buTJnWFsszGQtViFE9fWgNVQf+hBTeHg4Y8aM4e233zZs+9e//mX4+sMPP8Te3v6+71++fDkuLjVjRaRBbbRk5Rfzzx0XmbntPH8Z2FIW2xZC1EgP7Zbp0qULTk5OZe5TSrF161aGDh1a4YUZyzN+DXi1pydRp3V8uvOSTDQmhKiRnmj6gcOHD+Pq6krTpk3ve8y4cePQaDQ8++yzPPvss09yuiozrnsTbhcU879HbmBjac7EXk3RyBW8EKIGeaJw37x58wOv2lesWIGbmxupqalERkbi5eVFly5dnuSUVUKj0TAlwIuC4hKWH0zA1tKMcd09jV2WEEI8snKPlikuLubnn39myJAh9z3Gzc0NAFdXVwYMGEBcXFx5T1flNBoNb/VvTkhbLQv2XeW7w9eNXZIQQjyycof7/v378fLywt3dvcz9ubm5ZGdnG77et28fLVq0KO/pjMJMo+H9ga0IalmPf+2+zJrjN41dkhBCPJKHhvvUqVMZNWoUV65coU+fPqxevRqALVu2EBISUupYnU7HK6+8AkBqairPPfccw4YNY+TIkQQEBNCnT59K+CdULgszDTOGtKa3lwtzoi+y+XSSsUsSQoiHeug496pQXca5P0hBcQnTNpzi0LUMZoa0YUArmQteCGFcDxrnLk+oPiJrCzP+Odwb3//OBR9zKdXYJQkhxH1JuD8GW0tzPh3RjtZae97ZdIbY+HRjlySEEGWScH9M9tYWfB7ejqYudZi28TRHr8tqTkKI6kfCvRycbC354un2eDha88a605xKzDJ2SUIIUYqEezm51LHiy5E+uNhZMmntSU4nVf+bwkKI2kPC/QnUt7dm/kgfHG0seX1NHGck4IUQ1YSE+xNyd7RhwTM+OFpb8Pqak7LgthCiWpBwrwAejjYseNYXB2tzJq6WgBdCGJ+EewXxcLRh/jN3Av71NSc5JwEvhDAiCfcK1MDpTsDbWZkzUQJeCGFEEu4VrIGTDQt+E/C/6rKNXZIQohaScK8EdwO+jqU5E9fE8WuyBLwQompJuFeSBk42LHjWB1tLcyauloAXQlQtCfdK1NDJlvnP+NxZqm91HOcl4IUQVUTCvZI1crZlwX8D/g+r4+QmqxCiSki4V4G7AV/Hypw/rD4pc9EIISqdhHsVaeRsy8JnfXGyvfMk6/HrmcYuSQhhwiTcq5CHow0Ln/Glvr0Vk9ae5NA1mQ9eCFE5JNyrmNbBmgXP+NLQ2YY31p9m/5U0Y5ckhDBBEu5G4GpnxYKRvjR1qcOfNp5m90VZsk8IUbEeGu7Tp0+nR48eDB061LBt3rx59O7dm+HDhzN8+HB2795d5ntjYmIYOHAgAwYMYNGiRRVXtQlwrmPJlyPb07K+PW9vOsP2X1OMXZIQwoQ8NNzDw8NZsmTJPdtffPFFNm7cyMaNGwkICLhnv16vZ8aMGSxZsoSoqCg2b97MxYsXK6ZqE+Foc2dFp3buDrwXdZYtZ3TGLkkIYSIeGu5dunTBycnpsT84Li4OT09PGjdujJWVFSEhIURHR5erSFNmb23B3Ij2dGzkxN+2/soPJ5OMXZIQwgSUu8/9u+++IzQ0lOnTp5OZee+wPp1Oh7u7u+G1m5sbOp1cmZaljpU5n41oR7emdfnHtvOsPn7T2CUJIWq4coX76NGj+fnnn9m4cSNarZYPP/zwnmOUUvds02g05TldrWBjac4nw73p7eXCR9EX+eZggrFLEkLUYOUK93r16mFubo6ZmRkjR47k5MmT9xzj7u5OUtL/dzHodDq0Wm35K60FrCzMmDOsLcGt6jNvzxW+2HOlzF+SQgjxMOUK9+TkZMPX27dvp0WLFvcc0759e+Lj40lISKCwsJCoqCgCAwPLX2ktYWluxowhrQn38WD5wQTmRF+kRAJeCPGYLB52wNSpUzl48CDp6en06dOHSZMmcfDgQc6dOwdAw4YNmTFjBnDn6vz9999n8eLFWFhY8Je//IWXX34ZvV5PREREmb8ExL3MzTS8E9Qce2sLvjmUQHZBMX8b1AoLc3ksQQjxaDSqGvzdn5IiMyXez9ex1/j33nh6ebnwwdA22FiaG7skIUQ1Ub++w333yaVgNfditya8E9ScfZfT+OO6U2QXFBu7JCFEDSDhXgNE+DbgH0Nac+JmFn9YHUdGbpGxSxJCVHMS7jXEwDZaPh7elsupuYz//gTJtwuMXZIQohqTcK9Benm58nl4O5KzC3hl5XES0vOMXZIQopqScK9hOjV25suRPuQU6nl55XEupMi6rEKIe0m410Bt3R1YPKoDFmYaXll5giMJGcYuSQhRzUi411BPudZh6egOaO2tmbT2JDvOy5TBQoj/J+Feg7k72rB4lC+ttQ68s+ksa2TCMSHEf0m413BOtncW/fD3cmFO9EUW7IuX+WiEEBLupsDG0px/DvdmWDs3lv5yjdk/X6C4RAJeiNrsoXPLiJrBwkzD+8EtqWdnxbLYBNJyi5gV0lqmKxCilpIrdxOi0Wh4rddTvBnYjD2XUnl9zUky8+RpViFqIwl3E/SMX0NmDW3DGd1tXvn+BElZ+cYuSQhRxSTcTdSAVvWZG96e5NsFjFtxnMupOcYuSQhRhSTcTVjnJs4sfNYXvYJxK45z+Jo87CREbSHhbuJaae1ZNroD9e3uPOy05YwsUi5EbSDhXgs0cLJhyWhffBs68tetv7LkwFUZCy+EiZNwryUcbSyZF9GeIW21LNx/lZnbzlOsLzF2WUKISiLj3GsRS3Mz/jaoFR6ONiz95Rq62wV8GNoWe2v5byCEqZEr91pGo9Ewwb8pfw5uyeGETF5ZeQKdLPwhhMl56ALZ06dPZ9euXbi6urJ582YA5syZw86dO7G0tKRJkyZ88MEHODo63vPewMBA7OzsMDMzw9zcnHXr1pV5Dlkg2zhi49N5e9MZ6liZ89mIdrTS2hu7JCHEY3iiBbLDw8NZsmRJqW3+/v5s3ryZTZs20bRpUxYuXHjf9y9fvpyNGzfeN9iF8XRrWpclozqgAcavPMGB+DRjlySEqCAPDfcuXbrg5ORUaluvXr2wsLjTT9uhQweSkpIqpzpR6ZrXt+Pr5/1o5GzDG+tOsT4u0dglCSEqwBP3ua9du5Y+ffrcd/+4ceMIDw/n+++/f9JTiUpS396aRaN86da0LrN/vsBnuy6hl1klhajRnmiYxPz58zE3N2fYsGFl7l+xYgVubm6kpqYSGRmJl5cXXbp0eZJTikpiZ2XBJ2Ht+NeuS/zvkRtcS8/jH0Nay0gaIWqocl+5r1+/nl27dvHxxx+j0WjKPMbNzQ0AV1dXBgwYQFxcXHlPJ6qAhZmGPwU2552g5hy4ksbLK49zM1MmHROiJipXuMfExLB48WLmz5+Pra1tmcfk5uaSnZ1t+Hrfvn20aNGi/JWKKhPh24C5Ee1Jvl3Ii98d48SNTGOXJIR4TA8dCjl16lQOHjxIeno6rq6uTJo0iUWLFlFYWIizszMAvr6+zJgxA51Ox/vvv8/ixYtJSEhg4sSJAOj1eoYOHcprr71W5jlkKGT1FJ+Wy7QNp0nMyuf94JYMaetm7JKEEL/xoKGQDw33qiDhXn1l5hXxzuazHL6WwYtdG/Nar6aY3acbTghRtZ5onLuo3ZxsLZkX3o4RPu58fTCBt384Q16R3thlCSEeQq7cxSNRSrHy2E3+tesSzevZ8emIdrg5WBu7LCFqNblyF09Mo9EwumNDPh3RjhuZ+fyP3GgVolqTcBePxf8pF5aO7oCtpRkTVsWxQZ5oFaJaknAXj61ZPTuWP+9H5ybOzPr5AnO2X6BI5oYXolqRPndRbvoSxZd7r/DNoev4NXTkg9C2uNpZGbssIWoNGQopKtW2c8nM+Ok8TjYW/HO4N23d7/8fTghRceSGqqhUwa21LB3VATONhvHfn5BFuIWoBiTcRYVo5WbPN2P8aOfhwF+3/spnuy5RLDNLCmE00i0jKlSxvoTPY66w8ugNOjdx5oOhbXC2tTR2WUKYJOlzF1Vu06kkPtx+gXp2Vnw0zJtWbrKEnxAVTfrcRZULbefOomd9KS5RvLTiGD+clNW6hKhKcuUuKlV6biHvRZ3j0LUMhrdz583+zbG2kGsKISqCdMsIo9KXKBbtj2dZbAKttfZ8OKwNDZ3KXgdACPHoJNxFtbDnUip/3forADOGtKKXl6uRKxKiZpM+d1Et9G7myjdj/GjgZMMb608zf1+8LMQtRCWRK3dR5fKL9Hy84xIbTyXRtYkzM0NaU7eOTFsgxOOSbhlRLW08mchH0RdxtrXkw9C2tG/gaOyShKhRpFtGVEvD23uwbLQfFuZmjP/+BCuO3qAaXGsIYRLkyl0YXVZ+EX/b+it7LqfRt7kr7we3xEmeahXioZ74yn369On06NGDoUOHGrZlZGQQGRlJcHAwkZGRZGaWvSrP+vXrCQ4OJjg4mPXr1z9m6aI2cLSx5JMwb97o68Xey2mM+c9RTt7MMnZZQtRojxTu4eHhLFmypNS2RYsW0aNHD7Zt20aPHj1YtGjRPe/LyMjgiy++YNWqVaxevZovvvjivr8ERO2m0Wh4rlMjlozugJmZhle+P8F/DiVQYvw/LIWokR4p3Lt06YKTk1OpbdHR0YSFhQEQFhbG9u3b73nf3r178ff3x9nZGScnJ/z9/dmzZ08FlC1Mlbe7A9+O6UhAM1fmxlxh6vrTZOQWGbssIWqcct9QTU1NRavVAqDVaklLS7vnGJ1Oh7u7u+G1m5sbOp3M9S0ezMHGgg9D2/BW/+YcvJbOc/85wtHrGcYuS4gapVJHy5R1r1aj0VTmKYWJ0Gg0jOzQgK+e88PW0pzXVsWx+MBVeehJiEdU7nB3dXUlOTkZgOTkZFxcXO45xt3dnaSk/58NUKfTGa72hXgUrbR3FgEZ2FrLov1XmbDqBIlZ+cYuS4hqr9zhHhgYyIYNGwDYsGED/fv3v+eYXr16sXfvXjIzM8nMzGTv3r306tWr/NWKWsnOyoIZQ1ozY0grLqTk8Nw3R9h2LtnYZQlRrT3SOPepU6dy8OBB0tPTcXV1ZdKkSQQFBTFlyhQSExPx8PDg888/x9nZmZMnT7Jy5UpmzZoFwJo1a1i4cCEAEyZMICIi4p7Pl3Hu4lHdyMzjz1G/cjIxiyFttbwZ2Bx7awtjlyWEUcj0A8KkFJcolv1ylaW/XMPD0YZ/DGktUxeIWknCXZikEzcy+fOWcyTfLuDlHp5EdmuCuZncsBe1h4S7MFnZBcV8uP0CP51LwbeBI38b3IpGzrIQiKgdJNyFydtyRsc/d1xEX6L4Y4AX4T4eMuxWmDwJd1ErJGXlM3PbeWKvZtC9aV3+HNwSrYO1scsSotJIuItaQynFmhOJzN19GUtzM/4U2IzBbbRyFS9MkoS7qHUS0vP424+/Enczi34t6jE9qLms9iRMjoS7qJX0JYrvDl9nwf54HKwteHdACwKa1zN2WUJUGAl3UatdTMnhr1vPcT4lhxBvN/7Ur5k8+CRMgoS7qPWK9CUs+eUay2Ov4WpnxbvBLfF/6t75kISoSSTchfiv04lZ/P2n81xJzWVIWy1v9G2Gsywo1YstAAAT3ElEQVTpJ2ooCXchfqOwuIRlsdf4+mACTjYWvNW/Of1b1jd2WUI8Ngl3IcpwPjmbmdvOc1aXTb8W9XgrsBn17GVcvKg5JNyFuI/i/46oWbQ/HmsLc97o68VQbzcZFy9qBAl3IR4iPi2XWdvOc/xGFt2b1uXt/s1ljhpR7Um4C/EISpRizfFE/r3nCnqlGNe9CWM6N8LSvFJXoxSi3CTchXgMutsFfLLzEjsv3OIp1zpMD2qBXyMnY5clxD0k3IUohz2XUvnnjoskZhUwrJ0bk/p4ybBJUa1IuAtRTnlFepYcuMp3R25gb2XOHwPkhquoPiTchXhCF1Ny+GD7BeJuZtGxkRPvBLXgKdc6xi5L1HKVEu6XL1/mjTfeMLxOSEhg8uTJvPjii4ZtsbGx/OEPf6BRo0YADBgwgNdff/2ez5JwFzVBiVJsPJnEvJgr5BXpeaFLIyK7NcHG0tzYpYlaqtKv3PV6PX369GHVqlU0bNjQsD02NpZly5axcOHCB75fwl3UJGm5hfxr12W2nk3Gw9GaN/o2o29zV+mqEVXuQeFeIWO8Dhw4QOPGjUsFuxCmyqWOFTOGtGbBMz7UsTLnrR/OMGntSeJTc41dmhAGFRLuUVFRDB06tMx9x48fZ9iwYbz88stcuHChIk4nRLXQqbEz347txLR+zTiddJtR3xzh892XySksNnZpQjx5t0xhYSG9e/cmKiqKevVKL4SQnZ2NRqPBzs6O3bt3M2vWLLZt23bPZ0i3jKjp0nIL+feeK/xwSkc9Oysm9XlKlvcTla5Su2ViYmLw9va+J9gB7O3tsbOzAyAgIIDi4mLS0tKe9JRCVDsudaz488BWfPVcB7QO1vx166+8svIEv+qyjV2aqKWeONyjoqIICQkpc19KSgp3/zCIi4ujpKSEunXrPukphai22nk48tVzHXg/uAVX0/MY++1RZvz4K7eyC4xdmqhlnqhbJi8vj759+7J9+3YcHO78ebBixQoARo8ezbfffsuKFSswNzfHxsaGd955h44dO97zOdItI0zR7fxilsVeY+XRG1iaa3ixaxOe69RQhk6KCiMPMQlhRAnpecyNucyui6m4O1gzqc9TDGhVX/rjxROTcBeiGjiSkMGnOy9xPiWH9h6OTO3nRTsPR2OXJWowCXchqgl9iSLqtI4v98WTmlPIwNb1meDfVOaOF+Ui4S5ENZNTWMw3BxP47sgN9CWKCF8PxnVvQt06VsYuTdQgEu5CVFMp2QUsPnCVH04mYW1hzpgujXi+UyPqWMlNV/FwEu5CVHPxqbl8uS+enRdu4VLHkpd7eDKivTsWsgqUeAAJdyFqiJM3s5i35wrHrmfS2NmG13o9RVDLejKyRpRJwl2IGkQpxf4r6czbc5lLt3Jp42bP672foqunPAAoSpNwF6IG0pcofjybzIJ98STdLqBTYyde7dlU1nMVBhLuQtRgBcUlbIhL5KuDCaTmFNLN05lXezalfQMZI1/bSbgLYQLyi/SsOZHINwcTSM8rwv8pF17196SN2/1/wIVpk3AXwoTkFupZdewG3x6+TmZ+MX2buzK+pyct6tsbuzRRxSTchTBB2QXFrDx6g++OXCe7QE9Qy3q80tMTL1c7Y5cmqoiEuxAmLCu/iP89coOVR2+QW6inf8v6vNS9sVzJ1wIS7kLUAhl5RfzvkeusOnaTnEI9fZq58lL3Jni7S5+8qZJwF6IWycovYtWxm6w4eoOs/GK6N63LuG5N6CBDKE2OhLsQtVBOYTFrjyfy3ZHrpOUW0bGREy91b0LXJs7yxKuJkHAXohbLL9Kz4WQS/zmUQHJ2Ie09HHipexP8n3KRkK/hJNyFEBQWl7D5dBLLDyZwM6uA5vXsGNulEcGt6ssEZTWUhLsQwqBYX8JP51L4z+EELt3Kxc3BmtEdGxLm446dlYWxyxOPQcJdCHEPpRT749P5z6EEjiRkYm9tztO+DXi2Y0Pq2cmiITVBpYZ7YGAgdnZ2mJmZYW5uzrp160rtV0oxa9Ysdu/ejY2NDR9++CHe3t6ljpFwF8K4Tifd5ttDCey4cAtzMw1D2roxpnMjmrrUMXZp4gEeFO4V8jfY8uXLcXFxKXNfTEwM8fHxbNu2jRMnTvC3v/2N1atXV8RphRAVxNvdgQ9C23I9I4/vDl9n02kdP5xMok8zV57v3IgODR3l5msNU+kdbNHR0YSFhaHRaOjQoQNZWVkkJyej1Wor+9RCiMfUyNmWt4NaML6nJ6uP32TVsZvsvpRKK609ozo2ILiVFisLuflaE1TId2ncuHGEh4fz/fff37NPp9Ph7u5ueO3u7o5Op6uI0wohKkndOlaM79mUzeO7MX1ACwr1Jfz9x/OELo5l4b54buUUGrtE8RBPfOW+YsUK3NzcSE1NJTIyEi8vL7p06WLYX1aXvvx5J0TNYGNpTriPByPau3PwWgYrj95g6S/X+PpgAsGt6zOqY0OZcriaeuJwd3NzA8DV1ZUBAwYQFxdXKtzd3d1JSkoyvE5KSpIuGSFqGI1GQzfPunTzrEtCeh7fH7vBplM6tpxJxreBI6M6NqRvi3pYmMmFW3XxRN0yubm5ZGdnG77et28fLVq0KHVMYGAgGzZsQCnF8ePHcXBwkHAXogZrXNeWPwU2J+rVbrzR14tbOYVM33yW4YtjWbz/Ksm3C4xdouAJh0ImJCQwceJEAPR6PUOHDuW1115jxYoVAIwePRqlFDNmzGDPnj3Y2toye/Zs2rdvX+pzZCikEDWXvkSx93Iaa07c5Jf4dMw10LuZK+G+HnTzrIuZdMNWGnmISQhRJa5n5LE+LolNp5JIzyuioZMNI3w8CG3nhksdeTCqokm4CyGqVGFxCbsu3mLtiUSOXs/EwkxDYIt6hPt60LGRkwyqqCAS7kIIo7mSmsu6uESiTuu4XVBMUxdbhrVzZ3BbN5nm4AlJuAshjC6/SM/Pv6aw8WQSJ25mYa4Bfy9XhrVzw/8pF5mZshwk3IUQ1Up8Wi6bTumIOqMjNacQlzqWDG7jxrD2brLA92OQcBdCVEvFJYoDV9L44VQSey6noS9ReLs7MKStGwNa1aOu3IR9IAl3IUS1l5ZbyI9nk9l0SsfFWzmYm2no0bQug9to6dPMFRtLc2OXWO1IuAshapQLKdlsPZPMT+eSSc4uxM7KnMAW9RjcVkvHRs6Yy5OwgIS7EKKG0pcojl7PYOuZZHZcuEVOoR6tvRX9W9YnqFV92nk41OqHpCTchRA1Xn6Rnj2X0/jxbDIH4tMo0iu09lYEtapP/5a1M+gl3IUQJiW7oJiYS6ls/zWFX66mU6RXuDlY079lPYL+G/S14UEpCXchhMm6G/Q//5pC7O+CPrBFPdp5OJpsH72EuxCiVrid/98r+vMp/BKfTnGJoq6tJb2budCnmSvdPOua1KgbCXchRK2TXVDM/itpxFxKZd+VNLIL9FhbmNHNsy4BzVzp1cylxk9mJuEuhKjVivQlHL2eSczFVGIupZJ0uwAN0L6BIwHNXOndzJWmLrY1rp9ewl0IIf5LKcX5lBxD0J9LvrPgkIejNT2fcqHnUy50aeKMbQ3ovpFwF0KI+0jKymf/lTT2X0nn4LV08opKsDTX4NfQyRD21fWqXsJdCCEeQWFxCcdvZLL/Sjr749O4kpoL3Lmq79qkLl09nenSxLnazHkj4S6EEOWQmJXPgStpHIhP53BCBtkFegBa1rejm+edsO/Q0MloI3Ak3IUQ4gnpSxTndLc5eC2Dg1fTOXEziyK9wtJcg28DRzo1dqZTY2e83R2wsqiauekl3IUQooLlFek5fiOTg1czOHQtg/PJ2SjA2sIMnwaOdGrsRKdGzrRxd8C6ksK+UsI9MTGRt956i1u3bmFmZsYzzzzD//zP/5Q6JjY2lj/84Q80atQIgAEDBvD666/f81kS7kKImi4rv4hj1zM5kpDJkYQMLqTkoABLcw2ttQ74NnTEt4EjPg0dK2x8faWEe3JyMikpKXh7e5OdnU1ERAT//ve/ad68ueGY2NhYli1bxsKFCx/4WRLuQghTk5lXxPEbmZy4kcWJm1mc1d2mSH8nbhs72+DT0ImODZ0Y4u2GRTmnR3hQuFuU6xMBrVaLVqsFwN7eHi8vL3Q6XalwF0KI2srJ1pKA5vUIaF4PgILiEs7pbhvCfu+lVKJO69A6WNG9qUuFn7/c4f5b169f5+zZs/j6+t6z7/jx4wwbNgytVsvbb79NixYtKuKUQghRo1hbmOHb0Anfhk7AnYepMvKKKm1Y5RPfUM3JyWHs2LFMmDCB4ODgUvuys7PRaDTY2dmxe/duZs2axbZt2+75DOmWEUKIx/egbpknuoVbVFTE5MmTCQ0NvSfY4U53jZ3dnZXMAwICKC4uJi0t7UlOKYQQ4hGUO9yVUrz33nt4eXkRGRlZ5jEpKSnc/cMgLi6OkpIS6tatW95TCiGEeETl7nM/cuQIGzdupGXLlgwfPhyAqVOncvPmTQBGjx7NTz/9xIoVKzA3N8fGxoZPP/20Ws7PIIQQpkYeYhJCiBqq0vrchRBCVE8S7kIIYYIk3IUQwgRViz53IYQQFUuu3IUQwgRJuAshhAmScBdCCBNUo8M9JiaGgQMHMmDAABYtWmTscoA789yPHTuWwYMHExISwvLlywHIyMggMjKS4OBgIiMjyczMNGqder2esLAwXn31VQASEhIYOXIkwcHBTJkyhcLCQqPWl5WVxeTJkxk0aBCDBw/m2LFj1a4Nv/76a0JCQhg6dChTp06loKDA6O04ffp0evTowdChQw3b7tduSilmzpzJgAEDCA0N5fTp00arcc6cOQwaNIjQ0FAmTpxIVlaWYd/ChQsZMGAAAwcOZM+ePUar8a6lS5fSqlUrw1QqxmrHh1I1VHFxserfv7+6du2aKigoUKGhoerChQvGLkvpdDp16tQppZRSt2/fVsHBwerChQtqzpw5auHChUoppRYuXKg++ugjY5apli1bpqZOnarGjx+vlFJq8uTJavPmzUoppf785z+r7777zpjlqbfeekutWrVKKaVUQUGByszMrFZtmJSUpPr166fy8vKUUnfab+3atUZvx4MHD6pTp06pkJAQw7b7tduuXbvUuHHjVElJiTp27Jh6+umnjVbjnj17VFFRkVJKqY8++shQ44ULF1RoaKgqKChQ165dU/3791fFxcVGqVEppW7evKleeukl1bdvX5WamqqUMl47PkyNvXKPi4vD09OTxo0bY2VlRUhICNHR0cYuC61Wi7e3N1B6nvvo6GjCwsIACAsLY/v27UarMSkpiV27dvH0008Dd648fvnlFwYOHAjAiBEjjNqW2dnZHDp0yFCflZUVjo6O1aoN4c5fP/n5+RQXF5Ofn0/9+vWN3o5dunTBycmp1Lb7tdvd7RqNhg4dOpCVlUVycrJRauzVqxcWFndmQ+nQoQNJSUmGGkNCQrCysqJx48Z4enoSFxdnlBoBPvjgA958881S06gYqx0fpsaGu06nw93d3fDazc0NnU5nxIru9dt57lNTUw2Lm2i1WqPOjjl79mzefPNNzMzufPvT09NxdHQ0/HC5u7sbtS0TEhJwcXFh+vTphIWF8d5775Gbm1ut2tDNzY2XXnqJfv360atXL+zt7fH29q5W7XjX/drt9z9D1aXetWvX0qdPH6B6/ZxHR0ej1Wpp3bp1qe3VtR1rbLirMobnV6dJyXJycpg8eTLvvvsu9vb2xi7HYOfOnbi4uNCuXbsHHmfMtiwuLubMmTOMHj2aDRs2YGtrW23uqdyVmZlJdHQ00dHR7Nmzh7y8PGJiYu45rjr9n/y96vgzNH/+fMzNzRk2bBhQfWrMy8tjwYIF/PGPf7xnX3Wp8fcqZCUmY3B3dzf86QZ3fnvevToxtrLmuXd1dSU5ORmtVktycjIuLhW/rNajOHr0KDt27CAmJoaCggKys7OZNWsWWVlZFBcXY2FhQVJSklHb0t3dHXd3d8PKXoMGDWLRokXVpg0B9u/fT6NGjQw1BAcHc+zYsWrVjnfdr91+/zNk7HrXr1/Prl27+Prrrw3hWF1+zq9du8b169cNM+AmJSURHh7O6tWrq1073lVjr9zbt29PfHw8CQkJFBYWEhUVRWBgoLHLuu8894GBgWzYsAGADRs20L9/f6PUN23aNGJiYtixYweffvop3bt355NPPqFbt2789NNPwJ0fMmO2Zf369XF3d+fy5csAHDhwgGbNmlWbNgRo0KABJ06cIC8vD6UUBw4coHnz5tWqHe+6X7vd3a6U4vjx4zg4OBgtlGJiYli8eDHz58/H1ta2VO1RUVEUFhaSkJBAfHw8Pj4+VV5fq1atOHDgADt27GDHjh24u7uzbt066tevX63a8bdq9PQDu3fvZvbs2ej1eiIiInjttdeMXRKHDx/m+eefp2XLloY+7alTp+Lj48OUKVNITEzEw8ODzz//HGdnZ6PWGhsby7Jly1i4cCEJCQm88cYbZGZm0qZNGz7++GOsrCpnbcdHcfbsWd577z2Kiopo3LgxH3zwASUlJdWqDefOncuWLVuwsLCgTZs2zJo1C51OZ9R2nDp1KgcPHiQ9PR1XV1cmTZpEUFBQme2mlGLGjBns2bMHW1tbZs+eTfv27Y1S46JFiygsLDR8P319fZkxYwZwp6tm7dq1mJub8+677xIQEGCUGkeOHGnYHxgYyJo1a3BxcTFaOz5MjQ53IYQQZaux3TJCCCHuT8JdCCFMkIS7EEKYIAl3IYQwQRLuQghhgiTchRDCBEm4CyGECZJwF0IIE/R/T+Vsv3VZXasAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nSteps = 150\n",
    "learningRate = 0.5\n",
    "regType = 'lasso'\n",
    "regParam = 0.02\n",
    "\n",
    "Loss_save = []\n",
    "Model_norm = []\n",
    "#broadcast model\n",
    "model = BASELINE #model = sc.broadcast(BASELINE) #substitute this line with the comment when deploying it on the cloud\n",
    "for idx in range(nSteps):\n",
    "    #print(\"----------\")\n",
    "    #print(f\"STEP: {idx+1}\")\n",
    "    \n",
    "    # compute loss\n",
    "    loss = LogLoss(numPlusCatRdd, model, regType=regType, regParam=regParam)\n",
    "    # update model parameters\n",
    "    model = GDUpdate(numPlusCatRdd, model, regType=regType, regParam=regParam, learningRate=learningRate)\n",
    "    \n",
    "    #store results\n",
    "    Loss_save.append(loss)\n",
    "    Model_norm.append(np.linalg.norm(model))\n",
    "    \n",
    "    #broadcast model\n",
    "    #model = sc.broadcast(model) #uncomment this line when deploying it on the cloud\n",
    "\n",
    "print(f\"The estimated model is: {model}\")\n",
    "print(f\"The loss of the estimated model is: {loss}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(Loss_save)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.plot(Model_norm)\n",
    "plt.title('Norm of vector of parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of true positives is 2415.0\n",
      "number of predicted positives is 129\n"
     ]
    }
   ],
   "source": [
    "print(f'number of true positives is {numPlusCatRdd.map(lambda x: x[0]).sum()}')\n",
    "res = makePrediction(numPlusCatRdd, model).cache()\n",
    "print(f'number of predicted positives is {res.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Multi-column Feature Hashing\n",
    "Hashing each column independently resulted in a dimensionality reduction from several million vectors to 416 for the categorical variables. We could further reduce dimensionality through multi-column Feature Hashing with a likely tradeoff being the loss of accuracy (TBD). \n",
    "\n",
    "We used the multi-column FeatureHashing functionality in Apache Spark MLLib to look into how dimensionality reduction to fewer vectors for logistic regression. Multi-column FeatureHashing would be very handy when we start looking at adding in quadratic terms for logistic regression. Instead of a (416,416) quadratic feature matrix, we could deal with a smaller (32,32) or (64,64) features, leading to faster computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark MLLib FeatureHasher example\n",
    "The FeatureHasher in spark takes multiple columns of categorical (and numerical) variables and hashes them down to fewer features. It is possible to specify the number of output features so that we can restrict the dimensions to a more manageable number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature hashing example\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "dataset = spark.createDataFrame([\n",
    "    (2.2, True, \"1\", \"foo\"),\n",
    "    (3.3, False, \"2\", \"bar\"),\n",
    "    (4.4, False, \"3\", \"baz\"),\n",
    "    (5.5, False, \"4\", \"foo\")\n",
    "], [\"real\", \"bool\", \"stringNum\", \"string\"])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-column Feature Hashing with test set\n",
    "We applied Multi-column Feature Hashing to get to a dimensionality of 64 vectors using FeatureHasher. The results of the logistic regression following this hashing are given below (need to add time, accuracy etc...How do you compare otherwise?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sample training data and convert to dataframe\n",
    "train_sample1 = sc.textFile('data/sample_training.txt')\\\n",
    "                 .map(lambda x: x.split('\\t'))\\\n",
    "                 .toDF().limit(10000).cache()\n",
    "train_sample1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose categorical columns to Hash\n",
    "hashInpList = []\n",
    "for c in range(14,41):\n",
    "    col = \"_\"+str(c)\n",
    "    hashInpList.append(col)\n",
    "print (hashInpList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Multi-column hashing with 32 output features\n",
    "hasher = FeatureHasher(numFeatures=256, inputCols=hashInpList,outputCol=\"features\")\n",
    "\n",
    "featurized = hasher.transform(train_sample1)\n",
    "featurized.select(\"features\").show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized.show(3)\n",
    "featurized.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into a dense vector format to feed logistic regression model\n",
    "def extractVec (elem):\n",
    "    return(np.array(tuple(elem.features.toArray().tolist())))\n",
    "    \n",
    "multiHashCatRdd = featurized.select(\"features\").rdd.map(extractVec)\n",
    "#map(extractVec)\n",
    "multiHashCatRdd.take(3)\n",
    "multiHashCatRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now include this into the logistic regression model\n",
    "#Zip numerical and categorical variables\n",
    "\n",
    "def mergeNumPlusCatRdds(elem):\n",
    "    x, y = elem\n",
    "    xkey , xval = x\n",
    "    merge =  (xkey, np.hstack((xval,y)))\n",
    "    return(merge)\n",
    "\n",
    "numPlusCatRdd2 = normedRDD.zip(multiHashCatRdd) \\\n",
    "                         .map(mergeNumPlusCatRdds)\n",
    "numPlusCatRdd2.take(1)\n",
    "numPlusCatRdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of features\n",
    "numb_features = np.size(numPlusCatRdd2.first()[1])\n",
    "print(numb_features)\n",
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSteps = 100\n",
    "learningRate = 0.5\n",
    "regType = None\n",
    "regParam = 0.05\n",
    "\n",
    "Loss_save = []\n",
    "Model_norm = []\n",
    "#broadcast model\n",
    "model = BASELINE #model = sc.broadcast(BASELINE) #substitute this line with the comment when deploying it on the cloud\n",
    "for idx in range(nSteps):\n",
    "    #print(\"----------\")\n",
    "    #print(f\"STEP: {idx+1}\")\n",
    "    \n",
    "    # compute loss\n",
    "    loss = LogLoss(numPlusCatRdd2, model, regType=regType, regParam=regParam)\n",
    "    # update model parameters\n",
    "    model = GDUpdate(numPlusCatRdd2, model, regType=regType, regParam=regParam, learningRate=learningRate)\n",
    "    \n",
    "    #store results\n",
    "    Loss_save.append(loss)\n",
    "    Model_norm.append(np.linalg.norm(model))\n",
    "    \n",
    "    #broadcast model\n",
    "    #model = sc.broadcast(model) #uncomment this line when deploying it on the cloud\n",
    "\n",
    "print(f\"The estimated model is: {model}\")\n",
    "print(f\"The loss of the estimated model is: {loss}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(Loss_save)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.plot(Model_norm)\n",
    "plt.title('Norm of vector of parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of true positives is {numPlusCatRdd2.map(lambda x: x[0]).sum()}')\n",
    "res = makePrediction(numPlusCatRdd2, model).cache()\n",
    "print(f'number of predicted positives is {res.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOY EXAMPLE - comparison with ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOY EXAMPLE STARTS HERE\n",
    "numb_features = 2\n",
    "\n",
    "N = 100\n",
    "D = 3\n",
    "\n",
    "X_toy = np.random.randn(N,D)\n",
    "\n",
    "# center the first 50 points at (-1,-1)\n",
    "X_toy[:50,:] = X_toy[:50,:] - 1*np.ones((50,D))\n",
    "\n",
    "# center the last 50 points at (2, 2)\n",
    "X_toy[50:,:] = X_toy[50:,:] + 2*np.ones((50,D))\n",
    "\n",
    "X_toy[:50,0] = 0\n",
    "X_toy[50:,0] = 1\n",
    "\n",
    "rdd1 = sc.parallelize(X_toy)\n",
    "rdd1 = rdd1.map(lambda x: [float(i) for i in x])\n",
    "toy_sample_red = rdd1.toDF([\"_1\", \"_2\", \"_3\"])\n",
    "toy_sample_red_RDD = toy_sample_red.rdd.map(lambda x: (x[0], x[1:])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSteps = 150\n",
    "learningRate = 0.5\n",
    "regType = 'ridge'\n",
    "regParam = 0.05\n",
    "\n",
    "Loss_save = []\n",
    "Model_norm = []\n",
    "#broadcast model\n",
    "model = BASELINE\n",
    "for idx in range(nSteps):\n",
    "    #print(\"----------\")\n",
    "    #print(f\"STEP: {idx+1}\")\n",
    "    \n",
    "    # compute loss\n",
    "    loss = LogLoss(toy_sample_red_RDD, model, regType=regType, regParam=regParam)\n",
    "    # update model parameters\n",
    "    model = GDUpdate(toy_sample_red_RDD, model, regType=regType, regParam=regParam, learningRate=learningRate)\n",
    "    \n",
    "    #store results\n",
    "    Loss_save.append(loss)\n",
    "    Model_norm.append(np.linalg.norm(model))\n",
    "\n",
    "print(f\"The estimated model is: {model}\")\n",
    "print(f\"The loss of the estimated model is: {loss}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(Loss_save)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.plot(Model_norm)\n",
    "plt.title('Norm of vector of parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual simulated value is 50\n",
    "res = makePrediction(toy_sample_red_RDD, model).cache()\n",
    "res.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = X_toy[:,1:],X_toy[:,0]\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',fit_intercept=True).fit(X, y)\n",
    "print(clf.coef_, clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_toy[:,1], X_toy[:,2], c=X_toy[:,0], s=100, alpha=0.5)\n",
    "x_axis = np.linspace(-6, 6, 100)\n",
    "y_axis = -(model[0] + x_axis*model[1]) / model[2]\n",
    "plt.plot(x_axis, y_axis)\n",
    "y_axis = -(clf.intercept_+x_axis*clf.coef_[0][0]) / clf.coef_[0][1]\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_cols = ['_1','_2','_3','_4','_5','_6','_7','_8','_9','_10','_11','_12','_13','_14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate train data for homegrown solution - select only 1000 rows and only numerical features + one categorical variable +target \n",
    "train_sample_red = train_sample.select(convert_cols + ['_23']).limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_red.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values with averages\n",
    "from pyspark.sql.functions import avg\n",
    "for col in convert_cols:\n",
    "    train_sample_red = train_sample_red.na.fill(round(train_sample_red.na.drop().agg(avg(col)).first()[0],1), [col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "df = train_sample_red.withColumn(\"_23\", split(col(\"_23\"),\" \"))\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cat_Vectorizer = CountVectorizer(inputCol=\"_23\", outputCol=\"_23_array\", vocabSize=4, minDF=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catVectorizer_model = cat_Vectorizer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe = catVectorizer_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StringIndexer(inputCols=[\"gender\"], outputCols=[\"gender_numeric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cardinality of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique categorical values\n",
    "from pyspark.sql.functions import col\n",
    "for col in train_sample.columns[14:]:\n",
    "    print('Column ' + col + ' has ' + str(train_sample.select(col).distinct().count()) \\\n",
    "          + ' unique categorical values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plots of selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of target variable\n",
    "hist_c1 = train_sample.select('_1').rdd.flatMap(lambda x: x).histogram(2)\n",
    "pd.DataFrame(list(zip(*hist_c1))).set_index(0).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 20\n",
    "hist_c20 = train_sample.groupBy('_20').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c20))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 23\n",
    "hist_c23 = train_sample.groupBy('_23').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c23))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 28\n",
    "hist_c28 = train_sample.groupBy('_28').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c28))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 31\n",
    "hist_c31 = train_sample.groupBy('_31').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c31))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 34\n",
    "hist_c34 = train_sample.groupBy('_34').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c34))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 36\n",
    "hist_c36 = train_sample.groupBy('_36').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c36))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimates of ctr based on category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 20\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_20').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_20').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 23\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_23').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_23').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 28\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_28').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_28').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 31\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_31').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_31').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 34\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_34').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_34').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 36\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_36').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_36').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
