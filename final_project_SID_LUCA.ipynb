{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team 24   \n",
    "Vivian Lu, Siddhartha Jakkamreddy, Venky Nagapudi, Luca Garre   \n",
    "Summer 2019, sections 4 and 5   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Introduction__\n",
    "Online ad is a multibillion dollar industry fueled by large investments and ever increasing performance goals. Targeted advertisement, based on users' demographics and browsing history as well as tailored ad features (i.e. colors, text, placement on webpage, display size), is receiving more and more interest due to its potential for revenue generation. As such, machine learning is proving resourceful in the understanding of which features have the greatest impact on users' Click-Through Rates (CTR) and thus informing ad design to maximize performance metrics such as click and convertion rates. Such machine learning solutions can also be deployed in a data pipeline environment (i.e. streaming) to select and offer the best ad on a user/event specific basis that will maximize profit. \n",
    "\n",
    "## __Goal of the analysis__\n",
    "The purpose of the present analysis is to estimate whether a given ad will be clicked based on a set of features (both numerical and categorical) describing the ad. Our analysis attempts to first delve into understanding the anonymized features on an exploratory data analysis, and following such an investigation, we attempt to construct a machine learning model that predict a binary outcome variable (1 for successful click through, 0 for unsuccessful click through) based on features that have shown significant association with the labelled outcome variable of click through rate (CTR). \n",
    "\n",
    "## __Description of the dataset__\n",
    "The dataset is provided by Criteo on Kaggle(https://www.kaggle.com/c/criteo-display-ad-challenge) and is composed of three files, a `readme.txt`, a `train.txt` and a `test.txt` file, respectively. The readme file contains a brief description of the data. The `train.txt` and `test.txt` files contain the train and test data. Both files are formatted as tab separated value tables, and amount to 45840617 and 6042135 rows for the train and test data, respectively. Following the description of the data, each row represents an ad and contains the following fields (see commands below, these expect the data to be contained in a data folder inside the current working directory):\n",
    "\n",
    "- 1 binary field indicating whether the ad has been clicked (1) or not (0). This field is available only for the train data;\n",
    "- 13 fields containing integer features representing counts;\n",
    "- 26 categorical features. These are hashed as 32 bits keys for anonymization purposes;\n",
    "\n",
    "From a printout of the first rows of the data files it appears that the data contain no headers. This implies that, with the sole exception of the first binary field, it is not possible to characterize the various fields in terms of the features these represent. It is also noted that rows in the data can have missing values. This is again noticed when looking at the printed lines, as these have a number of entries which is lower than the number of fields specified in the `readme.txt` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows in the train data\n",
    "!wc -l data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows in the test data\n",
    "!wc -l data/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row of the train data\n",
    "!head -1 data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row of the test data\n",
    "!head -1 data/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression belongs to the family of so-called generalized linear models and is by far one of the most known and applied algorithms for the prediction of a target variable $Y$, which represents the possible occurrence of an event of interest $e$. This variable is binary, and usually is encoded such that $Y=1$ represents the occurrence of $e$. More specifically, given a set of explanatory features $X_i$, $i = 1,2, \\dots, n$, logistic regression characterizes the probability of occurrence of $e$, $\\pi[e] \\equiv \\pi$, as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi = \\frac{1}{1 + \\exp^{-z} }\n",
    "\\end{equation}\n",
    "\n",
    "where $z = \\beta_0 + \\sum_{i=1}^n \\beta_i X_i$ and $\\beta_i$ are model parameters. As can be seen from the equation above, and in compliance with probability rules, $\\pi \\in (0, 1)$ for any $\\beta_i$ and $X_i$, owing to the fact that the exponential function is strictly positive, and considering that the denominator is always higher than the numerator. After some algebraic manipulations an equivalent, and more compact, formulation of the above equation can be obtained as:\n",
    "\n",
    "\\begin{equation}\n",
    "log\\left( \\frac{\\pi}{1-\\pi} \\right) = \\beta_0 + \\sum_{i=1}^n \\beta_i X_i\n",
    "\\end{equation}\n",
    "\n",
    "where the left side is usually referred to as the logit function, $logit(\\pi)$, while the right side makes the linear nature of this model explicit. This becomes clearer when considering the decision boundary, i.e., the hypersurface that segments the feature space in positive versus vegative regions. For logistic regression, such boundary is associated with the locus of points in the feature space where $\\pi=0.5$, i.e., the model has no preference as to whether a point in this locus should be assigned to the positive or the negative class. Casting $\\pi=0.5$ in the left side of the equation above renders a linear equation of the decision boundary in the feature space, in compliance with the linear nature of this model.  \n",
    "\n",
    "## Log-loss function and parameter estimation\n",
    "\n",
    "In accordance with established practices in the fields of statistics and machine learning, the parameters $\\beta_i$ of the logistic regression model are estimated via maximization of the log-likelihood function. In essence, for a sample of $m$ data points $(x_{ij}, y_j)$, $i = 1,2,\\dots,n$, $j = 1,2,\\dots,m$, where $x_{ij}$ is the $j$-th record of the $i$-th feature, and $y_j$ is the $j$-th record of the target binary variable $Y$, the parameters $\\beta_i$ are estimated such that the log-likelihood function:\n",
    "\n",
    "\\begin{equation}\n",
    "log\\left[ L(\\beta_i|y_j) \\right] = \\frac{1}{m} log\\left( \\prod_{j = 1}^{m} \\pi_j^{y_j} \\left( 1-\\pi_j \\right)^{1-y_j} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "is maximized. The term in the multiplication corresponds to the likelihood function of the Bernoulli distribution for the (degenerate) case of one single trial and number of successes $y_j = 1$ and $y_j = 0$ for success and failure, respectively.\n",
    "\n",
    "Operationally, the above maximization is usually achieved taking the negative of the log-likelihood function and computing the parameters $\\beta_i$ as the argmin of the negated log-likelihood which, after some manipulations, can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{L} = -log\\left[ L(\\beta_i|y_j) \\right] = - \\frac{1}{m}\\sum_{j=1}^{m} \\left[ y_j \\cdot log(\\pi_j) + (1-y_j) \\cdot log(1-\\pi_j) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "The right term of the equation, also called Cross-Entropy or log-loss, being a function $\\pi$, is ultimately a function of the parameters $\\beta_i$ and the features $X_i$ through the logistic regression relationship. The log-loss gives some insights as to the role of this function during estimation of the parameters. Let us assume that for a certain data point, $(x_{ij}, y_j)$, the target variable is equal to $1$. For this given data point, the right term of the equation simplifies to $-log(\\pi_j)$. Since this term needs to be minimized, the parameters $\\beta_i$ of the model need to be chosen such that $\\pi_j$ approaches $1$ as closely as possible. Conversely for an observation $y_j = 0$, minimization of the log-loss, $-log(1 - \\pi_j)$, requires $\\pi_j$ to approach $0$. This dual role of the log-loss function makes such that likelihood maximization in logistic regression aims to find the set of model parameters which best separate positive from negative observations in the space of the explanatory features $X_i$, in the sense of mapping as closely as possible positive targets to $\\pi = 1$ and negative targets to $\\pi = 0$. Another appealing property, which turns out to the be of paramount importance for the strategy outlined below, is that this log-loss function is convex, i.e., one and only one point of minimum exists in the space of parameters $\\beta_i$.\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Finding the optimum set of parameters requires a suitable optimization framework. Among various approaches, gradient descent of $\\hat{L}$ is a well-established approach for functions. For a certain point of the $n$-th dimensional space of parameters $\\beta_i$, the gradient of the log-loss function, $\\nabla \\hat{L}$ is computed, and thereafter a translation is performed in the parameter space along the gradient direction (the steepest descent).\n",
    "\n",
    "Gradient descent requires the computation of the gradient. In order to derive its formulation, it is convenient to consider the $i$-th component of $\\nabla \\hat{L}$, i.e.:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_i \\hat{L} = \\frac{\\partial}{\\partial \\beta_i} \\hat{L}\n",
    "\\end{equation}\n",
    "\n",
    "Taking the derivative inside the summation and operating on the logarithm yields:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_i \\hat{L} = - \\frac{1}{m} \\sum_{j=1}^{m} \\left( \\frac{y_j}{\\pi_j} - \\frac{1-y_j}{1-\\pi_j} \\right) \\frac{\\partial \\pi_j}{\\partial \\beta_i}\n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the probability with respect to the parameter equates to (refer to the initial logistic regression formulation):\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\pi_j}{\\partial \\beta_i} = \\frac{\\exp^{-z_j}}{(1+\\exp^{-z_j})^2} \\frac{\\partial z_j} {\\partial \\beta_i} = \\frac{\\exp^{-z_j}}{1+\\exp^{-z_j}} \\frac{1}{1+\\exp^{-z_j}} \\frac{\\partial z_j} {\\partial \\beta_i} = (1-\\pi_j) \\pi_j \\frac{\\partial z_j} {\\partial \\beta_i}\n",
    "\\end{equation}\n",
    "\n",
    "The derivative of the linear combination term yields:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial z_j} {\\partial \\beta_i} = x_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "Putting it all together, one finally obtains: \n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_i \\hat{L} = - \\frac{1}{m} \\sum_{j=1}^{m} \\left[ y_j (1-\\pi_j) - (1-y_j) \\pi_j \\right] x_{ij} = \\frac{1}{m}\\sum_{j=1}^{m} (\\pi_j-y_j) x_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "for $i = 1,2,\\dots,n$. \n",
    "\n",
    "Assuming a certain starting point in the space of parameters, $\\beta_i^0$, gradient descent first computes the gradient $\\nabla \\hat{L}$ at this starting point, and shifts the point along the direction of this gradient by computing a new point $\\beta_i^1 = \\beta_i^0 - \\alpha \\cdot \\nabla \\hat{L}$, where $\\alpha$ is a learning rate. This is done iteratively until suitable stopping criteria are met.\n",
    "\n",
    "## Algorithm for scalable implementation of logistic regression\n",
    "\n",
    "- Assume starting values for logistic parameters $\\beta_i^0$\n",
    "- Set learning parameter $\\alpha$\n",
    "- For each iteration $k$:\n",
    "- Broadcast parameters $\\beta_i^{k}$ to all worker nodes\n",
    "- Map: emit key-value pairs. Key: index $j$, values: target variable $y_j$ and array of explanatory features $x_{ij}$, for $j = 1,2,\\dots,n$\n",
    "- Map: for every $j = 1,2,\\dots,n$ compute probability $\\pi_j$ and $\\left[ y_j (1-\\pi_j) - (1-y_j) \\pi_j \\right] x_{ij}$\n",
    "- Reduce: sum over $j$ and divide by $m$, for $i = 1,2,\\dots,n$\n",
    "- Update $\\beta_i^{k}$\n",
    "- Run next iteration\n",
    "\n",
    "__References:__\n",
    "\n",
    "Bilder, C.R. and Loughin, T.M. (2015). Analysis of Categorical Data with R. CRC Press. \n",
    "\n",
    "Kremonic, Z. (2017). Maximum likelihood and gradient descent demonstration. Blog post. Accessed on July 2019 at https://zlatankr.github.io/posts/2017/03/06/mle-gradient-descent.\n",
    "\n",
    "\n",
    "\n",
    "Additional resources:\n",
    "\n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "\n",
    "https://ttic.uchicago.edu/~suriya/website-intromlss2018/course_material/Day3b.pdf \n",
    "\n",
    "http://www.holehouse.org/mlclass/06_Logistic_Regression.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For EDA purposes and notebook display, we randomly sampled 5% of the Criteo labs data via code **[Sid: put reference here]**. \n",
    "* This sampled code has a total length of 2292037 records. \n",
    "* *****Delete or add as needed: address the potential of bias; we could address this via bootstrapping and making sure that our clickthrough rate was robust** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"final_project\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sample training data and convert to dataframe\n",
    "#### EDA version below \n",
    "train_sample_EDA = sqlContext.read.format(\"csv\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"header\", \"false\") \\\n",
    "               .option(\"delimiter\", \"\\t\")\\\n",
    "               .load(\"data/sample_training.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----+----+-----+----+---+---+---+----+----+----+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+----+----+--------+--------+--------+--------+----+----+\n",
      "|_c0| _c1|_c2| _c3| _c4|  _c5| _c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|    _c14|    _c15|    _c16|    _c17|    _c18|    _c19|    _c20|    _c21|    _c22|    _c23|    _c24|    _c25|    _c26|    _c27|    _c28|    _c29|    _c30|    _c31|_c32|_c33|    _c34|    _c35|    _c36|    _c37|_c38|_c39|\n",
      "+---+----+---+----+----+-----+----+---+---+---+----+----+----+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+----+----+--------+--------+--------+--------+----+----+\n",
      "|  0|   0|  1|null|   0|16597| 557|  3|  5|123|   0|   1|null|   1|8cf07265|7cd19acc|77f2f2e5|d16679b9|4cf72387|fbad5c96|8fb24933|0b153874|a73ee510|0095a535|3617b5f5|9f32b866|428332cf|b28479f6|83ebd498|31ca40b6|e5ba7672|d0e5eb07|null|null|dfcfc3fa|ad3062eb|32c7478e|aee52b6f|null|null|\n",
      "|  0|   1|  0|   1|null| 1427|   3| 16| 11| 50|   0|   2|   1|null|05db9164|26a88120|615e3e4e|2788fed8|4cf72387|7e0ccccf|3f4ec687|0b153874|a73ee510|0e9ead52|c4adf918|f5d19c1c|85dbe138|07d13a8f|24ff9452|1034ac0d|3486227d|b486119d|null|null|63580fba|    null|32c7478e|2a90c749|null|null|\n",
      "|  0|null|  1|null|null|23255|null|  0|  1| 73|null|   0|null|null|7e5c2ff4|d833535f|b00d1501|d16679b9|25c83c98|7e0ccccf|65c53f25|1f89b562|a73ee510|3b08e48b|ad2bc6f4|e0d76380|39ccb769|b28479f6|a733d362|1203a270|776ce399|281769c2|null|null|73d06dde|    null|32c7478e|aee52b6f|null|null|\n",
      "+---+----+---+----+----+-----+----+---+---+---+----+----+----+----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+----+----+--------+--------+--------+--------+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# head first three rows\n",
    "train_sample_EDA.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice immediately from the first three rows that column names are not provided. From the Kaggle contest description, however, we do know that the first column (denoted as `c0`) contains the y variable we are interested in; specifically, a 0 denotes an ad that was not clicked on and a 1 denotes an ad that was clicked on. The following 13 fields (denoted from `_c1` to `_c13`) are numerical features, and the remaining 26 columns (denoted from `_14` to `_39` are categorical features. **Further, we see the presence of missing values, encoded as `null` in the dataframe**.\n",
    "\n",
    "For easier reference in columns, we will rename the following columns as such with the code provided below: \n",
    "* `_c0` as `CTR` to denote our y variable of interest (click-through rate). \n",
    "* Numerical columns `_1` to `_13` to be denoted as `Var1` to `Var13`. \n",
    "* Categorical columns `_14` to `_39` to be denoted as `Var14` to `Var39`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming of columns for easy reference \n",
    "train_sample_EDA = train_sample_EDA.withColumnRenamed(\"_c0\", \"CTR\") \\\n",
    "       .withColumnRenamed(\"_c1\", \"Var1\") \\\n",
    "       .withColumnRenamed(\"_c2\", \"Var2\") \\\n",
    "       .withColumnRenamed(\"_c3\", \"Var3\") \\\n",
    "       .withColumnRenamed(\"_c4\", \"Var4\") \\\n",
    "       .withColumnRenamed(\"_c5\", \"Var5\") \\\n",
    "       .withColumnRenamed(\"_c6\", \"Var6\") \\\n",
    "       .withColumnRenamed(\"_c7\", \"Var7\") \\\n",
    "       .withColumnRenamed(\"_c8\", \"Var8\") \\\n",
    "       .withColumnRenamed(\"_c9\", \"Var9\") \\\n",
    "       .withColumnRenamed(\"_c10\", \"Var10\") \\\n",
    "       .withColumnRenamed(\"_c11\", \"Var11\") \\\n",
    "       .withColumnRenamed(\"_c12\", \"Var12\") \\\n",
    "       .withColumnRenamed(\"_c13\", \"Var13\") \\\n",
    "        .withColumnRenamed(\"_c14\", \"Var14\") \\\n",
    "        .withColumnRenamed(\"_c15\", \"Var15\") \\\n",
    "        .withColumnRenamed(\"_c16\", \"Var16\") \\\n",
    "        .withColumnRenamed(\"_c17\", \"Var17\") \\\n",
    "        .withColumnRenamed(\"_c18\", \"Var18\") \\\n",
    "        .withColumnRenamed(\"_c19\", \"Var19\") \\\n",
    "        .withColumnRenamed(\"_c20\", \"Var20\") \\\n",
    "        .withColumnRenamed(\"_c21\", \"Var21\") \\\n",
    "        .withColumnRenamed(\"_c22\", \"Var22\") \\\n",
    "        .withColumnRenamed(\"_c23\", \"Var23\") \\\n",
    "        .withColumnRenamed(\"_c24\", \"Var24\") \\\n",
    "        .withColumnRenamed(\"_c25\", \"Var25\") \\\n",
    "        .withColumnRenamed(\"_c26\", \"Var26\") \\\n",
    "        .withColumnRenamed(\"_c27\", \"Var27\") \\\n",
    "        .withColumnRenamed(\"_c28\", \"Var28\") \\\n",
    "        .withColumnRenamed(\"_c29\", \"Var29\") \\\n",
    "        .withColumnRenamed(\"_c30\", \"Var30\") \\\n",
    "        .withColumnRenamed(\"_c31\", \"Var31\") \\\n",
    "        .withColumnRenamed(\"_c32\", \"Var32\") \\\n",
    "        .withColumnRenamed(\"_c33\", \"Var33\") \\\n",
    "        .withColumnRenamed(\"_c34\", \"Var34\") \\\n",
    "        .withColumnRenamed(\"_c35\", \"Var35\") \\\n",
    "        .withColumnRenamed(\"_c36\", \"Var36\") \\\n",
    "        .withColumnRenamed(\"_c37\", \"Var37\") \\\n",
    "        .withColumnRenamed(\"_c38\", \"Var38\") \\\n",
    "        .withColumnRenamed(\"_c39\", \"Var39\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+----+-----+----+----+----+----+-----+-----+-----+-----+\n",
      "|CTR|Var1|Var2|Var3|Var4| Var5|Var6|Var7|Var8|Var9|Var10|Var11|Var12|Var13|\n",
      "+---+----+----+----+----+-----+----+----+----+----+-----+-----+-----+-----+\n",
      "|  0|   0|   1|null|   0|16597| 557|   3|   5| 123|    0|    1| null|    1|\n",
      "|  0|   1|   0|   1|null| 1427|   3|  16|  11|  50|    0|    2|    1| null|\n",
      "|  0|null|   1|null|null|23255|null|   0|   1|  73| null|    0| null| null|\n",
      "|  0|   0|  37|  23|   9| 1635|  84|   2|  17| 109|    0|    2| null|   50|\n",
      "|  0|   2|   0|   9|   5|   44|   5|   2|   4|   5|    2|    2| null|    5|\n",
      "|  0|   0|   1|  14|   2|  120| 733|   0|  12| 606|    0|    0|   98|    2|\n",
      "|  0|null|   1|null|null| null|null|null|   0|null| null| null| null| null|\n",
      "|  0|   0|3295|null|   0| 4546| 149|  11|  30| 220|    0|    3| null|    2|\n",
      "|  0|   1|   1|   4|  17|  108|  22|   1|  24|  22|    1|    1| null|   22|\n",
      "|  0|null|  19|  32|   0| 1994|null|   0|  19|  26| null|    0| null|    3|\n",
      "|  0|null|  14|  28|  14|26324|  39|   3|  21|  32| null|    1| null|   14|\n",
      "|  0|null|  25|   1|null|17330|  62|   1|   3|  76| null|    1| null| null|\n",
      "|  0|null|   0|  31|   2|41016|null|   0|   9|  17| null|    0| null|    8|\n",
      "|  0|null| 164|   7|   2|10118| 259|  20|  11| 256| null|    5|    0|   30|\n",
      "|  0|   0|  35|null|   9|40556|null|   0|  41|  10|    0|    0| null|    9|\n",
      "|  0|null|   0|  21|   4|10121|null|   0|   7|  77| null|    0| null|    5|\n",
      "|  0|   0|  65|  29|   1| 7777| 566|   1|   1|  22|    0|    1| null|    1|\n",
      "|  0|   0|   4|null|null| 9092| 536|   1|  42| 146|    0|    1| null| null|\n",
      "|  1|   0|   0|   3|   1| 1516|   8|   3|   2|   4|    0|    3| null|    1|\n",
      "|  0|   3|  44|   1|   2|    1|   2|   3|   1|   2|    1|    1| null|    2|\n",
      "+---+----+----+----+----+-----+----+----+----+----+-----+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing top 20 rows of the numerical columns \n",
    "train_sample_EDA.select(\"CTR\", \"Var1\", \"Var2\", \"Var3\", \"Var4\", \"Var5\", \\\n",
    "                        \"Var6\", \"Var7\", \"Var8\", \"Var9\", \"Var10\", \"Var11\", \"Var12\", \"Var13\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|   Var14|   Var15|   Var16|   Var17|   Var18|   Var19|   Var20|   Var21|   Var22|   Var23|   Var24|   Var25|   Var26|   Var27|   Var28|   Var29|   Var30|   Var31|   Var32|   Var33|   Var34|   Var35|   Var36|   Var37|   Var38|   Var39|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|8cf07265|7cd19acc|77f2f2e5|d16679b9|4cf72387|fbad5c96|8fb24933|0b153874|a73ee510|0095a535|3617b5f5|9f32b866|428332cf|b28479f6|83ebd498|31ca40b6|e5ba7672|d0e5eb07|    null|    null|dfcfc3fa|ad3062eb|32c7478e|aee52b6f|    null|    null|\n",
      "|05db9164|26a88120|615e3e4e|2788fed8|4cf72387|7e0ccccf|3f4ec687|0b153874|a73ee510|0e9ead52|c4adf918|f5d19c1c|85dbe138|07d13a8f|24ff9452|1034ac0d|3486227d|b486119d|    null|    null|63580fba|    null|32c7478e|2a90c749|    null|    null|\n",
      "|7e5c2ff4|d833535f|b00d1501|d16679b9|25c83c98|7e0ccccf|65c53f25|1f89b562|a73ee510|3b08e48b|ad2bc6f4|e0d76380|39ccb769|b28479f6|a733d362|1203a270|776ce399|281769c2|    null|    null|73d06dde|    null|32c7478e|aee52b6f|    null|    null|\n",
      "|05db9164|9b25e48b|2d9b2559|96302ef8|43b19349|fbad5c96|e64ca89e|5b392875|a73ee510|3b76bfa9|87bb382c|3d899a5a|d95a2a6d|8ceecbc8|8f3ef960|24352c5c|07c540c4|7d8c03aa|fbf39fb5|a458ea53|0c61029b|    null|32c7478e|216a829e|001f3601|abc00283|\n",
      "|5a9ed9b0|3e4b7926|e1266b28|09e3cd5a|384874ce|7e0ccccf|675e81f6|5b392875|a73ee510|d1ebaddf|4a77ddca|eb8ded57|dc1d72e4|07d13a8f|e6863a8e|3d9023a4|07c540c4|e261f8d8|21ddcdc9|b1252a9d|31b4af04|    null|32c7478e|8d653a3e|445bbe3b|32280082|\n",
      "|5a9ed9b0|097e9399|915bbd8f|26ac5cc6|f3474129|fe6b92e5|875679ca|37e4aa92|a73ee510|3b08e48b|bbd0e773|87408e45|79ae8b9a|1adce6ef|5c3dbd29|8759a2d9|8efede7f|97029569|21ddcdc9|b1252a9d|6ec2bcf7|    null|55dd3565|7f686ab3|2bf691b1|bebc2875|\n",
      "|68fd1e64|791f3f76|d032c263|c18be181|4cf72387|7e0ccccf|970f01b2|0b153874|7cc72ec2|3b08e48b|36bccca0|dfbb09fb|80467802|07d13a8f|876a7e78|84898b2a|2005abd1|0f4a15b0|    null|    null|0014c32a|    null|55dd3565|3b183c5c|    null|    null|\n",
      "|05db9164|4c2bc594|d032c263|c18be181|25c83c98|7e0ccccf|7956c2ff|0b153874|a73ee510|44ac3250|4b0929e2|dfbb09fb|c0ed8bfc|8ceecbc8|7ac43a46|84898b2a|e5ba7672|bc48b783|    null|    null|0014c32a|c9d4222a|55dd3565|3b183c5c|    null|    null|\n",
      "|05db9164|d833535f|77f2f2e5|d16679b9|4cf72387|7e0ccccf|def4a4d4|5b392875|a73ee510|0c70a731|4ba74619|9f32b866|879fa878|b28479f6|a66dcf27|31ca40b6|d4bb7bd8|7b49e3d2|    null|    null|dfcfc3fa|    null|423fab69|aee52b6f|    null|    null|\n",
      "|be589b51|38d50e09|4724f2c8|8510f416|4cf72387|7e0ccccf|38eb9cf4|0b153874|a73ee510|2462946f|7f8ffe57|f6fe1d50|46f42a63|b28479f6|42b3012c|ad774107|1e88c74f|582152eb|21ddcdc9|5840adea|fbaf98df|    null|32c7478e|e773f0cb|001f3601|1b0ebd59|\n",
      "|ae82ea21|4c2bc594|d032c263|c18be181|25c83c98|fe6b92e5|869243b9|5b392875|a73ee510|3b08e48b|6a77517a|dfbb09fb|e0230d57|1adce6ef|ae0c3875|84898b2a|27c07bd6|15a36060|    null|    null|0014c32a|    null|3a171ecb|3b183c5c|    null|    null|\n",
      "|05db9164|bce95927|f69c6d34|13508380|384874ce|fe6b92e5|4c5c7066|0b153874|a73ee510|d077d4d4|8924112e|0fd301b6|3cf672d1|07d13a8f|fec218c0|5c495dbe|07c540c4|04d863d5|21ddcdc9|b1252a9d|a3a82059|78e2e389|423fab69|45ab94c8|e8b83407|c84c4aec|\n",
      "|05db9164|38a947a1|    null|    null|25c83c98|3bf701e7|49042125|062b5529|7cc72ec2|4624c4e8|ba1ff80a|    null|b95f83fa|b28479f6|0cfbc5df|    null|1e88c74f|be457d6e|    null|    null|    null|    null|bcdee96c|    null|    null|    null|\n",
      "|39af2607|26a88120|b00d1501|d16679b9|25c83c98|fbad5c96|49b74ebc|1f89b562|a73ee510|7f79890b|c4adf918|e0d76380|85dbe138|b28479f6|2ebbf26a|1203a270|e5ba7672|b486119d|    null|    null|73d06dde|    null|32c7478e|aee52b6f|    null|    null|\n",
      "|68fd1e64|d833535f|b00d1501|d16679b9|25c83c98|fe6b92e5|01a0f3f6|0b153874|a73ee510|3b08e48b|f045731b|e0d76380|252ee845|b28479f6|a733d362|1203a270|e5ba7672|281769c2|    null|    null|73d06dde|    null|32c7478e|aee52b6f|    null|    null|\n",
      "|05db9164|71ca0a25|396df967|328b42c3|25c83c98|7e0ccccf|5c116cc3|0b153874|a73ee510|f62834dd|d02264a7|8481d649|14e6fd48|b28479f6|a67c19b7|b2f2a0c7|1e88c74f|9bf8ffef|21ddcdc9|5840adea|f0bb1194|    null|3a171ecb|a5ce2d0d|001f3601|984e0db0|\n",
      "|05db9164|09e68b86|aa8c1539|85dd697c|25c83c98|13718bbd|24db17b4|0b153874|a73ee510|eaaee8c6|d6ea7935|d8c29807|74838342|8ceecbc8|d2f03b75|c64d548f|d4bb7bd8|63cdbb21|cf99e5de|5840adea|5f957280|    null|3a171ecb|1793a828|e8b83407|b7d9c3bc|\n",
      "|39af2607|e18b1e61|    null|    null|43b19349|7e0ccccf|9e5b069d|0b153874|a73ee510|550727c0|6153cf57|    null|769a1844|b28479f6|972b922a|    null|d4bb7bd8|832bedbd|    null|    null|    null|    null|be7c41b4|    null|    null|    null|\n",
      "|5a9ed9b0|6887a43c|5d1ca1e5|196060ff|25c83c98|7e0ccccf|2b322b66|0b153874|a73ee510|99fe7a1f|24adbadc|71c32035|fc371461|051219e6|61f52294|cf393f9f|e5ba7672|1c130afa|21ddcdc9|b1252a9d|06c4ef72|ad3062eb|32c7478e|6bc2bf95|445bbe3b|dc39e0d1|\n",
      "|a86f8721|38a947a1|2ca5a244|eca41678|4cf72387|fe6b92e5|7fafef37|5b392875|a73ee510|5ba575e7|78f92234|5db0c556|9be66b48|1adce6ef|7fb3b951|f5f5bdfa|07c540c4|7e345426|    null|    null|82e27b23|    null|3a171ecb|f4cd3bc3|    null|    null|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing top 20 rows of the categorical columns \n",
    "train_sample_EDA.select('Var14','Var15','Var16','Var17','Var18','Var19','Var20','Var21',\n",
    "                    'Var22','Var23','Var24','Var25','Var26','Var27','Var28','Var29','Var30',\n",
    "                   'Var31','Var32','Var33','Var34','Var35','Var36','Var37','Var38','Var39').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Brief Statistics and Coverage of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our code below, we depict brief statistics about our sampled data: \n",
    "* Our sampled data shows a click through rate of approximately 25% \n",
    "* Coverage: For the most part, most of our columns show pretty good coverage (>95%). We do see, however, a troubling number of Null (missing) values for a couple columns. \n",
    "    * Perhaps the most alarming: columns Var32, Var33, Var38, Var39, Var10, Var1, Var35, and Var12 have approximately 50% or less of their values as non-Null. \n",
    "    \n",
    "A significant challenge for us will involve having to deal with high number of Null values in some of our columns. For numerical values, it is certainly possible to impute a number to fill in for Null values, but this challenge gets tricky with categorical variables. The handling of null values for categorical variables will be discussed in Section 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2292037"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting a count of the total number of rows in our sampled data. \n",
    "total_count = train_sample_EDA.count()\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2564675003064959"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating approximate click through rate of our sampled training data \n",
    "train_sample_EDA.filter(train_sample_EDA.CTR==1).count()/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage_nonNull</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CTR</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var14</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var36</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var31</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var30</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var28</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var27</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var26</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var24</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var23</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var22</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var21</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var18</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var15</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var20</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var2</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var8</th>\n",
       "      <td>0.999496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var5</th>\n",
       "      <td>0.974168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var17</th>\n",
       "      <td>0.965857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var37</th>\n",
       "      <td>0.965857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var34</th>\n",
       "      <td>0.965857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var25</th>\n",
       "      <td>0.965857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var16</th>\n",
       "      <td>0.965857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var29</th>\n",
       "      <td>0.965857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var11</th>\n",
       "      <td>0.956850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var9</th>\n",
       "      <td>0.956850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var7</th>\n",
       "      <td>0.956850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var19</th>\n",
       "      <td>0.878906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var3</th>\n",
       "      <td>0.785205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var13</th>\n",
       "      <td>0.782994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var4</th>\n",
       "      <td>0.782994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var6</th>\n",
       "      <td>0.776426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var32</th>\n",
       "      <td>0.559533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var33</th>\n",
       "      <td>0.559533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var38</th>\n",
       "      <td>0.559533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var39</th>\n",
       "      <td>0.559533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var10</th>\n",
       "      <td>0.546833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var1</th>\n",
       "      <td>0.546833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var35</th>\n",
       "      <td>0.237235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var12</th>\n",
       "      <td>0.234542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Coverage_nonNull\n",
       "CTR            1.000000\n",
       "Var14          1.000000\n",
       "Var36          1.000000\n",
       "Var31          1.000000\n",
       "Var30          1.000000\n",
       "Var28          1.000000\n",
       "Var27          1.000000\n",
       "Var26          1.000000\n",
       "Var24          1.000000\n",
       "Var23          1.000000\n",
       "Var22          1.000000\n",
       "Var21          1.000000\n",
       "Var18          1.000000\n",
       "Var15          1.000000\n",
       "Var20          1.000000\n",
       "Var2           1.000000\n",
       "Var8           0.999496\n",
       "Var5           0.974168\n",
       "Var17          0.965857\n",
       "Var37          0.965857\n",
       "Var34          0.965857\n",
       "Var25          0.965857\n",
       "Var16          0.965857\n",
       "Var29          0.965857\n",
       "Var11          0.956850\n",
       "Var9           0.956850\n",
       "Var7           0.956850\n",
       "Var19          0.878906\n",
       "Var3           0.785205\n",
       "Var13          0.782994\n",
       "Var4           0.782994\n",
       "Var6           0.776426\n",
       "Var32          0.559533\n",
       "Var33          0.559533\n",
       "Var38          0.559533\n",
       "Var39          0.559533\n",
       "Var10          0.546833\n",
       "Var1           0.546833\n",
       "Var35          0.237235\n",
       "Var12          0.234542"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating coverage: \n",
    "# The column \"Coverage_nonNull\" (expressed as percentage) depicts how many non-null values are in the column. \n",
    "# This gives a sense of how well a particular variable covers our entire dataset. \n",
    "\n",
    "from pyspark.sql.functions import col, count, isnan, stddev, lit, sum\n",
    "\n",
    "coverage = train_sample_EDA.agg(*[\n",
    "    (count(c)/total_count).alias(c)    # vertical (column-wise) operations in SQL ignore NULLs\n",
    "    for c in train_sample_EDA.columns\n",
    "]).toPandas()\n",
    "\n",
    "coverage_summary = coverage.T\n",
    "coverage_summary.columns = ['Coverage_nonNull']\n",
    "coverage_summary.sort_values(by='Coverage_nonNull', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Numerical Variables\n",
    "\n",
    "### 3.2.1 Basic Statistics\n",
    "* In Table 3.0, we calculate basic statistics to summarize our numerical variables: count of observations that have the variable, the mean of the variable, the standard deviation of the variable, and the min and max of the variable. \n",
    "    * We note that the range for each variable is different; although we do not know the exact definition of what each variable stands for, we can tell that the range of each variable can vary greatly, such as 0 to 8 for Var10 but 0 to 2,634,953 for Var5. \n",
    "    * Variable 2 has a minimum value of -2, which is odd given that the Kaggle criteo data description (https://www.kaggle.com/c/criteo-display-ad-challenge/data) specifically states that the numerical variables are mostly counts but there is no specification as to whether all numbers should be non-negative. This will be addressed in section 3.4 on deciding how negative values are to be treated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var1</th>\n",
       "      <td>1253362</td>\n",
       "      <td>3.52458746954192</td>\n",
       "      <td>9.456074158477476</td>\n",
       "      <td>0</td>\n",
       "      <td>1575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var2</th>\n",
       "      <td>2292037</td>\n",
       "      <td>105.5236778463873</td>\n",
       "      <td>387.9460727424658</td>\n",
       "      <td>-2</td>\n",
       "      <td>19219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var3</th>\n",
       "      <td>1799718</td>\n",
       "      <td>27.030229180349366</td>\n",
       "      <td>402.6173876100675</td>\n",
       "      <td>0</td>\n",
       "      <td>65535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var4</th>\n",
       "      <td>1794652</td>\n",
       "      <td>7.327119129502544</td>\n",
       "      <td>8.845164661144699</td>\n",
       "      <td>0</td>\n",
       "      <td>969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var5</th>\n",
       "      <td>2232828</td>\n",
       "      <td>18536.03390498507</td>\n",
       "      <td>69153.1755142129</td>\n",
       "      <td>0</td>\n",
       "      <td>2634953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var6</th>\n",
       "      <td>1779597</td>\n",
       "      <td>115.58777689555556</td>\n",
       "      <td>337.0288166711634</td>\n",
       "      <td>0</td>\n",
       "      <td>66619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var7</th>\n",
       "      <td>2193136</td>\n",
       "      <td>16.459486324605496</td>\n",
       "      <td>70.80815785839796</td>\n",
       "      <td>0</td>\n",
       "      <td>34536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var8</th>\n",
       "      <td>2290882</td>\n",
       "      <td>12.536948651218177</td>\n",
       "      <td>16.92910564553372</td>\n",
       "      <td>0</td>\n",
       "      <td>4513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var9</th>\n",
       "      <td>2193136</td>\n",
       "      <td>105.99417500784266</td>\n",
       "      <td>219.74396942043634</td>\n",
       "      <td>0</td>\n",
       "      <td>18345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var10</th>\n",
       "      <td>1253362</td>\n",
       "      <td>0.6180863948324586</td>\n",
       "      <td>0.6845344117066056</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var11</th>\n",
       "      <td>2193136</td>\n",
       "      <td>2.7380673154788395</td>\n",
       "      <td>5.22875057526333</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var12</th>\n",
       "      <td>537580</td>\n",
       "      <td>0.9924978607835113</td>\n",
       "      <td>6.08334075723253</td>\n",
       "      <td>0</td>\n",
       "      <td>1831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var13</th>\n",
       "      <td>1794652</td>\n",
       "      <td>8.217247689245603</td>\n",
       "      <td>15.876313370693108</td>\n",
       "      <td>0</td>\n",
       "      <td>4317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0                   1                   2    3        4\n",
       "summary    count                mean              stddev  min      max\n",
       "Var1     1253362    3.52458746954192   9.456074158477476    0     1575\n",
       "Var2     2292037   105.5236778463873   387.9460727424658   -2    19219\n",
       "Var3     1799718  27.030229180349366   402.6173876100675    0    65535\n",
       "Var4     1794652   7.327119129502544   8.845164661144699    0      969\n",
       "Var5     2232828   18536.03390498507    69153.1755142129    0  2634953\n",
       "Var6     1779597  115.58777689555556   337.0288166711634    0    66619\n",
       "Var7     2193136  16.459486324605496   70.80815785839796    0    34536\n",
       "Var8     2290882  12.536948651218177   16.92910564553372    0     4513\n",
       "Var9     2193136  105.99417500784266  219.74396942043634    0    18345\n",
       "Var10    1253362  0.6180863948324586  0.6845344117066056    0        8\n",
       "Var11    2193136  2.7380673154788395    5.22875057526333    0      163\n",
       "Var12     537580  0.9924978607835113    6.08334075723253    0     1831\n",
       "Var13    1794652   8.217247689245603  15.876313370693108    0     4317"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "# Table 3.0: Basic statistics \n",
    "########\n",
    "numerical_cols = ['Var1','Var2','Var3','Var4','Var5','Var6','Var7',\\\n",
    "                  'Var8','Var9','Var10','Var11','Var12','Var13']\n",
    "numerical_stats = train_sample_EDA.describe(numerical_cols).toPandas()\n",
    "numerical_stats.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below in Table 3.1, we also calculated the mean, standard deviation, count, min, and max of each numerical variable grouped by the click through rate (CTR) target variable. \n",
    "    * From here, we made a rough guess-timate of the 95% confidence interval of the mean of the numerical variables to see if we could distinguish enough difference between observations that had a successful or unsuccessful click through on an ad. We realize that the distribution of the numerical variables were not exactly normal, but given the high number of observations, a 95% confidence interval by the form of $(\\mu - 1.96*\\frac{\\sigma}{n_{group}}, \\mu +1.96*\\frac{\\sigma}{n_{group}})$ should give a rough estimate of the group means and their standard deviations. \n",
    "    * We can see that most of numerical variables show quite some difference between their group means. The columns `lowerCI_CTR0`, `upperCI_CTR0` depict the lower and upper confidence interval bounds for the observations under CTR = 0, while the columns `lowerCI_CTR1`, `upperCI_CTR1` depict the lower and upper confidence interval bounds for the observations under CTR = 1. Most of the confidence intervals between the 2 groups do not overlap; there appears to be enough information in our numerical variables such that they should all be included into our model to distinguish the difference between CTR=0 or CTR=1 prediction. That being said, it should be noted that Var4, Var8, and Var10 have the closest confidence interval boundaries compared to the other columns, and it is possible that these 3 columns might have smaller explaining power compared to the other numerical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>avg_CTR0</th>\n",
       "      <th>avg_CTR1</th>\n",
       "      <th>std_CTR0</th>\n",
       "      <th>std_CTR1</th>\n",
       "      <th>min_CTR0</th>\n",
       "      <th>min_CTR1</th>\n",
       "      <th>max_CTR0</th>\n",
       "      <th>max_CTR1</th>\n",
       "      <th>count_CTR0</th>\n",
       "      <th>count_CTR1</th>\n",
       "      <th>lowerCI_CTR0</th>\n",
       "      <th>upperCI_CTR0</th>\n",
       "      <th>lowerCI_CTR1</th>\n",
       "      <th>upperCI_CTR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Var1</td>\n",
       "      <td>2.946473</td>\n",
       "      <td>4.790206</td>\n",
       "      <td>8.674151</td>\n",
       "      <td>10.868277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1575</td>\n",
       "      <td>908</td>\n",
       "      <td>860362</td>\n",
       "      <td>393000</td>\n",
       "      <td>2.928143</td>\n",
       "      <td>2.964802</td>\n",
       "      <td>4.756226</td>\n",
       "      <td>4.824186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Var2</td>\n",
       "      <td>95.612975</td>\n",
       "      <td>134.256090</td>\n",
       "      <td>361.190633</td>\n",
       "      <td>455.522574</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>19219</td>\n",
       "      <td>16956</td>\n",
       "      <td>1704204</td>\n",
       "      <td>587833</td>\n",
       "      <td>95.070685</td>\n",
       "      <td>96.155265</td>\n",
       "      <td>133.091591</td>\n",
       "      <td>135.420588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Var3</td>\n",
       "      <td>23.801411</td>\n",
       "      <td>37.103464</td>\n",
       "      <td>248.094061</td>\n",
       "      <td>689.683142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65535</td>\n",
       "      <td>65535</td>\n",
       "      <td>1362871</td>\n",
       "      <td>436847</td>\n",
       "      <td>23.384882</td>\n",
       "      <td>24.217940</td>\n",
       "      <td>35.058240</td>\n",
       "      <td>39.148689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Var4</td>\n",
       "      <td>7.601784</td>\n",
       "      <td>6.499002</td>\n",
       "      <td>9.112409</td>\n",
       "      <td>7.928093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>969</td>\n",
       "      <td>295</td>\n",
       "      <td>1347666</td>\n",
       "      <td>446986</td>\n",
       "      <td>7.586399</td>\n",
       "      <td>7.617169</td>\n",
       "      <td>6.475760</td>\n",
       "      <td>6.522244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Var5</td>\n",
       "      <td>21823.328636</td>\n",
       "      <td>9209.995004</td>\n",
       "      <td>76456.296149</td>\n",
       "      <td>40591.419351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2634953</td>\n",
       "      <td>1906516</td>\n",
       "      <td>1650907</td>\n",
       "      <td>581921</td>\n",
       "      <td>21706.699242</td>\n",
       "      <td>21939.958031</td>\n",
       "      <td>9105.701252</td>\n",
       "      <td>9314.288757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Var6</td>\n",
       "      <td>136.768530</td>\n",
       "      <td>64.598590</td>\n",
       "      <td>377.688449</td>\n",
       "      <td>199.877834</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66619</td>\n",
       "      <td>17917</td>\n",
       "      <td>1257313</td>\n",
       "      <td>522284</td>\n",
       "      <td>136.108342</td>\n",
       "      <td>137.428719</td>\n",
       "      <td>64.056505</td>\n",
       "      <td>65.140675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Var7</td>\n",
       "      <td>13.125326</td>\n",
       "      <td>25.870308</td>\n",
       "      <td>63.142022</td>\n",
       "      <td>88.273909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34536</td>\n",
       "      <td>11839</td>\n",
       "      <td>1619399</td>\n",
       "      <td>573737</td>\n",
       "      <td>13.028074</td>\n",
       "      <td>13.222577</td>\n",
       "      <td>25.641889</td>\n",
       "      <td>26.098727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Var8</td>\n",
       "      <td>12.799855</td>\n",
       "      <td>11.775066</td>\n",
       "      <td>18.047062</td>\n",
       "      <td>13.133923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4513</td>\n",
       "      <td>733</td>\n",
       "      <td>1703164</td>\n",
       "      <td>587718</td>\n",
       "      <td>12.772751</td>\n",
       "      <td>12.826959</td>\n",
       "      <td>11.741487</td>\n",
       "      <td>11.808644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Var9</td>\n",
       "      <td>103.372396</td>\n",
       "      <td>113.394268</td>\n",
       "      <td>214.584881</td>\n",
       "      <td>233.533582</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18345</td>\n",
       "      <td>11996</td>\n",
       "      <td>1619399</td>\n",
       "      <td>573737</td>\n",
       "      <td>103.041890</td>\n",
       "      <td>103.702901</td>\n",
       "      <td>112.789973</td>\n",
       "      <td>113.998563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Var10</td>\n",
       "      <td>0.537784</td>\n",
       "      <td>0.793885</td>\n",
       "      <td>0.642230</td>\n",
       "      <td>0.739217</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>860362</td>\n",
       "      <td>393000</td>\n",
       "      <td>0.536427</td>\n",
       "      <td>0.539141</td>\n",
       "      <td>0.791574</td>\n",
       "      <td>0.796197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Var11</td>\n",
       "      <td>2.254756</td>\n",
       "      <td>4.102235</td>\n",
       "      <td>4.503326</td>\n",
       "      <td>6.689281</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>148</td>\n",
       "      <td>1619399</td>\n",
       "      <td>573737</td>\n",
       "      <td>2.247820</td>\n",
       "      <td>2.261692</td>\n",
       "      <td>4.084926</td>\n",
       "      <td>4.119544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Var12</td>\n",
       "      <td>0.685031</td>\n",
       "      <td>1.752763</td>\n",
       "      <td>5.412193</td>\n",
       "      <td>7.434564</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1831</td>\n",
       "      <td>347</td>\n",
       "      <td>382777</td>\n",
       "      <td>154803</td>\n",
       "      <td>0.667885</td>\n",
       "      <td>0.702176</td>\n",
       "      <td>1.715727</td>\n",
       "      <td>1.789799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Var13</td>\n",
       "      <td>8.959959</td>\n",
       "      <td>5.977968</td>\n",
       "      <td>17.427552</td>\n",
       "      <td>9.466685</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4317</td>\n",
       "      <td>629</td>\n",
       "      <td>1347666</td>\n",
       "      <td>446986</td>\n",
       "      <td>8.930535</td>\n",
       "      <td>8.989383</td>\n",
       "      <td>5.950215</td>\n",
       "      <td>6.005721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column      avg_CTR0     avg_CTR1      std_CTR0      std_CTR1  min_CTR0  \\\n",
       "0    Var1      2.946473     4.790206      8.674151     10.868277         0   \n",
       "1    Var2     95.612975   134.256090    361.190633    455.522574        -2   \n",
       "2    Var3     23.801411    37.103464    248.094061    689.683142         0   \n",
       "3    Var4      7.601784     6.499002      9.112409      7.928093         0   \n",
       "4    Var5  21823.328636  9209.995004  76456.296149  40591.419351         0   \n",
       "5    Var6    136.768530    64.598590    377.688449    199.877834         0   \n",
       "6    Var7     13.125326    25.870308     63.142022     88.273909         0   \n",
       "7    Var8     12.799855    11.775066     18.047062     13.133923         0   \n",
       "8    Var9    103.372396   113.394268    214.584881    233.533582         0   \n",
       "9   Var10      0.537784     0.793885      0.642230      0.739217         0   \n",
       "10  Var11      2.254756     4.102235      4.503326      6.689281         0   \n",
       "11  Var12      0.685031     1.752763      5.412193      7.434564         0   \n",
       "12  Var13      8.959959     5.977968     17.427552      9.466685         0   \n",
       "\n",
       "    min_CTR1  max_CTR0  max_CTR1  count_CTR0  count_CTR1  lowerCI_CTR0  \\\n",
       "0          0      1575       908      860362      393000      2.928143   \n",
       "1         -2     19219     16956     1704204      587833     95.070685   \n",
       "2          0     65535     65535     1362871      436847     23.384882   \n",
       "3          0       969       295     1347666      446986      7.586399   \n",
       "4          0   2634953   1906516     1650907      581921  21706.699242   \n",
       "5          0     66619     17917     1257313      522284    136.108342   \n",
       "6          0     34536     11839     1619399      573737     13.028074   \n",
       "7          0      4513       733     1703164      587718     12.772751   \n",
       "8          0     18345     11996     1619399      573737    103.041890   \n",
       "9          0         8         7      860362      393000      0.536427   \n",
       "10         0       163       148     1619399      573737      2.247820   \n",
       "11         0      1831       347      382777      154803      0.667885   \n",
       "12         0      4317       629     1347666      446986      8.930535   \n",
       "\n",
       "    upperCI_CTR0  lowerCI_CTR1  upperCI_CTR1  \n",
       "0       2.964802      4.756226      4.824186  \n",
       "1      96.155265    133.091591    135.420588  \n",
       "2      24.217940     35.058240     39.148689  \n",
       "3       7.617169      6.475760      6.522244  \n",
       "4   21939.958031   9105.701252   9314.288757  \n",
       "5     137.428719     64.056505     65.140675  \n",
       "6      13.222577     25.641889     26.098727  \n",
       "7      12.826959     11.741487     11.808644  \n",
       "8     103.702901    112.789973    113.998563  \n",
       "9       0.539141      0.791574      0.796197  \n",
       "10      2.261692      4.084926      4.119544  \n",
       "11      0.702176      1.715727      1.789799  \n",
       "12      8.989383      5.950215      6.005721  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########\n",
    "# Table 3.1 Basic Statistics by Group (click through rate == 0 vs click through rate == 1) in each column. \n",
    "########## \n",
    "grouped_CTR0 = []\n",
    "grouped_CTR1 = []\n",
    "\n",
    "for p in range(0, len(numerical_cols)): \n",
    "    g1_avg, g0_avg=train_sample_EDA.groupBy('CTR').agg({numerical_cols[p]:\"avg\"}).collect()\n",
    "    g1_std, g0_std=train_sample_EDA.groupBy('CTR').agg({numerical_cols[p]:\"stddev\"}).collect()\n",
    "    g1_count, g0_count = train_sample_EDA.groupBy('CTR').agg({numerical_cols[p]:\"count\"}).collect()\n",
    "    g1_min, g0_min = train_sample_EDA.groupBy('CTR').agg({numerical_cols[p]:\"min\"}).collect()\n",
    "    g1_max, g0_max = train_sample_EDA.groupBy('CTR').agg({numerical_cols[p]:\"max\"}).collect()\n",
    "    grouped_CTR1.append({'Column': numerical_cols[p], 'avg_CTR1': g1_avg[1], 'std_CTR1': g1_std[1], \n",
    "                       'count_CTR1': g1_count[1], 'min_CTR1': g1_min[1], 'max_CTR1': g1_max[1]})\n",
    "    grouped_CTR0.append({'Column': numerical_cols[p], 'avg_CTR0': g0_avg[1], 'std_CTR0': g0_std[1],\n",
    "                       'count_CTR0': g0_count[1], 'min_CTR0': g0_min[1], 'max_CTR0': g0_max[1]})\n",
    "\n",
    "CTR0_df = pd.DataFrame(grouped_CTR0)\n",
    "CTR1_df = pd.DataFrame(grouped_CTR1)\n",
    "merged_group_agg = CTR0_df.merge(CTR1_df, how='left', on='Column')[['Column','avg_CTR0','avg_CTR1','std_CTR0','std_CTR1','min_CTR0','min_CTR1','max_CTR0','max_CTR1','count_CTR0','count_CTR1']]\n",
    "merged_group_agg['lowerCI_CTR0']=merged_group_agg['avg_CTR0']-((1.96)*(merged_group_agg['std_CTR0']/np.sqrt(merged_group_agg['count_CTR0'])))\n",
    "merged_group_agg['upperCI_CTR0']=merged_group_agg['avg_CTR0']+((1.96)*(merged_group_agg['std_CTR0']/np.sqrt(merged_group_agg['count_CTR0'])))\n",
    "merged_group_agg['lowerCI_CTR1']=merged_group_agg['avg_CTR1']-((1.96)*(merged_group_agg['std_CTR1']/np.sqrt(merged_group_agg['count_CTR1'])))\n",
    "merged_group_agg['upperCI_CTR1']=merged_group_agg['avg_CTR1']+((1.96)*(merged_group_agg['std_CTR1']/np.sqrt(merged_group_agg['count_CTR1'])))\n",
    "merged_group_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Histograms \n",
    "* We also plot the histograms of each of the numerical variables to gain a sense of distribution. \n",
    "* We note that pretty much all of the numerical variables are skewed right; most of the observations generally are centered around the lower range of the values and the remaining higher values are tailed off to the right. \n",
    "* As the range of all variables varies widely across all variables, we plan to standardize the numerical variables by **demeaning and dividing by the standard deviation** such that not one variable overpowers over the other numerical variables solely due to scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 14 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########\n",
    "# Histogram of numerical variables \n",
    "########\n",
    "# Note that the last chart is empty as we only have 13 numerical variables. \n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline \n",
    "numerical_cols = ['Var1','Var2','Var3','Var4','Var5','Var6','Var7',\\\n",
    "                  'Var8','Var9','Var10','Var11','Var12','Var13']\n",
    "\n",
    "fig, axs = plt.subplots(2,7, figsize=(20,10))\n",
    "axs = axs.ravel()\n",
    "for p in range(0,len(numerical_cols)): \n",
    "    list_values = train_sample_EDA.select(numerical_cols[p]).rdd.flatMap(lambda x: x).collect()\n",
    "    cleaned_list = [x for x in list_values if x is not None]\n",
    "    axs[p].hist(cleaned_list, bins=20, density=True)\n",
    "    axs[p].set_title(numerical_cols[p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Categorical Variables and Cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are a total of 26 categorical columns that are also included within this dataset. \n",
    "* In Table 3.2, we immediately notice the high dimensionality when we take note of the number of distinct values in each categorical column. Some (ex: Var33 and Var22) have fewer than 5 distinct values while others (ex: Var16 and Var25) have over 10,000 distinct values. We explore our methods of preprocessing and reducing dimensionality in Section 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>count_distinct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Var16</td>\n",
       "      <td>753315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Var25</td>\n",
       "      <td>661596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Var34</td>\n",
       "      <td>588319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Var29</td>\n",
       "      <td>489035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Var17</td>\n",
       "      <td>258094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Var37</td>\n",
       "      <td>69117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Var39</td>\n",
       "      <td>47492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Var23</td>\n",
       "      <td>42422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Var20</td>\n",
       "      <td>11741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Var28</td>\n",
       "      <td>10943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Var24</td>\n",
       "      <td>5126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Var31</td>\n",
       "      <td>4479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Var26</td>\n",
       "      <td>3169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Var32</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Var14</td>\n",
       "      <td>1407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Var21</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Var15</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Var18</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Var38</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Var27</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Var19</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Var35</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Var36</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Var30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Var33</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Var22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column  count_distinct\n",
       "2   Var16          753315\n",
       "11  Var25          661596\n",
       "20  Var34          588319\n",
       "15  Var29          489035\n",
       "3   Var17          258094\n",
       "23  Var37           69117\n",
       "25  Var39           47492\n",
       "9   Var23           42422\n",
       "6   Var20           11741\n",
       "14  Var28           10943\n",
       "10  Var24            5126\n",
       "17  Var31            4479\n",
       "12  Var26            3169\n",
       "18  Var32            1959\n",
       "0   Var14            1407\n",
       "7   Var21             614\n",
       "1   Var15             555\n",
       "4   Var18             300\n",
       "24  Var38              83\n",
       "13  Var27              26\n",
       "5   Var19              16\n",
       "21  Var35              15\n",
       "22  Var36              15\n",
       "16  Var30              10\n",
       "19  Var33               4\n",
       "8   Var22               3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols = ['Var14','Var15','Var16','Var17','Var18','Var19','Var20','Var21','Var22',\n",
    "                    'Var23','Var24','Var25','Var26','Var27','Var28','Var29','Var30','Var31',\n",
    "                   'Var32','Var33','Var34','Var35','Var36','Var37','Var38','Var39']\n",
    "\n",
    "###########\n",
    "# Table 3.2: Count distinct values in each categorical column \n",
    "###########\n",
    "distinct_temp = []\n",
    "for column in categorical_cols: \n",
    "    distinct_temp.append({'Column':column, 'count_distinct': train_sample_EDA.select(column).distinct().count()})\n",
    "    \n",
    "distinct_df = pd.DataFrame(distinct_temp)\n",
    "distinct_df.sort_values(by='count_distinct', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high dimensionality of our categorical columns if we were to one hot encode so many categorical features, we wanted to know if there were certain values within the categorical variables that were more common than others, and thus would be more predictive of the final click through rate label of 1 or 0. \n",
    "* It is likely that certain values of a categorical column may be rare and therefore may not contribute much predictive power. If we were to tally up the counts of each distinct value in a categorical column (split by CTR group, or CTR == 0 vs CTR == 1) and calculate a cumulative summation of the number of observations observed per distinct value, we could distinguish just how many of the individual values within a categorical variable can cover the majority of the dataset. By further breaking down the top most frequent values in each CTR == 0 and CTR == 1 group, we can also observe which values are more telling of whether an ad will be clicked on or not. \n",
    "    * It should be noted that we defined 'majority' here as 95% of the observations within each group (CTR==0, CTR==1). \n",
    "* Furthermore, we would like to observe a sense of \"correlation\" between the labelled variable (CTR == 0, CTR==1) and our categorical features to see if there is any sort of association between the two variables. Pearson's Correlation unfortunately will not suffice, as we would be comparing a categorical variable (with no particular ordering) against a binary output variable that simply labels a successful/unsuccessful ad click. \n",
    "    * One particular workaround is the Cramer's V statistic (https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V): Based on the chi-squared statistic, Cramer's V is a measure of association between two nominal variables and gives an output number between 0 and 1 to quantify the strength of the association between two nominal variables. We therefore calculate Cramer's V statistic between successful/unsuccessful ad clicks (CTR==0, CTR==1) against the categorical values in each column to measure (if any) the strength of association between the two variables. \n",
    "\n",
    "\n",
    "* **TO CONSIDER: What to do if 'null' is one of the top values for a categorical variable?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Cumulative Summation \n",
    "# Intent: In each column, we want to know which values contribute the most in terms of frequency for CTR=1. \n",
    "##########\n",
    "\n",
    "from pyspark.sql.functions import col, desc\n",
    "from pyspark.sql import window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "count_1 = train_sample_EDA.filter(col(\"CTR\")==1).count()\n",
    "count_0 = train_sample_EDA.filter(col(\"CTR\")==0).count()\n",
    "\n",
    "# Set aside a subset \n",
    "subset_CTR1=train_sample_EDA.filter(col(\"CTR\")==1)[categorical_cols]\n",
    "subset_CTR0=train_sample_EDA.filter(col(\"CTR\")==0)[categorical_cols]\n",
    "\n",
    "win_spec = (window.Window\n",
    "                  .partitionBy()\n",
    "                  .rowsBetween(window.Window.unboundedPreceding, 0))\n",
    "\n",
    "top_cumsum_ctr1 = []\n",
    "all_cumsum_ctr1 = []\n",
    "top_cumsum_ctr0 = []\n",
    "all_cumsum_ctr0 = []\n",
    "summary_cumsum = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # --- Calculate cumulative summation for CTR==1 \n",
    "    tempdf = subset_CTR1.groupBy([col]).count().sort(desc('count')).cache()\n",
    "    tempdf = tempdf.withColumn('count_frequ', tempdf['count']/count_1).cache()\n",
    "    tempdf = tempdf.withColumn('cumsum',F.sum(tempdf.count_frequ).over(win_spec))\n",
    "    all_cumsum_ctr1.append(tempdf)\n",
    "    temp_pd_CTR1 = tempdf.where(tempdf.cumsum<=0.95).toPandas()\n",
    "    top_cumsum_ctr1.append(temp_pd_CTR1)\n",
    "    \n",
    "    # --- Calculate cumulative summation for CTR==0 \n",
    "    tempdf = subset_CTR0.groupBy([col]).count().sort(desc('count')).cache()\n",
    "    tempdf = tempdf.withColumn('count_frequ', tempdf['count']/count_0).cache()\n",
    "    tempdf = tempdf.withColumn('cumsum',F.sum(tempdf.count_frequ).over(win_spec))\n",
    "    all_cumsum_ctr0.append(tempdf)\n",
    "    temp_pd_CTR0 = tempdf.where(tempdf.cumsum<=0.95).toPandas()\n",
    "    top_cumsum_ctr0.append(temp_pd_CTR0)\n",
    "    \n",
    "    # Summary stats: \n",
    "    # For each column: count up the number of distinct values that are needed to cover 95% of the observations in each group\n",
    "    summary_cumsum.append({'Column': col, 'top_freq_CTR0_count': len(temp_pd_CTR0), 'top_freq_CTR1_count': len(temp_pd_CTR1)})\n",
    "\n",
    "summary_cumsum_df = pd.DataFrame(summary_cumsum)\n",
    "\n",
    "############\n",
    "# Save the top values for each CTR group out for each categorical column \n",
    "# DO NOT run this part of the code unless you want to save out the individual CSVs. \n",
    "# These CSVs can be used for broadcasting later on. \n",
    "############ \n",
    "\n",
    "#for i in range(0, 26): \n",
    "#    filename = 'CTR0_'+str(categorical_cols[i])+'.csv'\n",
    "#    all_cumsum_ctr0[i].toPandas().to_csv(filename, index=False)\n",
    "#    filename = 'CTR1_'+str(categorical_cols[i])+'.csv'\n",
    "#    all_cumsum_ctr1[i].toPandas().to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>top_freq_CTR0_count</th>\n",
       "      <th>top_freq_CTR1_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Var14</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Var15</td>\n",
       "      <td>193</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Var16</td>\n",
       "      <td>484485</td>\n",
       "      <td>201579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Var17</td>\n",
       "      <td>120554</td>\n",
       "      <td>63302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Var18</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Var19</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Var20</td>\n",
       "      <td>5202</td>\n",
       "      <td>4407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Var21</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Var22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Var23</td>\n",
       "      <td>9279</td>\n",
       "      <td>7720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Var24</td>\n",
       "      <td>2193</td>\n",
       "      <td>1879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Var25</td>\n",
       "      <td>414817</td>\n",
       "      <td>179614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Var26</td>\n",
       "      <td>1640</td>\n",
       "      <td>1434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Var27</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Var28</td>\n",
       "      <td>2407</td>\n",
       "      <td>2333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Var29</td>\n",
       "      <td>288313</td>\n",
       "      <td>132526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Var30</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Var31</td>\n",
       "      <td>873</td>\n",
       "      <td>925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Var32</td>\n",
       "      <td>179</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Var33</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Var34</td>\n",
       "      <td>362959</td>\n",
       "      <td>158059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Var35</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Var36</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Var37</td>\n",
       "      <td>11359</td>\n",
       "      <td>8203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Var38</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Var39</td>\n",
       "      <td>6065</td>\n",
       "      <td>4824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column  top_freq_CTR0_count  top_freq_CTR1_count\n",
       "0   Var14                   18                   18\n",
       "1   Var15                  193                  187\n",
       "2   Var16               484485               201579\n",
       "3   Var17               120554                63302\n",
       "4   Var18                    5                    5\n",
       "5   Var19                    4                    4\n",
       "6   Var20                 5202                 4407\n",
       "7   Var21                    9                    8\n",
       "8   Var22                    1                    0\n",
       "9   Var23                 9279                 7720\n",
       "10  Var24                 2193                 1879\n",
       "11  Var25               414817               179614\n",
       "12  Var26                 1640                 1434\n",
       "13  Var27                    6                    5\n",
       "14  Var28                 2407                 2333\n",
       "15  Var29               288313               132526\n",
       "16  Var30                    7                    6\n",
       "17  Var31                  873                  925\n",
       "18  Var32                  179                  176\n",
       "19  Var33                    3                    3\n",
       "20  Var34               362959               158059\n",
       "21  Var35                    2                    2\n",
       "22  Var36                    6                    6\n",
       "23  Var37                11359                 8203\n",
       "24  Var38                    9                   10\n",
       "25  Var39                 6065                 4824"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_cumsum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# Cramer's V \n",
    "###### \n",
    "\n",
    "# Cramer's Value \n",
    "# referenced from https://stackoverflow.com/questions/20892799/using-pandas-calculate-cram%C3%A9rs-coefficient-matrix\n",
    "# and https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n",
    "\n",
    "def cramers_corrected_stat(confusion_matrix): \n",
    "    \"\"\"\n",
    "    This function calculates the Cramer's coefficient matrix given a contingency table. \n",
    "    It has been discussed before that cramer's V can be over-optimistic in estimating association between between categories. \n",
    "    Therefore, a correction for bias has been implemented in this function as well. \n",
    "    \"\"\"\n",
    "    # --- get total n \n",
    "    n = np.nansum(confusion_matrix.sum()) \n",
    "    \n",
    "    # --- get r,k (shape), where r = number of rows, k = number of columns \n",
    "    r,k = confusion_matrix.shape \n",
    "    \n",
    "    # --- get chi-2 statistic\n",
    "    chi2 = 0\n",
    "    row_sums = confusion_matrix.sum(axis=1) \n",
    "    col_sums = confusion_matrix.sum(axis=0) \n",
    "    for index, row in confusion_matrix.iterrows(): \n",
    "        # index will denote the row number \n",
    "        for col in confusion_matrix.columns: # iterate across rows \n",
    "            chi2+=((row[col]-(row_sums[index]*col_sums[col]/n))**2)/(row_sums[index]*col_sums[col]/n)\n",
    "    # --- get phi2 \n",
    "    phi2 = chi2/n \n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1)) \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "\n",
    "all_categorical_columns = ['Var14','Var15','Var16','Var17','Var18','Var19','Var20',\\\n",
    "                           'Var21','Var22','Var23','Var24','Var25','Var26','Var27',\\\n",
    "                           'Var28','Var29','Var30','Var31','Var32','Var33','Var34',\\\n",
    "                           'Var35','Var36','Var37','Var38','Var39']\n",
    "\n",
    "def calculate_adj_cramersV_allcolumns(columns, rdd_obj): \n",
    "    \"\"\"\n",
    "    This function will apply Cramer's value function (cramers_corrected_stat) \n",
    "    onto the values of categorical columns in the RDD. \n",
    "    \"\"\"\n",
    "    cramers_df = []\n",
    "    for c in columns: \n",
    "        # --- make contingency table \n",
    "        contingency = rdd_obj.map(lambda x: ((x['CTR'],x[c]),1)) \\\n",
    "            .reduceByKey(lambda x,y: x+y) \\\n",
    "            .map(lambda x: ((x[0][0],x[0][1]),x[1])).collect()\n",
    "        \n",
    "        # --- Unwrap contingency mapped output \n",
    "        unwrapped_df = pd.DataFrame([[contingency[i][0][0], contingency[i][0][1], \\\n",
    "                                    contingency[i][1]] for i in range(0, len(contingency))], \\\n",
    "                                    columns = ['ctr','category','count'])\\\n",
    "                                    .sort_values(by=['category','ctr']).reset_index(drop=True)\n",
    "        \n",
    "        # --- Make contingency table \n",
    "        matr_obj = pd.DataFrame(0, columns = unwrapped_df['category']\\\n",
    "                                .drop_duplicates().values, index = unwrapped_df['ctr']\\\n",
    "                                .drop_duplicates().values)\n",
    "        for index, row in unwrapped_df.iterrows(): \n",
    "            matr_obj.at[row['ctr'], row['category']]=row['count']\n",
    "        \n",
    "        # --- Calculate Cramer's V (adjusted) \n",
    "        cramersV_value = cramers_corrected_stat(matr_obj)\n",
    "        cramers_df.append({'column': c, 'cramersvalue': cramersV_value})\n",
    "    return(cramers_df)\n",
    "\n",
    "data_rdd = train_sample_EDA.rdd\n",
    "\n",
    "all_categorical_columns = ['Var14','Var15','Var16','Var17','Var18','Var19','Var20',\n",
    "                           'Var21','Var22','Var23','Var24','Var25','Var26','Var27',\n",
    "                           'Var28','Var29','Var30','Var31','Var32','Var33','Var34',\n",
    "                           'Var35','Var36','Var37','Var38','Var39']\n",
    "\n",
    "cramers_results = pd.DataFrame(calculate_adj_cramersV_allcolumns(all_categorical_columns, data_rdd))\\\n",
    "                    .reset_index(drop=True)\n",
    "cramers_results.columns=['Column','cramersvalue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore combine all of our summary statistics together. In table 3.3, for each categorical variable, we provide the \n",
    "* number of distinct values (`count_distinct`)\n",
    "* number of categorical values that cover 95% of the observations in each labelled group (CTR==0 in `top_freq_CTR0_count` and CTR==1 in `top_freq_CTR1_count`). \n",
    "* percentage of observations in the categorical column that are not null (`Coverage_nonNull`). \n",
    "* Cramer's Value between the categorical column and labelled CTR variable (`Cramersvalue`). \n",
    "\n",
    "When sorted, we note a few things: \n",
    "* In general, it looks like columns with a lower number of distinct values tend to also have lower Cramer's coefficient with our labelled CTR variable. On the opposite end, it seems like columns with a higher number of distinct values tend to have more association with the labelled CTR variable. \n",
    "* For almost all of the categorical columns, we can see that there are fewer features that cover the majority of observations for CTR = 1 than CTR = 0. \n",
    "    * For example: For column Var25, we observe that there are 10844 categorical values (26% of all distinct values in Var25) that cover 95% or more of the observations that have a label of CTR = 1. In contrast, we see that there are 28315 categorical values (68% of all distinct values in Var25) that cover 95% or more of the observations that have a label of CTR = 0. \n",
    "    * This leads to our discussion of frequency-based reduction in dimensionality in Section 4.1.1, as it looks likely that not all individual values are needed within a column to provide prediction power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# Combine summary statistics together for all categorical data \n",
    "####### \n",
    "\n",
    "# (1) Merge distinct counts of values for each column against the cumulative summations \n",
    "ver1 = distinct_df.merge(summary_cumsum_df, how='left', on='Column')\n",
    "\n",
    "# (2) Merge output from (1) against percentage of non-null values for each categorical column. \n",
    "coverage_tomerge = pd.DataFrame([list(coverage_summary.index.values), list(coverage_summary['Coverage_nonNull'].values)]).T\n",
    "coverage_tomerge.columns=['Column','Coverage_nonNull']\n",
    "ver2 = ver1.merge(coverage_tomerge, how='left', on='Column')\n",
    "\n",
    "# (3) Merge output from (2) against the Cramer's coefficient results \n",
    "categorical_combined_df = ver2.merge(cramers_results, how='left', on='Column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>count_distinct</th>\n",
       "      <th>top_freq_CTR0_count</th>\n",
       "      <th>top_freq_CTR1_count</th>\n",
       "      <th>Coverage_nonNull</th>\n",
       "      <th>cramersvalue</th>\n",
       "      <th>pct_topfreq_CTR0</th>\n",
       "      <th>pct_topfreq_CTR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Var29</td>\n",
       "      <td>34617</td>\n",
       "      <td>23274</td>\n",
       "      <td>9441</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.315933</td>\n",
       "      <td>0.672329</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Var25</td>\n",
       "      <td>41312</td>\n",
       "      <td>28315</td>\n",
       "      <td>10844</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.312090</td>\n",
       "      <td>0.685394</td>\n",
       "      <td>0.262490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Var34</td>\n",
       "      <td>38618</td>\n",
       "      <td>26336</td>\n",
       "      <td>10208</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.310233</td>\n",
       "      <td>0.681962</td>\n",
       "      <td>0.264333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Var16</td>\n",
       "      <td>43870</td>\n",
       "      <td>30314</td>\n",
       "      <td>11370</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.307947</td>\n",
       "      <td>0.690996</td>\n",
       "      <td>0.259175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Var17</td>\n",
       "      <td>25184</td>\n",
       "      <td>16671</td>\n",
       "      <td>6950</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.299285</td>\n",
       "      <td>0.661968</td>\n",
       "      <td>0.275969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Var28</td>\n",
       "      <td>5238</td>\n",
       "      <td>2135</td>\n",
       "      <td>1826</td>\n",
       "      <td>1</td>\n",
       "      <td>0.287163</td>\n",
       "      <td>0.407598</td>\n",
       "      <td>0.348606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Var20</td>\n",
       "      <td>7623</td>\n",
       "      <td>4350</td>\n",
       "      <td>3125</td>\n",
       "      <td>1</td>\n",
       "      <td>0.270162</td>\n",
       "      <td>0.570641</td>\n",
       "      <td>0.409944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Var37</td>\n",
       "      <td>12335</td>\n",
       "      <td>6635</td>\n",
       "      <td>3449</td>\n",
       "      <td>0.96065</td>\n",
       "      <td>0.269548</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.279611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Var31</td>\n",
       "      <td>2548</td>\n",
       "      <td>795</td>\n",
       "      <td>811</td>\n",
       "      <td>1</td>\n",
       "      <td>0.259051</td>\n",
       "      <td>0.312009</td>\n",
       "      <td>0.318289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Var24</td>\n",
       "      <td>3799</td>\n",
       "      <td>2000</td>\n",
       "      <td>1534</td>\n",
       "      <td>1</td>\n",
       "      <td>0.247067</td>\n",
       "      <td>0.526454</td>\n",
       "      <td>0.403790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Var26</td>\n",
       "      <td>2796</td>\n",
       "      <td>1494</td>\n",
       "      <td>1191</td>\n",
       "      <td>1</td>\n",
       "      <td>0.239850</td>\n",
       "      <td>0.534335</td>\n",
       "      <td>0.425966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Var39</td>\n",
       "      <td>9527</td>\n",
       "      <td>4198</td>\n",
       "      <td>2478</td>\n",
       "      <td>0.58529</td>\n",
       "      <td>0.232717</td>\n",
       "      <td>0.440642</td>\n",
       "      <td>0.260103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Var23</td>\n",
       "      <td>10997</td>\n",
       "      <td>5866</td>\n",
       "      <td>3702</td>\n",
       "      <td>1</td>\n",
       "      <td>0.208153</td>\n",
       "      <td>0.533418</td>\n",
       "      <td>0.336637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Var15</td>\n",
       "      <td>497</td>\n",
       "      <td>185</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>0.201925</td>\n",
       "      <td>0.372233</td>\n",
       "      <td>0.358149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Var30</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.169796</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Var36</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.124470</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Var27</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.117776</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Var32</td>\n",
       "      <td>1303</td>\n",
       "      <td>188</td>\n",
       "      <td>206</td>\n",
       "      <td>0.58529</td>\n",
       "      <td>0.104115</td>\n",
       "      <td>0.144282</td>\n",
       "      <td>0.158097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Var22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098506</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Var38</td>\n",
       "      <td>51</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0.58529</td>\n",
       "      <td>0.086958</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Var19</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.86292</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Var33</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.58529</td>\n",
       "      <td>0.045960</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Var35</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18434</td>\n",
       "      <td>0.029709</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Var14</td>\n",
       "      <td>541</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033272</td>\n",
       "      <td>0.033272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Var18</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Var21</td>\n",
       "      <td>257</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031128</td>\n",
       "      <td>0.035019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column  count_distinct  top_freq_CTR0_count  top_freq_CTR1_count  \\\n",
       "15  Var29           34617                23274                 9441   \n",
       "11  Var25           41312                28315                10844   \n",
       "20  Var34           38618                26336                10208   \n",
       "2   Var16           43870                30314                11370   \n",
       "3   Var17           25184                16671                 6950   \n",
       "14  Var28            5238                 2135                 1826   \n",
       "6   Var20            7623                 4350                 3125   \n",
       "23  Var37           12335                 6635                 3449   \n",
       "17  Var31            2548                  795                  811   \n",
       "10  Var24            3799                 2000                 1534   \n",
       "12  Var26            2796                 1494                 1191   \n",
       "25  Var39            9527                 4198                 2478   \n",
       "9   Var23           10997                 5866                 3702   \n",
       "1   Var15             497                  185                  178   \n",
       "16  Var30              10                    7                    6   \n",
       "22  Var36              14                    6                    6   \n",
       "13  Var27              26                    6                    5   \n",
       "18  Var32            1303                  188                  206   \n",
       "8   Var22               3                    1                    1   \n",
       "24  Var38              51                   11                   12   \n",
       "5   Var19              12                    4                    5   \n",
       "19  Var33               4                    3                    3   \n",
       "21  Var35              11                    2                    2   \n",
       "0   Var14             541                   18                   18   \n",
       "4   Var18             145                    5                    5   \n",
       "7   Var21             257                    8                    9   \n",
       "\n",
       "   Coverage_nonNull  cramersvalue  pct_topfreq_CTR0  pct_topfreq_CTR1  \n",
       "15          0.96065      0.315933          0.672329          0.272727  \n",
       "11          0.96065      0.312090          0.685394          0.262490  \n",
       "20          0.96065      0.310233          0.681962          0.264333  \n",
       "2           0.96065      0.307947          0.690996          0.259175  \n",
       "3           0.96065      0.299285          0.661968          0.275969  \n",
       "14                1      0.287163          0.407598          0.348606  \n",
       "6                 1      0.270162          0.570641          0.409944  \n",
       "23          0.96065      0.269548          0.537900          0.279611  \n",
       "17                1      0.259051          0.312009          0.318289  \n",
       "10                1      0.247067          0.526454          0.403790  \n",
       "12                1      0.239850          0.534335          0.425966  \n",
       "25          0.58529      0.232717          0.440642          0.260103  \n",
       "9                 1      0.208153          0.533418          0.336637  \n",
       "1                 1      0.201925          0.372233          0.358149  \n",
       "16                1      0.169796          0.700000          0.600000  \n",
       "22                1      0.124470          0.428571          0.428571  \n",
       "13                1      0.117776          0.230769          0.192308  \n",
       "18          0.58529      0.104115          0.144282          0.158097  \n",
       "8                 1      0.098506          0.333333          0.333333  \n",
       "24          0.58529      0.086958          0.215686          0.235294  \n",
       "5           0.86292      0.054795          0.333333          0.416667  \n",
       "19          0.58529      0.045960          0.750000          0.750000  \n",
       "21          0.18434      0.029709          0.181818          0.181818  \n",
       "0                 1      0.000000          0.033272          0.033272  \n",
       "4                 1      0.000000          0.034483          0.034483  \n",
       "7                 1      0.000000          0.031128          0.035019  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "# Table 3.3\n",
    "########\n",
    "\n",
    "categorical_combined_df['pct_topfreq_CTR0'] = categorical_combined_df['top_freq_CTR0_count']/categorical_combined_df['count_distinct']\n",
    "categorical_combined_df['pct_topfreq_CTR1'] = categorical_combined_df['top_freq_CTR1_count']/categorical_combined_df['count_distinct']\n",
    "\n",
    "categorical_combined_df.sort_values(by=['cramersvalue','Coverage_nonNull'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Clean-up and transforming numerical variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA reveiled the presence of negative values contained in the second numerical variable (`Var2`). Given the  interpretation of these variables as counters of some sort, a negative value appears unjustified, and as such need to be imputed. It is first noticed that `Var2` is the only numerical variable among those surveyed which does not contain null values. This may be indicative of the fact that negative values are used for this variable to effectively identify missing values.\n",
    "\n",
    "The above being said, the relative frequency of negative numbers in this feature is estimated as approximately 10%, which is aligned with the coverage of the other numerical variables. This reinforces the interpretation of a negative value in this column as representative of invalid entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10351229059565792"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_EDA.filter(train_sample_EDA[\"Var2\"]<0).count()/train_sample_EDA.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Owing to the above, it is decided to impute negative values with the average of the non-negative values of the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "avgNonnegVar2 = train_sample_EDA.filter(train_sample_EDA[\"Var2\"]>=0).agg(avg('Var2')).first()[0]\n",
    "from pyspark.sql.functions import when\n",
    "train_sample_EDA = train_sample_EDA.withColumn('Var2', when(train_sample_EDA['Var2']<0, avgNonnegVar2)\\\n",
    "                            .otherwise(train_sample_EDA.Var2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the same token, missing values in all other nuerical columns are imputed to the average of their respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "for col in numerical_cols:\n",
    "    train_sample_EDA = train_sample_EDA.na.fill(round(train_sample_EDA.na.drop().agg(avg(col)).first()[0],1), [col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Do we want to impute/discuss imputation of categorical variables here as well?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple options: (TBD...)\n",
    "\n",
    "# For categorical: \n",
    "# Things to consider: (A) Cramer's value (pick only features that hit X threshold of Cramer's value)? \n",
    "# Option 1: Take the features that cover 95% of class CTR==1 only, code everything else + Null as a \"lump all\" category. \n",
    "# Cons of Option 1: Can we treat 'null' and 'everything else' equally? \n",
    "# Option 2: If count of a specific distinct value is < 10, we ignore that value. \n",
    "# Cons of Option 2: Doesn't exactly reduce dimensionality, especially when we have distinct values that skyrocket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate train data for homegrown solution - select only 10000 rows and only numerical features + target \n",
    "train_sample_red = train_sample_EDA.select(['CTR'] + numerical_cols).limit(10000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+----+-----+----+----+----+----+-----+-----+-----+-----+\n",
      "|CTR|Var1|Var2|Var3|Var4| Var5|Var6|Var7|Var8|Var9|Var10|Var11|Var12|Var13|\n",
      "+---+----+----+----+----+-----+----+----+----+----+-----+-----+-----+-----+\n",
      "|  0|   0| 1.0|  28|   0|16597| 557|   3|   5| 123|    0|    1|    1|    1|\n",
      "|  0|   1| 0.0|   1|   9| 1427|   3|  16|  11|  50|    0|    2|    1|    8|\n",
      "|  0|   6| 1.0|  28|   9|23255|  62|   0|   1|  73|    0|    0|    1|    8|\n",
      "|  0|   0|37.0|  23|   9| 1635|  84|   2|  17| 109|    0|    2|    1|   50|\n",
      "|  0|   2| 0.0|   9|   5|   44|   5|   2|   4|   5|    2|    2|    1|    5|\n",
      "+---+----+----+----+----+-----+----+----+----+----+-----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sample_red.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted here that we apply a log transformation (specifically `log(x+1)`) to the numerical variables. \n",
    "* Such a decision came after EDA revealed that most numerical variables are skewed right and have very long right tails. To \"correct\" for the heavy unequal distribution of numerical variables, the log transformation helps to \"unskew\" the numerical data. \n",
    "* We also chose `log(x+1)` for the convenience of returning 0 when x = 0 (i.e. `log(x+1) when x = 0 will still return log(1) = 0`)). Given that Criteo has mentioned on Kaggle that most of the numerical variables are in fact counts, we do not wish to apply a transformation where 0's are suddenly changed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache into rdd and apply log transformation\n",
    "#train_sample_red_RDD = train_sample_red.rdd.map(lambda x: (x[0], np.log(np.array(x[1:])+1))).cache()\n",
    "train_sample_red_RDD = train_sample_red.rdd.map(lambda x: (x[0], np.log(np.array(x[1:])+1))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataRDD):\n",
    "    \"\"\"\n",
    "    Scale and center data round mean of each feature.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "    Returns:\n",
    "        normedRDD - records are tuples of (y, features_array)\n",
    "    \"\"\"\n",
    "    featureMeans = dataRDD.map(lambda x: x[1]).mean()\n",
    "    featureStdev = np.sqrt(dataRDD.map(lambda x: x[1]).variance())\n",
    "    normedRDD = dataRDD.map(lambda x: (x[0], (x[1] - featureMeans)/featureStdev))\n",
    "\n",
    "    return normedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogLoss(dataRDD, W, regType = None, regParam=0.05):\n",
    "    \"\"\"\n",
    "    Compute log loss function.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient defaults to 0.1\n",
    "    Returns:\n",
    "        loss - (float) the regularized loss\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[1]), x[0])).cache()\n",
    "\n",
    "    # add regularization term\n",
    "    reg_term = 0\n",
    "    if regType == 'ridge':\n",
    "        reg_term = regParam*np.linalg.norm(W[1:])\n",
    "    elif regType == 'lasso':\n",
    "        reg_term = regParam*np.sum(np.abs(W[1:]))\n",
    "        \n",
    "    #broadcast model\n",
    "    #W = sc.broadcast(W) #uncomment this line when deploying it on the cloud\n",
    "    \n",
    "    # compute loss\n",
    "    loss = augmentedData.map(lambda x: x[1]*np.log(1 + np.exp(-np.dot(x[0], W))) + \\\n",
    "                             (1 - x[1])*(np.dot(x[0], W) + np.log(1 + np.exp(-np.dot(x[0], W))))).sum()\\\n",
    "                            /augmentedData.count()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def LogLoss_grad(dataRDD, W, regType = None, regParam=0.05):\n",
    "    \"\"\"\n",
    "    Compute log loss function inside gradient descent.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient defaults to 0.1\n",
    "    Returns:\n",
    "        loss - (float) the regularized loss\n",
    "    \"\"\"\n",
    "\n",
    "    # add regularization term\n",
    "    reg_term = 0\n",
    "    if regType == 'ridge':\n",
    "        reg_term = regParam*np.linalg.norm(W[1:])\n",
    "    elif regType == 'lasso':\n",
    "        reg_term = regParam*np.sum(np.abs(W[1:]))\n",
    "    \n",
    "    # compute loss\n",
    "    loss = dataRDD.map(lambda x: x[1]*np.log(1 + np.exp(-np.dot(x[0], W))) + \\\n",
    "                             (1 - x[1])*(np.dot(x[0], W) + np.log(1 + np.exp(-np.dot(x[0], W))))).sum()\\\n",
    "                            /dataRDD.count()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GDUpdate(trainRDD, testRDD, W, nSteps = 20, regType = None, regParam=0.05, learningRate = 0.05, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps of regularized gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        regType - (str) 'ridge' or 'lasso', defaults to None\n",
    "        regParam - (float) regularization term coefficient defaults to 0.1\n",
    "        learningRate - (float) defaults to 0.1\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "        training_loss (float) training loss for new_model\n",
    "        test_loss (float) test loss for new_model\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedTrainData = trainRDD.map(lambda x: (np.append([1.0], x[1]), x[0])).cache()\n",
    "    augmentedTestData = testRDD.map(lambda x: (np.append([1.0], x[1]), x[0])).cache()\n",
    "    \n",
    "    # compute size of training sample\n",
    "    sizeTrainSample = augmentedTrainData.count()\n",
    "    \n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history = [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = W\n",
    "    #broadcast model\n",
    "    #model = sc.broadcast(W) #uncomment this line when deploying it on the cloud\n",
    "    for idx in range(nSteps):\n",
    "        # add regularization term\n",
    "        reg_term = np.zeros(len(model))\n",
    "        if regType == 'ridge':\n",
    "            reg_term = np.append(0,2*regParam*model[1:])\n",
    "        elif regType == 'lasso':\n",
    "            reg_term = np.append(0,regParam*np.sign(model[1:]))\n",
    "    \n",
    "        # compute gradient\n",
    "        grad = augmentedTrainData.map(lambda x: ((1/(1 + np.exp(-np.dot(x[0], model))) - x[1])*x[0]))\\\n",
    "               .sum()/sizeTrainSample + reg_term\n",
    "    \n",
    "        #update model parameters\n",
    "        new_model = model - learningRate*grad\n",
    "        #new_model = sc.broadcast(new_model) #uncomment this line when deploying it on the cloud\n",
    "        training_loss = LogLoss_grad(augmentedTrainData, new_model, regType=regType, regParam=regParam)\n",
    "        test_loss = LogLoss_grad(augmentedTestData, new_model, regType=regType, regParam=regParam)\n",
    "        \n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(new_model)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            print(f\"Model: {[k for k in new_model]}\")\n",
    "        \n",
    "        model = new_model\n",
    "        #broadcast model\n",
    "        #model = sc.broadcast(new_model) #uncomment this line when deploying it on the cloud\n",
    "   \n",
    "    return train_history, test_history, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrediction(dataRDD, W, treshProb=0.5):\n",
    "    \"\"\"\n",
    "    Make predictions of target and compute number of: true positives, true negatives, \n",
    "    false positive, false negatives .\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "        treshProb- (float) threshold probability for imputation of positive labels\n",
    "    Returns:\n",
    "        pred - (rdd) predicted targets\n",
    "        ntp - (integer) number of true positives\n",
    "        ntn - (integer) number of true negatives\n",
    "        nfp - (integer) number of false positives\n",
    "        nfn - (integer) number of false negatives\n",
    "    \"\"\"\n",
    "    # add a bias 'feature' of 1 at index 0\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[1]), x[0])).cache()\n",
    "    \n",
    "    # compute prediction\n",
    "    pred = augmentedData.map(lambda x: (int((1/(1 + np.exp(-np.dot(x[0], W))))>treshProb), x[1] )).cache()\n",
    "    \n",
    "    ntp = pred.map(lambda x: int((x[0]*x[1]) == 1)).sum()\n",
    "    ntn = pred.map(lambda x: int((x[0]+x[1]) == 0)).sum()\n",
    "    nfp = pred.map(lambda x: int((x[0] == 1) * (x[1] == 0))).sum()\n",
    "    nfn = pred.map(lambda x: int((x[0] == 0) * (x[1] == 1))).sum()\n",
    "   \n",
    "    return pred, ntp, ntn, nfp, nfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "normedRDD = normalize(train_sample_red_RDD).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train, validation and test sets\n",
    "train, validation, test = normedRDD.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of features\n",
    "numb_features = len(train_sample_red.columns) - 1\n",
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nSteps = 150\n",
    "regType = 'ridge'\n",
    "regParam = 0.01\n",
    "learningRate = 0.1\n",
    "\n",
    "# run gradient descent\n",
    "train_loss, test_loss, model = GDUpdate(train, validation, BASELINE, nSteps, regType=regType, \n",
    "                                        regParam=regParam, learningRate=learningRate, verbose = False)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.7638821825205215\n",
      "Precision is:  0.5042016806722689\n",
      "Recall is:  0.12244897959183673\n",
      "F1 score is:  0.19704433497536944\n",
      "False positive rate is:  0.03731815306767868\n",
      "True positive rate is:  0.12244897959183673\n"
     ]
    }
   ],
   "source": [
    "# make predictions and compute metrics for treshProb = 0.5\n",
    "pred, ntp, ntn, nfp, nfn = makePrediction(validation, model[-1], 0.5)\n",
    "\n",
    "acc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'True positive rate')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate ROC curve\n",
    "tprSave = []\n",
    "fprSave = []\n",
    "for i in np.arange(0.0, 1.0, 0.1):\n",
    "    pred, ntp, ntn, nfp, nfn = makePrediction(validation, model[-1], i)\n",
    "    fprSave.append(nfp/(ntn+nfp))\n",
    "    tprSave.append(ntp/(ntp+nfn))\n",
    "\n",
    "plt.plot(fprSave, tprSave)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression will take care of the linear terms, now to account for the interaction term, we expand to include 2nd Degree polynomial features. The below equation represents the formulation.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} +  \\sum_{i=1}^n \\sum_{j=i+1}^n w_{ij} x_{i} x_{j}\n",
    "\\end{align}\n",
    "\n",
    "The challenge with solving the above equation is that the time complexity is $O(n^2)$\n",
    "\n",
    "In order to work with this, we use a matrix factorization technique for the interaction terms, inspired by Matrix factorization. We introduce a hyperpameter K which represent the latent factors for factorizing the weight vector $w_{ij}$.\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\sum_{i=1}^{n} \\sum_{j=i+1}^n \\langle \\textbf{v}_i , \\textbf{v}_{j} \\rangle x_i x_{j}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Using the computation specified in Stephen Rendles paper, we can simplify the interaction term to the below equation.\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j}\n",
    "&= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j} - \\frac{1}{2} \\sum_{i=1}^n \\langle \\textbf{v}_i , \\textbf{v}_{i} \\rangle x_{i} x_{i}  \\\\\n",
    "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j} \\right)\\frac{1}{2}\\left( \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
    "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j}  -  \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
    "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left(\\sum_{i=1}^n v_{i,f}x_{i} \\right) \\left( \\sum_{j=1}^n v_{j,f}x_{j} \\right) - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right) \\\\\n",
    "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align}\n",
    "\n",
    "So, we can rewrite the equation to compute in $O(n)$ as\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align}\n",
    "\n",
    "#### Gradient\n",
    "\n",
    "For our classification problem, we can define the gradients as :\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial\\theta}\\hat{y}(\\textbf{x}) =\n",
    "\\begin{cases}\n",
    "1,  & \\text{if $\\theta$ is $w_0$} \\\\\n",
    "x_i, & \\text{if $\\theta$ is $w_i$} \\\\\n",
    "x_i\\sum_{j=1}^{n} v_{j,f}x_j - v_{i,f}x_{i}^2 & \\text{if $\\theta$ is $v_{i,f}$}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "#### For labels -1&1\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\hat{y} }(\\textbf{L}) =\n",
    "\\frac{d}{d \\hat{y}}\\left[ -y \\hat{y} + \\ln \\big(e^{y \\hat{y}} + 1 \\big) \\right] \n",
    "&= \\frac{1}{e^{y \\hat{y}} + 1} \\cdot  \\frac{d}{dx}\\left[e^{y \\hat{y}} + 1 \\right] - y \\\\\n",
    "&= \\frac{ye^{y \\hat{y}}}{e^{y \\hat{y}} + 1} - y\\\\\n",
    "&= \\frac{-y}{e^{y \\hat{y}} + 1}\n",
    "\\end{align}\n",
    "\n",
    "#### For labels 0,1\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\hat{y} }(\\textbf{L}) =\n",
    "\\frac{d}{d \\hat{y}}\\left[ -y \\hat{y} + \\ln \\big(e^{ \\hat{y}} + 1 \\big) \\right] \n",
    "&= \\frac{1}{e^{\\hat{y}} + 1} \\cdot  \\frac{d}{dx}\\left[e^{ \\hat{y}} + 1 \\right] - y \\\\\n",
    "&= \\frac{e^{ \\hat{y}}}{e^{\\hat{y}} + 1} - y\\\\\n",
    "&= \\frac{-y-e^{\\hat{y}}(y-1)}{e^{ \\hat{y}} + 1}\n",
    "\\end{align}\n",
    "\n",
    "The gradient of loss is defined by (by using chain rule):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta }(\\textbf{L}) = \n",
    "\\frac{\\partial}{\\partial \\hat{y} }(\\textbf{L}) \\centerdot \\frac{\\partial}{\\partial \\theta}\\hat{y}(\\textbf{x}) \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmLoss(dataRDD, w, w1,w0) :\n",
    "    \"\"\"\n",
    "    Computes the logloss given the data and model W\n",
    "    dataRDD - array of features, label\n",
    "    \"\"\"\n",
    "    w_bc = sc.broadcast(w)\n",
    "    w1_bc = sc.broadcast(w1)\n",
    "    w0_bc = sc.broadcast(w0)\n",
    "    def probability_value(x,W,W1,W0): \n",
    "        xa = np.array([x])\n",
    "        V =  xa.dot(W)\n",
    "        V_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(V*V - V_square).sum() + xa.dot(W1.T) + W0\n",
    "        return 1.0/(1.0 + np.exp(-phi))\n",
    "    \n",
    "    loss = dataRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x:  (probability_value(x[1],w_bc.value, w1_bc.value, w0_bc.value), x[0])) \\\n",
    "        .map(lambda x: (1 - 1e-12, x[1]) if x[0] == 1 else ((1e-12, x[1]) if x[0] == 0  else (x[0],x[1]))) \\\n",
    "        .map(lambda x: -(x[1] * np.log(x[0]) + (1-x[1])*np.log(1-x[0]))).mean()\n",
    "    \n",
    "    \n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmGradUpdate_v1(dataRDD, w, w1, w0, alpha, regParam, regParam1, regParam0):\n",
    "    \"\"\"\n",
    "    Computes the gradient and updates the model\n",
    "    \"\"\"\n",
    "    \n",
    "    w_bc = sc.broadcast(w)\n",
    "    w1_bc = sc.broadcast(w1)\n",
    "    w0_bc = sc.broadcast(w0)\n",
    "    rp_bc = sc.broadcast(regParam)\n",
    "    rp1_bc = sc.broadcast(regParam1)\n",
    "    rp0_bc = sc.broadcast(regParam0)\n",
    "    \n",
    "    #Gradient for interaction term\n",
    "    \n",
    "    def row_grad(x, y, W, W1, W0, regParam, regParam1, regParam0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        expnyt = np.exp(y*phi) \n",
    "        grad_loss = (-y/(1+expnyt))*(xa.T.dot(xa).dot(W) - np.diag(np.square(x)).dot(W))\n",
    "        return 2*regParam*W + grad_loss\n",
    "    \n",
    "    #Gradient for Linear term\n",
    "    def row_grad1(x, y, W, W1, W0, regParam, regParam1, regParam0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        expnyt = np.exp(y*phi)\n",
    "        grad_loss1 = (-y/(1+expnyt))*xa\n",
    "        return 2*regParam1*W1 + grad_loss1\n",
    "    \n",
    "    #Gradient for bias term\n",
    "    def row_grad0(x, y, W, W1, W0, regParam, regParam1, regParam0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        expnyt = np.exp(y*phi)\n",
    "        grad_loss0 = (-y/(1+expnyt))*1\n",
    "        return 2*regParam0*W0 +grad_loss0\n",
    "    \n",
    "   \n",
    "    \n",
    "    batchRDD = dataRDD.sample(False, 0.01, 2019)  \n",
    "    grad = batchRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x: (1, row_grad(x[1], x[0], w_bc.value, w1_bc.value, w0_bc.value, rp_bc.value,rp1_bc.value,rp0_bc.value))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    model = w - alpha * grad.values().collect()[0] \n",
    "    \n",
    "    grad1 = batchRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x: (1, row_grad1(x[1], x[0], w_bc.value, w1_bc.value, w0_bc.value, rp_bc.value,rp1_bc.value,rp0_bc.value))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    model1 = w1 - alpha * grad1.values().collect()[0]\n",
    "    \n",
    "    grad0 = batchRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x: (1, row_grad0(x[1], x[0], w_bc.value, w1_bc.value, w0_bc.value, rp_bc.value,rp1_bc.value,rp0_bc.value))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    model0 = w0 - alpha * grad0.values().collect()[0]\n",
    "    \n",
    "    return model, model1 ,model0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent(trainRDD, testRDD, model, model1, model0, nSteps = 20, \n",
    "                    learningRate = 0.01, regParam = 0.01,regParam1 = 0.01,regParam0 = 0.01, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of OLS gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history, model1_history, model0_history = [], [], [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    model1 = wInit1\n",
    "    model0 = wInit0\n",
    "    for idx in range(nSteps): \n",
    "        \n",
    "        ############## YOUR CODE HERE #############\n",
    "        \n",
    "        model, model1, model0 = fmGradUpdate_v1(trainRDD, model, model1, model0, learningRate, regParam, regParam1, regParam0)\n",
    "        training_loss = fmLoss(trainRDD, model, model1, model0) \n",
    "        test_loss = fmLoss(testRDD, model, model1, model0) \n",
    "        ############## (END) YOUR CODE #############\n",
    "        # keep track of test/train loss for plotting\n",
    "        train_history.append(training_loss)\n",
    "        test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        model1_history.append(model1)\n",
    "        model0_history.append(model0)\n",
    "        \n",
    "        # console output if desired\n",
    "        if verbose:\n",
    "            print(\"----------\")\n",
    "            print(f\"STEP: {idx+1}\")\n",
    "            print(f\"training loss: {training_loss}\")\n",
    "            print(f\"test loss: {test_loss}\")\n",
    "            #print(f\"Model: {[k for k in model]}\")\n",
    "   \n",
    "    return train_history, test_history, model_history, model1_history, model0_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wInitialization(dataRDD, factor):\n",
    "    nrFeat = len(dataRDD.first()[1])\n",
    "    np.random.seed(int(time.time())) \n",
    "    w =  np.random.ranf((nrFeat, factor))\n",
    "    w = w / np.sqrt((w*w).sum())\n",
    "    \n",
    "    w1 =  np.random.ranf(nrFeat)\n",
    "    w1 = w1 / np.sqrt((w1*w1).sum())\n",
    "    \n",
    "    w0 =  np.random.ranf(1)\n",
    "    \n",
    "    return w, w1, w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n",
      "training loss: 2.5145740236000593\n",
      "test loss: 2.6119237063792937\n",
      "----------\n",
      "STEP: 2\n",
      "training loss: 1.6852464572870445\n",
      "test loss: 1.7561552201332404\n",
      "----------\n",
      "STEP: 3\n",
      "training loss: 1.1257873518006614\n",
      "test loss: 1.1777418372393724\n",
      "----------\n",
      "STEP: 4\n",
      "training loss: 0.7383719342093316\n",
      "test loss: 0.7760927624417066\n",
      "----------\n",
      "STEP: 5\n",
      "training loss: 0.46374464568493085\n",
      "test loss: 0.4905294835054661\n",
      "----------\n",
      "STEP: 6\n",
      "training loss: 0.2627317564227274\n",
      "test loss: 0.28092319932853976\n",
      "----------\n",
      "STEP: 7\n",
      "training loss: 0.11085935782171655\n",
      "test loss: 0.12215120575838116\n",
      "----------\n",
      "STEP: 8\n",
      "training loss: -0.007081911216139475\n",
      "test loss: -0.0014376233778369105\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "wInit, wInit1, wInit0 = wInitialization(train, 2)\n",
    "logerr_train, logerr_test, models, model1s, model0s = GradientDescent(train, validation, wInit, wInit1, wInit0, nSteps = 100,\n",
    "                                                    learningRate = 0.002, regParam = 0.01, regParam1 = 0.01, regParam0 = 0.01, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"The estimated model is: {models[-1]}\")\n",
    "print(f\"The loss of the estimated model is: {logerr_train[-1]}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(logerr_train)\n",
    "plt.plot(logerr_test)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmMakePrediction(dataRDD, w, w1, w0):\n",
    "    \"\"\"\n",
    "    Perform one regularized gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        pred - (rdd) predicted targets\n",
    "    \"\"\"\n",
    "    w_bc = sc.broadcast(w)\n",
    "    w1_bc = sc.broadcast(w1)\n",
    "    w0_bc = sc.broadcast(w0)\n",
    "    def predict_fm(x, W, W1, W0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        return 1.0/(1.0 + np.exp(-phi))\n",
    "    \n",
    "    # compute prediction\n",
    "    pred = dataRDD.map(lambda x: (int(predict_fm(x[1],w_bc.value, w1_bc.value, w0_bc.value)>0.5), x[0] ))\n",
    "    ntp = pred.map(lambda x: int((x[0]*x[1]) == 1)).sum()\n",
    "    ntn = pred.map(lambda x: int((x[0]+x[1]) == 0)).sum()\n",
    "    nfp = pred.map(lambda x: int((x[0] == 1) * (x[1] == 0))).sum()\n",
    "    nfn = pred.map(lambda x: int((x[0] == 0) * (x[1] == 1))).sum()\n",
    "   \n",
    "    return pred, ntp, ntn, nfp, nfn\n",
    "   \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, ntp, ntn, nfp, nfn = fmMakePrediction(validation, models[-1], model1s[-1], model0s[-1])\n",
    "\n",
    "cc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Adding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "The earlier section showed a logistic regression model with numerical variables only. In this section, we incrementally add categorical variables and redo the logistic regression model on a small data set. Categorical variables present a challenge because each of them can have a million different values thereby creating millions of dimensions. To get around \"the curse of dimensionality\", we looked at quite a few methods to reduce the dimensions to a manageable level. This includes frequency based dimensionality reduction and hashing techniques at a feature level and at a collection of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"The curse of dimensionality\" with categorical variables\n",
    "Below, we take a look at the number of dimensions in the train_sample dataset for each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column _15 has 541 unique categorical values\n",
      "Column _16 has 497 unique categorical values\n",
      "Column _17 has 43870 unique categorical values\n",
      "Column _18 has 25184 unique categorical values\n",
      "Column _19 has 145 unique categorical values\n",
      "Column _20 has 12 unique categorical values\n",
      "Column _21 has 7623 unique categorical values\n",
      "Column _22 has 257 unique categorical values\n",
      "Column _23 has 3 unique categorical values\n",
      "Column _24 has 10997 unique categorical values\n",
      "Column _25 has 3799 unique categorical values\n",
      "Column _26 has 41312 unique categorical values\n",
      "Column _27 has 2796 unique categorical values\n",
      "Column _28 has 26 unique categorical values\n",
      "Column _29 has 5238 unique categorical values\n",
      "Column _30 has 34617 unique categorical values\n",
      "Column _31 has 10 unique categorical values\n",
      "Column _32 has 2548 unique categorical values\n",
      "Column _33 has 1303 unique categorical values\n",
      "Column _34 has 4 unique categorical values\n",
      "Column _35 has 38618 unique categorical values\n",
      "Column _36 has 11 unique categorical values\n",
      "Column _37 has 14 unique categorical values\n",
      "Column _38 has 12335 unique categorical values\n",
      "Column _39 has 51 unique categorical values\n",
      "Column _40 has 9527 unique categorical values\n",
      "Total number of distinct categorical variables in train_sample is: 241338\n"
     ]
    }
   ],
   "source": [
    "# number of unique categorical values\n",
    "from pyspark.sql.functions import col\n",
    "distCatVarCnt = 0\n",
    "for col in train_sample.columns[14:]:\n",
    "    cnt = train_sample.select(col).distinct().count()\n",
    "    print('Column ' + col + ' has ' + str(cnt) \\\n",
    "          + ' unique categorical values')\n",
    "    distCatVarCnt += cnt\n",
    "print(\"Total number of distinct categorical variables in train_sample is:\", distCatVarCnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the number of unique categorical variables is close to 3 million just for the train_sample dataset. We need to look into reducing the dimensionality without losing too much of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Per column frequency based dimensionality reduction\n",
    "\n",
    "One possible methold is to figure out if there are any frequent values in these categorical variables and choose the top 15 of them. If they make up nearly 100% of the values, then we can lump the rest under \"Other\" and come up with 16 column bins. This is a compute intensive operation as we try to figure out the top 15 values for each column and their contribution to the total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_rdd = sc.textFile('data/sample_training.txt')\n",
    "testRdd = sc.parallelize(train_sample_rdd.take(100000))\n",
    "\n",
    "def getTop15FreqCols (rdd):\n",
    "    \n",
    "    #First get a count of rdd length\n",
    "    rddLen = rdd.count()\n",
    "\n",
    "    #Create pandas dataframe to store the top 15 values\n",
    "    top15df = pd.DataFrame(columns=['col', 'top15_values', 'top15_pct_contribution'])\n",
    "\n",
    "    for col in range(14,40):\n",
    "        catRdd = rdd.map(lambda x: x.split('\\t')[col]) \\\n",
    "                    .map(lambda x: (x,1)) \\\n",
    "                    .reduceByKey(lambda x,y: x + y)\n",
    "    \n",
    "        freqRecord = catRdd.takeOrdered(15, key=lambda x: -x[1])\n",
    "        freqList = []\n",
    "        freqCnt = 0\n",
    "        for (k,v) in freqRecord:\n",
    "            freqCnt += v\n",
    "            freqList.append(str(k))\n",
    "        top15df = top15df.append({'col': col, 'top15_values': freqList, 'top15_pct_contribution': 100*freqCnt/rddLen}, ignore_index=True)\n",
    "    \n",
    "    return(top15df)\n",
    "\n",
    "top15df = getTop15FreqCols(train_sample_rdd)\n",
    "\n",
    "#top15df = top15df.set_index('col')\n",
    "    \n",
    "print(top15df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, we can see that the categorical variables in some of the columns account for almost 100% of all values. These include columns 14, 18, 19, 21, 22, 27, 30, 32, 33, 35, 36 and 38. For these 11 columns, it would make sense to keep the top 15 values and lump everything else under an \"Other\" column.\n",
    "\n",
    "For the rest of the columns that don't exhibit this behavior, it might make sense to look at other strategies such as hashing to reduce dimensionality. Before we take this approach of using both the frequency related information and hashing, let's first take a look at hashing all columns next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Per column Feature Hashing\n",
    "There are 26 categorical features in this model represented by strings that are 8 bytes long. So, theoretically, each string can take $2^{64} -1$ different values and lead to that many dimensions. We need to have far fewer dimensions so that we can make the problem computationally achievable and as well lead to a generalized algorithm as well. One way of achieving this is through what is popularly called the \"hashing trick\" (provide references). \n",
    "\n",
    "A simple way to reduce dimensionality is to hash the 8 byte long strings into, say 16 or 32 groups. We used the murmurHash3 hashing which is generally the preferred way of hashing strings (provide references and more details). Hashing leads to collisions as many strings could end up hashing to the same hash value. However, it has been proven (references) that even with collisions, hashing leads to very generalized models.\n",
    "\n",
    "One the categorical variables are hashed down to, say 16 values, they are then 1-hot encoded and fed into the logistic regression model. This section presents the results with the inclusion of 27 categorical variables, each individually hashed to 16 values, in addition to the numerical variables in the earlier section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install murmurhash3 if needed\n",
    "!pip install murmurhash3\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the complete dataset\n",
    "train_sample_rdd = sc.textFile('data/sample_training.txt')\n",
    "\n",
    "#Get the top 1000 rows only (as before for numerical variables)\n",
    "testRdd = sc.parallelize(train_sample_rdd.take(10000),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashTrans (rdd):\n",
    "    \n",
    "\n",
    "    def createHash (elem,hlen):\n",
    "        import mmh3\n",
    "        hashStr = []\n",
    "        for str in elem:\n",
    "            hashStr.append(mmh3.hash(str) % int(hlen))\n",
    "        return(hashStr)\n",
    "\n",
    "    def create1Hot(elem, hlen):\n",
    "        oneHotStr = []\n",
    "        #for hashStr in elem:\n",
    "        for i in range (hlen):\n",
    "            if (i == elem):\n",
    "                oneHotStr.append(1)\n",
    "            else:\n",
    "                oneHotStr.append(0)\n",
    "        return(oneHotStr)\n",
    "\n",
    "    def createCatArray(elem):\n",
    "        catArray = []\n",
    "        for array in elem:\n",
    "            for x in array:\n",
    "                catArray.append(x)\n",
    "        return(np.array(catArray))\n",
    "\n",
    "    #Define murmurHash level for 1-hot encoding\n",
    "    HASHLEN = 16\n",
    "\n",
    "    #testRdd.map(lambda x : x.split('\\t')[14:40]).map(lambda x: [mmh3.hash(xn)%16 for xn in x]).take(5)\n",
    "    categoricalRdd = rdd.map(lambda x : x.split('\\t')[14:40]) \\\n",
    "                        .map(lambda x: createHash(x,HASHLEN)) \\\n",
    "                        .map(lambda x: [create1Hot(xn, HASHLEN) for xn in x]) \\\n",
    "                        .map(createCatArray)\n",
    "    return(categoricalRdd)\n",
    "\n",
    "categoricalRdd = hashTrans(testRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now include this into the logistic regression model\n",
    "#Zip numerical and categorical variables\n",
    "\n",
    "def mergeNumPlusCatRdds(elem):\n",
    "    x, y = elem\n",
    "    xkey , xval = x\n",
    "    merge =  (xkey, np.hstack((xval,y)))\n",
    "    return(merge)\n",
    "\n",
    "numPlusCatRdd = normedRDD.zip(categoricalRdd) \\\n",
    "                         .map(mergeNumPlusCatRdds)\n",
    "numPlusCatRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of features\n",
    "numb_features = np.size(numPlusCatRdd.first()[1])\n",
    "print(numb_features)\n",
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train, validation and test sets\n",
    "train, validation, test = numPlusCatRdd.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nSteps = 150\n",
    "regType = 'ridge'\n",
    "regParam = 0.1\n",
    "learningRate = 0.1\n",
    "\n",
    "# run gradient descent\n",
    "train_loss, test_loss, model = GDUpdate(train, validation, BASELINE, nSteps, regType=regType, \n",
    "                                        regParam=regParam, learningRate=learningRate, verbose = False)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.765871000507872\n",
      "Precision is:  0.4\n",
      "Recall is:  0.004347826086956522\n",
      "F1 score is:  0.008602150537634409\n",
      "False positive rate is:  0.0019880715705765406\n",
      "True positive rate is:  0.004347826086956522\n"
     ]
    }
   ],
   "source": [
    "# make predictions and compute metrics for treshProb = 0.5\n",
    "pred, ntp, ntn, nfp, nfn = makePrediction(validation, model[-1], 0.5)\n",
    "\n",
    "acc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "wInit, wInit1, wInit0 = wInitialization(train, 2)\n",
    "#print(wInit)\n",
    "logerr_train, logerr_test, models_cat, model1s_cat, model0s_cat = GradientDescent(train, validation, wInit,wInit1, wInit0, nSteps = 100,\n",
    "                                                    learningRate = 0.001, regParam = 0.01, regParam1 = 0.01, regParam0 = 0.01, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"The estimated model is: {models[-1]}\")\n",
    "#print(f\"The loss of the estimated model is: {logerr_train[-1]}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(logerr_train)\n",
    "plt.plot(logerr_test)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, ntp, ntn, nfp, nfn = fmMakePrediction(validation, models[-1], model1s[-1], model0s[-1])\n",
    "\n",
    "cc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Multi-column Feature Hashing\n",
    "Hashing each column independently resulted in a dimensionality reduction from several million vectors to 416 for the categorical variables. We could further reduce dimensionality through multi-column Feature Hashing with a likely tradeoff being the loss of accuracy (TBD). \n",
    "\n",
    "We used the multi-column FeatureHashing functionality in Apache Spark MLLib to look into how dimensionality reduction to fewer vectors for logistic regression. Multi-column FeatureHashing would be very handy when we start looking at adding in quadratic terms for logistic regression. Instead of a (416,416) quadratic feature matrix, we could deal with a smaller (32,32) or (64,64) features, leading to faster computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark MLLib FeatureHasher example\n",
    "The FeatureHasher in spark takes multiple columns of categorical (and numerical) variables and hashes them down to fewer features. It is possible to specify the number of output features so that we can restrict the dimensions to a more manageable number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature hashing example\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "dataset = spark.createDataFrame([\n",
    "    (2.2, True, \"1\", \"foo\"),\n",
    "    (3.3, False, \"2\", \"bar\"),\n",
    "    (4.4, False, \"3\", \"baz\"),\n",
    "    (5.5, False, \"4\", \"foo\")\n",
    "], [\"real\", \"bool\", \"stringNum\", \"string\"])\n",
    "\n",
    "hasher = FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"],\n",
    "                       outputCol=\"features\")\n",
    "\n",
    "featurized = hasher.transform(dataset)\n",
    "featurized.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-column Feature Hashing with test set\n",
    "We applied Multi-column Feature Hashing to get to a dimensionality of 64 vectors using FeatureHasher. The results of the logistic regression following this hashing are given below (need to add time, accuracy etc...How do you compare otherwise?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sample training data and convert to dataframe\n",
    "train_sample1 = sc.textFile('data/sample_training.txt')\\\n",
    "                 .map(lambda x: x.split('\\t'))\\\n",
    "                 .toDF().limit(10000).cache()\n",
    "train_sample1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose categorical columns to Hash\n",
    "hashInpList = []\n",
    "for c in range(14,41):\n",
    "    col = \"_\"+str(c)\n",
    "    hashInpList.append(col)\n",
    "print (hashInpList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Multi-column hashing with 32 output features\n",
    "hasher = FeatureHasher(numFeatures=256, inputCols=hashInpList,outputCol=\"features\")\n",
    "\n",
    "featurized = hasher.transform(train_sample1)\n",
    "featurized.select(\"features\").show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized.show(3)\n",
    "featurized.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into a dense vector format to feed logistic regression model\n",
    "def extractVec (elem):\n",
    "    return(np.array(tuple(elem.features.toArray().tolist())))\n",
    "    \n",
    "multiHashCatRdd = featurized.select(\"features\").rdd.map(extractVec)\n",
    "#map(extractVec)\n",
    "multiHashCatRdd.take(3)\n",
    "multiHashCatRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now include this into the logistic regression model\n",
    "#Zip numerical and categorical variables\n",
    "\n",
    "def mergeNumPlusCatRdds(elem):\n",
    "    x, y = elem\n",
    "    xkey , xval = x\n",
    "    merge =  (xkey, np.hstack((xval,y)))\n",
    "    return(merge)\n",
    "\n",
    "numPlusCatRdd2 = normedRDD.zip(multiHashCatRdd) \\\n",
    "                         .map(mergeNumPlusCatRdds)\n",
    "numPlusCatRdd2.take(1)\n",
    "numPlusCatRdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of features\n",
    "numb_features = np.size(numPlusCatRdd2.first()[1])\n",
    "print(numb_features)\n",
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train, validation and test sets\n",
    "train, validation, test = numPlusCatRdd2.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nSteps = 150\n",
    "regType = 'ridge'\n",
    "regParam = 0.1\n",
    "learningRate = 0.1\n",
    "\n",
    "# run gradient descent\n",
    "train_loss, test_loss, model = GDUpdate(train, validation, BASELINE, nSteps, regType=regType, \n",
    "                                        regParam=regParam, learningRate=learningRate, verbose = False)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.8019559902200489\n",
      "Precision is:  0.5\n",
      "Recall is:  0.009876543209876543\n",
      "F1 score is:  0.01937046004842615\n",
      "False positive rate is:  0.0024390243902439024\n",
      "True positive rate is:  0.009876543209876543\n"
     ]
    }
   ],
   "source": [
    "# make predictions and compute metrics for treshProb = 0.5\n",
    "pred, ntp, ntn, nfp, nfn = makePrediction(validation, model[-1], 0.5)\n",
    "\n",
    "acc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "wInit, wInit1, wInit0 = wInitialization(train, 2)\n",
    "#print(wInit)\n",
    "logerr_train, logerr_test, models_cat2, model1s_cat2, model0s_cat2 = GradientDescent(train, validation, wInit,wInit1, wInit0, nSteps = 100,\n",
    "                                                    learningRate = 0.001, regParam = 0.01, regParam1 = 0.01, regParam0 = 0.01, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The estimated model is: {models[-1]}\")\n",
    "print(f\"The loss of the estimated model is: {logerr_train[-1]}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(logerr_train)\n",
    "plt.plot(logerr_test)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, ntp, ntn, nfp, nfn = fmMakePrediction(validation, models[-1], model1s[-1], model0s[-1])\n",
    "\n",
    "cc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Combining Frequency and Hash Based Dimensionality Reduction\n",
    "For some categorical features, it makes sense to use a frequency based approach, while hashing is useful in other cases. 12 of the columns were amenable to frequency based approaches since 90% or more of the values were accounted for by the top 15 frequent values. The hybrid approach uses frequencies to encode these columns, while using the hash only for the other 14 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hybridFreqHashTrans (top15df, rdd):\n",
    "\n",
    "    #First create Broadcast variable\n",
    "    top15dfB = sc.broadcast(top15df)\n",
    "    \n",
    "    #set FreqThr at 90% for using frequency to bin the categories\n",
    "    TOP15_FREQ_THR = 90\n",
    "\n",
    "    #Define murmurHash level for 1-hot encoding\n",
    "    HASHLEN = 16\n",
    "    \n",
    "    #top15df = pd.DataFrame(columns=['col', 'top15_values', 'top15_pct_contribution'])\n",
    "    \n",
    "    def transformRow (row, top15df, hlen):\n",
    "        col = 0\n",
    "        transformedRow = []\n",
    "        \n",
    "        for x in row:\n",
    "            #Get frequency based map\n",
    "            if (top15df.loc[col]['top15_pct_contribution'] > 90):\n",
    "                xtrans = 15\n",
    "                for (ind,topElem) in enumerate(top15df.loc[col]['top15_values']):\n",
    "                    if (x == topElem):\n",
    "                        xtrans = ind\n",
    "            #else hash\n",
    "            else:\n",
    "                xtrans = mmh3.hash(x+str(col)) % int(hlen)\n",
    "        \n",
    "            transformedRow.append(xtrans)\n",
    "            \n",
    "            #Do this for all elements\n",
    "            col += 1\n",
    "        return (transformedRow)\n",
    "\n",
    "    def create1Hot(elem, hlen):\n",
    "        oneHotStr = []\n",
    "        #for hashStr in elem:\n",
    "        for i in range (hlen):\n",
    "            if (i == elem):\n",
    "                oneHotStr.append(1)\n",
    "            else:\n",
    "                oneHotStr.append(0)\n",
    "        return(oneHotStr)\n",
    "\n",
    "    def createCatArray(elem):\n",
    "        catArray = []\n",
    "        for array in elem:\n",
    "            for x in array:\n",
    "                catArray.append(x)\n",
    "        return(np.array(catArray))\n",
    "\n",
    "\n",
    "\n",
    "    categoricalRdd = rdd.map(lambda x : x.split('\\t')[14:40]) \\\n",
    "                        .map(lambda x: transformRow(x, top15dfB.value, HASHLEN)) \\\n",
    "                        .map(lambda x: [create1Hot(xn, HASHLEN) for xn in x]) \\\n",
    "                        .map(createCatArray)\n",
    "    return(categoricalRdd)\n",
    "\n",
    "#Read in the complete dataset\n",
    "train_sample_rdd = sc.textFile('data/sample_training.txt')\n",
    "\n",
    "#Get the top 1000 rows only (as before for numerical variables)\n",
    "testRdd = sc.parallelize(train_sample_rdd.take(10000),1)\n",
    "\n",
    "hybridCatRdd = hybridFreqHashTrans(top15df, testRdd)\n",
    "\n",
    "hybridCatRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, array([-0.45879823, -0.24774127, -0.04629014, -0.85019922, -0.24482546,\n",
       "         -0.40934022, -0.02096037, -0.63143545,  0.18286248,  0.7486028 ,\n",
       "         -0.10449796,  0.01324891, -0.49600354,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ]))]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now include this into the logistic regression model\n",
    "#Zip numerical and categorical variables\n",
    "\n",
    "def mergeNumPlusCatRdds(elem):\n",
    "    x, y = elem\n",
    "    xkey , xval = x\n",
    "    merge =  (xkey, np.hstack((xval,y)))\n",
    "    return(merge)\n",
    "\n",
    "numPlusCatRdd3 = normedRDD.zip(hybridCatRdd) \\\n",
    "                         .map(mergeNumPlusCatRdds)\n",
    "numPlusCatRdd3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n"
     ]
    }
   ],
   "source": [
    "#compute the number of features\n",
    "numb_features = np.size(numPlusCatRdd3.first()[1])\n",
    "print(numb_features)\n",
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train, validation and test sets\n",
    "train, validation, test = numPlusCatRdd3.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nSteps = 150\n",
    "regType = 'ridge'\n",
    "regParam = 0.1\n",
    "learningRate = 0.1\n",
    "\n",
    "# run gradient descent\n",
    "train_loss, test_loss, model = GDUpdate(train, validation, BASELINE, nSteps, regType=regType, \n",
    "                                        regParam=regParam, learningRate=learningRate, verbose = False)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.7901541521631029\n",
      "Precision is:  0.47619047619047616\n",
      "Recall is:  0.023752969121140142\n",
      "F1 score is:  0.04524886877828054\n",
      "False positive rate is:  0.006918238993710692\n",
      "True positive rate is:  0.023752969121140142\n"
     ]
    }
   ],
   "source": [
    "# make predictions and compute metrics for treshProb = 0.5\n",
    "pred, ntp, ntn, nfp, nfn = makePrediction(validation, model[-1], 0.5)\n",
    "\n",
    "acc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "wInit, wInit1, wInit0 = wInitialization(train, 2)\n",
    "logerr_train, logerr_test, models, model1s, model0s = GradientDescent(train, validation, wInit, wInit1, wInit0, nSteps = 100,\n",
    "                                                    learningRate = 0.002, regParam = 0.01, regParam1 = 0.01, regParam0 = 0.01, verbose = True)\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The estimated model is: {models[-1]}\")\n",
    "print(f\"The loss of the estimated model is: {logerr_train[-1]}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(logerr_train)\n",
    "plt.plot(logerr_test)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, ntp, ntn, nfp, nfn = fmMakePrediction(validation, models[-1], model1s[-1], model0s[-1])\n",
    "\n",
    "cc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Shall we put a conclusion here in combination with course concepts? This can cover our submission to Kaggle results]** \n",
    "\n",
    "In this assignment, we were provided with a training data set from Criteo on Kaggle that consisted of numerical and categorical features that described particular ads that were provided to an audience, thus resulting in either a successful click-through or not. Our goal was to construct a predictive model that would predict a final binary outcome (1 for successful click through, 0 for unsuccessful click through) given new data consisting of the same numerical and categorical features. \n",
    "\n",
    "Such a challenge certainly is not new, but the sheer scale of such a data set (particularly the high dimensionality of the categorical columns) definitely warranted a second look at taking advantage of the map-reduce framework behind Spark as well as optimizations through statistical observations on value frequency as well as pipelining processes to preprocess features, train and cross-validate our hyperparameters, and finally test our model.\n",
    "\n",
    "Logistic regression remains a trusty binary predictor; under supervised machine learning, we applied the distributed framework under map-reduce in Spark to scale gradient descent for efficiency and performance on such a large dataset. Furthermore, we learned to take advantage of Spark's optimization for Big Data--particularly DataFrames, FeatureHash, and Pipeline--to productionalize our model [**TO DO: double-check here where FFM fits]**\n",
    "\n",
    "============================================\n",
    "\n",
    "(Scratch notes for now; listing out points to mention for each core concept we covered) \n",
    "\n",
    "* Parallel Computation, Map Reduce Framework with Spark RDDs \n",
    "* Distributed Supervised ML \n",
    "    * Logistic Regression\n",
    "* Spark Optimizations for Big Data and DataFrames\n",
    "    * Spark MLLib Feature Hasher \n",
    "    * Pipeline \n",
    "* ALS and Spark MLLib\n",
    "    * Factorization Machines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOY EXAMPLE - comparison with ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOY EXAMPLE STARTS HERE\n",
    "numb_features = 2\n",
    "\n",
    "N = 100\n",
    "D = 3\n",
    "\n",
    "X_toy = np.random.randn(N,D)\n",
    "\n",
    "# center the first 50 points at (-1,-1)\n",
    "X_toy[:50,:] = X_toy[:50,:] - 1*np.ones((50,D))\n",
    "\n",
    "# center the last 50 points at (2, 2)\n",
    "X_toy[50:,:] = X_toy[50:,:] + 2*np.ones((50,D))\n",
    "\n",
    "X_toy[:50,0] = 0\n",
    "X_toy[50:,0] = 1\n",
    "\n",
    "rdd1 = sc.parallelize(X_toy)\n",
    "rdd1 = rdd1.map(lambda x: [float(i) for i in x])\n",
    "toy_sample_red = rdd1.toDF([\"_1\", \"_2\", \"_3\"])\n",
    "toy_sample_red_RDD = toy_sample_red.rdd.map(lambda x: (x[0], x[1:])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define baseline model, add one parameter representing the intercept\n",
    "BASELINE = np.random.randn(numb_features + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSteps = 150\n",
    "learningRate = 0.5\n",
    "regType = 'ridge'\n",
    "regParam = 0.05\n",
    "\n",
    "Loss_save = []\n",
    "Model_norm = []\n",
    "#broadcast model\n",
    "model = BASELINE\n",
    "for idx in range(nSteps):\n",
    "    #print(\"----------\")\n",
    "    #print(f\"STEP: {idx+1}\")\n",
    "    \n",
    "    # compute loss\n",
    "    loss = LogLoss(toy_sample_red_RDD, model, regType=regType, regParam=regParam)\n",
    "    # update model parameters\n",
    "    model = GDUpdate(toy_sample_red_RDD, model, regType=regType, regParam=regParam, learningRate=learningRate)\n",
    "    \n",
    "    #store results\n",
    "    Loss_save.append(loss)\n",
    "    Model_norm.append(np.linalg.norm(model))\n",
    "\n",
    "print(f\"The estimated model is: {model}\")\n",
    "print(f\"The loss of the estimated model is: {loss}\")\n",
    "print()\n",
    "print()\n",
    "plt.plot(Loss_save)\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "plt.plot(Model_norm)\n",
    "plt.title('Norm of vector of parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual simulated value is 50\n",
    "res = makePrediction(toy_sample_red_RDD, model).cache()\n",
    "res.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = X_toy[:,1:],X_toy[:,0]\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',fit_intercept=True).fit(X, y)\n",
    "print(clf.coef_, clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_toy[:,1], X_toy[:,2], c=X_toy[:,0], s=100, alpha=0.5)\n",
    "x_axis = np.linspace(-6, 6, 100)\n",
    "y_axis = -(model[0] + x_axis*model[1]) / model[2]\n",
    "plt.plot(x_axis, y_axis)\n",
    "y_axis = -(clf.intercept_+x_axis*clf.coef_[0][0]) / clf.coef_[0][1]\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_cols = ['_1','_2','_3','_4','_5','_6','_7','_8','_9','_10','_11','_12','_13','_14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate train data for homegrown solution - select only 1000 rows and only numerical features + one categorical variable +target \n",
    "train_sample_red = train_sample.select(convert_cols + ['_23']).limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_red.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values with averages\n",
    "from pyspark.sql.functions import avg\n",
    "for col in convert_cols:\n",
    "    train_sample_red = train_sample_red.na.fill(round(train_sample_red.na.drop().agg(avg(col)).first()[0],1), [col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "df = train_sample_red.withColumn(\"_23\", split(col(\"_23\"),\" \"))\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cat_Vectorizer = CountVectorizer(inputCol=\"_23\", outputCol=\"_23_array\", vocabSize=4, minDF=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catVectorizer_model = cat_Vectorizer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe = catVectorizer_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StringIndexer(inputCols=[\"gender\"], outputCols=[\"gender_numeric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cardinality of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique categorical values\n",
    "from pyspark.sql.functions import col\n",
    "for col in train_sample.columns[14:]:\n",
    "    print('Column ' + col + ' has ' + str(train_sample.select(col).distinct().count()) \\\n",
    "          + ' unique categorical values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plots of selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of target variable\n",
    "hist_c1 = train_sample.select('_1').rdd.flatMap(lambda x: x).histogram(2)\n",
    "pd.DataFrame(list(zip(*hist_c1))).set_index(0).plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 20\n",
    "hist_c20 = train_sample.groupBy('_20').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c20))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 23\n",
    "hist_c23 = train_sample.groupBy('_23').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c23))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 28\n",
    "hist_c28 = train_sample.groupBy('_28').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c28))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 31\n",
    "hist_c31 = train_sample.groupBy('_31').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c31))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 34\n",
    "hist_c34 = train_sample.groupBy('_34').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c34))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of column 36\n",
    "hist_c36 = train_sample.groupBy('_36').count().collect()\n",
    "pd.DataFrame(list(zip(*hist_c36))).T.set_index(0).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimates of ctr based on category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 20\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_20').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_20').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 23\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_23').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_23').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 28\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_28').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_28').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 31\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_31').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_31').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 34\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_34').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_34').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column 36\n",
    "list_labls_vals = list(zip(*train_sample.groupBy('_36').sum('_1').collect()))\n",
    "labls = list_labls_vals[0]\n",
    "vals = np.array(list_labls_vals[1])/np.array(list(zip(*train_sample.groupBy('_36').count().collect()))[1])\n",
    "\n",
    "ax = pd.DataFrame(vals).plot(kind='bar')\n",
    "ax.set_xticklabels(labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
