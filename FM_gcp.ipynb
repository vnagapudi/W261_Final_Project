{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 5 - supplemental notebook\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "This notebook contains supplemental materials to help you run your HW5 solution to question 8 using Google Compute Platform. __Important Note:__ _the graders will not read this notebook. If you do use it, please be sure to copy any solutions back into the main homework notebook to receive credit for your results._ \n",
    "\n",
    "### Instructions\n",
    "1. Create your GCP account & apply for credit through the w261 education grant. (see [create_account.md](https://github.com/UCB-w261/Instructors/blob/master/GCP/create_account.md))\n",
    "2. Set up your project, bucket, service account, access key and virtual environment. (steps 1-15 in [setup.md](https://github.com/UCB-w261/Instructors/blob/master/GCP/setup.md))\n",
    "3. (OPTIONAL) Review the GCP documentation to become more familiar with the setup steps you've just performed: [key terms & concepts described here](https://cloud.google.com/storage/docs/concepts) \n",
    "4. Run this notebook on your local machine using the virtual environment you created in step 2. (_Note that you may have to install jupyter in that environment in order to do so) _\n",
    "5. Follow the example below to learn how you can write a spark script & submit it to be run on a cluster all from your Jupyter Notebook.\n",
    "6. Turn your HW5 Q8 solution into a script that follows the example provided.\n",
    "7. Copy all the code to write this^ script & run the job back into the HW notebook where it will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "PROJECT_ID = 'w262-245821' # fill in your GCP project id\n",
    "BUCKET_NAME = 'w261_sj_data' # fill in the name of your GCP bucket\n",
    "CLUSTER_NAME = 'w261-sj' # choose a cluster name, this should include only a-z, 0-9 & start with a letter\n",
    "HOME = '/Users/sid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Script\n",
    "The cell below will create a python script in the current working directory called `submit_job_to_cluster.py` -- this script will help you run your own spark jobs on the cluster. You can read more about it in the [w261-environment](https://github.com/UCB-w261/w261-environment/tree/master/gcp-files/dataproc) repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submit_job_to_cluster.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submit_job_to_cluster.py\n",
    "#!/usr/bin/env python\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\" Sample command-line program for listing Google Dataproc Clusters\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "import googleapiclient.discovery\n",
    "\n",
    "DEFAULT_FILENAME = 'pyspark_sort.py'\n",
    "\n",
    "\n",
    "def get_default_pyspark_file():\n",
    "    \"\"\"Gets the PySpark file from this directory\"\"\"\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    f = open(os.path.join(current_dir, DEFAULT_FILENAME), 'rb')\n",
    "    return f, DEFAULT_FILENAME\n",
    "\n",
    "\n",
    "def get_pyspark_file(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    return f, os.path.basename(filename)\n",
    "\n",
    "\n",
    "def upload_pyspark_file(project_id, bucket_name, filename, file):\n",
    "    \"\"\"Uploads the PySpark file in this directory to the configured\n",
    "    input bucket.\"\"\"\n",
    "    print('Uploading pyspark file to GCS')\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(filename)\n",
    "    blob.upload_from_file(file)\n",
    "\n",
    "\n",
    "def download_output(project_id, cluster_id, output_bucket, job_id):\n",
    "    \"\"\"Downloads the output file from Cloud Storage and returns it as a\n",
    "    string.\"\"\"\n",
    "    print('Downloading output file')\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(output_bucket)\n",
    "    output_blob = (\n",
    "        'google-cloud-dataproc-metainfo/{}/jobs/{}/driveroutput.000000000'\n",
    "        .format(cluster_id, job_id))\n",
    "    return bucket.blob(output_blob).download_as_string()\n",
    "\n",
    "\n",
    "# [START create_cluster]\n",
    "def create_cluster(dataproc, project, zone, region, cluster_name,\n",
    "                   instance_type, master_nodes, worker_nodes):\n",
    "    print('Creating cluster...')\n",
    "    zone_uri = \\\n",
    "        'https://www.googleapis.com/compute/v1/projects/{}/zones/{}'.format(\n",
    "            project, zone)\n",
    "    cluster_data = {\n",
    "        'projectId': project,\n",
    "        'clusterName': cluster_name,\n",
    "        'config': {\n",
    "            'gceClusterConfig': {\n",
    "                'zoneUri': zone_uri,\n",
    "                \"metadata\": {\n",
    "                    \"CONDA_PACKAGES\": \"\\\"numpy pandas\\\"\",\n",
    "                    \"MINICONDA_VARIANT\": \"2\"\n",
    "                }\n",
    "            },\n",
    "            \"softwareConfig\": {\n",
    "                'properties': {\n",
    "                    'spark:spark.jars.packages': 'com.databricks:spark-xml_2.11:0.4.1,graphframes:graphframes:0.5.0-spark2.1-s_2.11,com.databricks:spark-avro_2.11:4.0.0'\n",
    "                }\n",
    "            },\n",
    "            'masterConfig': {\n",
    "                'numInstances': master_nodes,\n",
    "                'machineTypeUri': instance_type\n",
    "            },\n",
    "            'workerConfig': {\n",
    "                'numInstances': worker_nodes,\n",
    "                'machineTypeUri': instance_type\n",
    "            },\n",
    "            'secondaryWorkerConfig': {\n",
    "                'numInstances': \"2\",\n",
    "                'machineTypeUri': instance_type,\n",
    "                \"isPreemptible\": \"True\"\n",
    "            },\n",
    "            \"initializationActions\": [\n",
    "                {\n",
    "                    \"executableFile\": \"gs://dataproc-initialization-actions/conda/bootstrap-conda.sh\"\n",
    "                },\n",
    "                {\n",
    "                    \"executableFile\": \"gs://dataproc-initialization-actions/conda/install-conda-env.sh\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    result = dataproc.projects().regions().clusters().create(\n",
    "        projectId=project,\n",
    "        region=region,\n",
    "        body=cluster_data).execute()\n",
    "    return result\n",
    "# [END create_cluster]\n",
    "\n",
    "\n",
    "def wait_for_cluster_creation(dataproc, project_id, region, cluster_name):\n",
    "    print('Waiting for cluster creation...')\n",
    "\n",
    "    while True:\n",
    "        result = dataproc.projects().regions().clusters().list(\n",
    "            projectId=project_id,\n",
    "            region=region).execute()\n",
    "        cluster_list = result['clusters']\n",
    "        cluster = [c\n",
    "                   for c in cluster_list\n",
    "                   if c['clusterName'] == cluster_name][0]\n",
    "        if cluster['status']['state'] == 'ERROR':\n",
    "            raise Exception(result['status']['details'])\n",
    "        if cluster['status']['state'] == 'RUNNING':\n",
    "            print(\"Cluster created.\")\n",
    "            break\n",
    "\n",
    "\n",
    "# [START list_clusters_with_detail]\n",
    "def list_clusters_with_details(dataproc, project, region):\n",
    "    result = dataproc.projects().regions().clusters().list(\n",
    "        projectId=project,\n",
    "        region=region).execute()\n",
    "    cluster_list = result['clusters']\n",
    "    for cluster in cluster_list:\n",
    "        print(\"{} - {}\"\n",
    "              .format(cluster['clusterName'], cluster['status']['state']))\n",
    "    return result\n",
    "# [END list_clusters_with_detail]\n",
    "\n",
    "\n",
    "def get_cluster_id_by_name(cluster_list, cluster_name):\n",
    "    \"\"\"Helper function to retrieve the ID and output bucket of a cluster by\n",
    "    name.\"\"\"\n",
    "    cluster = [c for c in cluster_list if c['clusterName'] == cluster_name][0]\n",
    "    return cluster['clusterUuid'], cluster['config']['configBucket']\n",
    "\n",
    "\n",
    "# [START submit_pyspark_job]\n",
    "def submit_pyspark_job(dataproc, project, region,\n",
    "                       cluster_name, bucket_name, filename):\n",
    "    \"\"\"Submits the Pyspark job to the cluster, assuming `filename` has\n",
    "    already been uploaded to `bucket_name`\"\"\"\n",
    "    job_details = {\n",
    "        'projectId': project,\n",
    "        'job': {\n",
    "            'placement': {\n",
    "                'clusterName': cluster_name\n",
    "            },\n",
    "            'pysparkJob': {\n",
    "                'mainPythonFileUri': 'gs://{}/{}'.format(bucket_name, filename)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    result = dataproc.projects().regions().jobs().submit(\n",
    "        projectId=project,\n",
    "        region=region,\n",
    "        body=job_details).execute()\n",
    "    job_id = result['reference']['jobId']\n",
    "    print('Submitted job ID {}'.format(job_id))\n",
    "    return job_id\n",
    "# [END submit_pyspark_job]\n",
    "\n",
    "\n",
    "# [START delete]\n",
    "def delete_cluster(dataproc, project, region, cluster):\n",
    "    print('Tearing down cluster')\n",
    "    result = dataproc.projects().regions().clusters().delete(\n",
    "        projectId=project,\n",
    "        region=region,\n",
    "        clusterName=cluster).execute()\n",
    "    return result\n",
    "# [END delete]\n",
    "\n",
    "\n",
    "# [START wait]\n",
    "def wait_for_job(dataproc, project, region, job_id):\n",
    "    print('Waiting for job to finish...')\n",
    "    while True:\n",
    "        result = dataproc.projects().regions().jobs().get(\n",
    "            projectId=project,\n",
    "            region=region,\n",
    "            jobId=job_id).execute()\n",
    "        # Handle exceptions\n",
    "        if result['status']['state'] == 'ERROR':\n",
    "            raise Exception(result['status']['details'])\n",
    "        elif result['status']['state'] == 'DONE':\n",
    "            print('Job finished.')\n",
    "            return result\n",
    "# [END wait]\n",
    "\n",
    "\n",
    "# [START get_client]\n",
    "def get_client():\n",
    "    \"\"\"Builds an http client authenticated with the service account\n",
    "    credentials.\"\"\"\n",
    "    dataproc = googleapiclient.discovery.build('dataproc', 'v1')\n",
    "    return dataproc\n",
    "# [END get_client]\n",
    "\n",
    "\n",
    "def main(project_id, zone, cluster_name, bucket_name,\n",
    "         instance_type, master_nodes, worker_nodes,\n",
    "         pyspark_file=None, create_new_cluster=True):\n",
    "    dataproc = get_client()\n",
    "    region = 'global'\n",
    "    try:\n",
    "        if pyspark_file:\n",
    "            spark_file, spark_filename = get_pyspark_file(pyspark_file)\n",
    "        else:\n",
    "            spark_file, spark_filename = get_default_pyspark_file()\n",
    "\n",
    "        if create_new_cluster:\n",
    "            create_cluster(\n",
    "                dataproc, project_id, zone, region, cluster_name,\n",
    "                instance_type, master_nodes, worker_nodes)\n",
    "            wait_for_cluster_creation(\n",
    "                dataproc, project_id, region, cluster_name)\n",
    "\n",
    "        upload_pyspark_file(\n",
    "            project_id, bucket_name, spark_filename, spark_file)\n",
    "\n",
    "        cluster_list = list_clusters_with_details(\n",
    "            dataproc, project_id, region)['clusters']\n",
    "\n",
    "        (cluster_id, output_bucket) = (\n",
    "            get_cluster_id_by_name(cluster_list, cluster_name))\n",
    "\n",
    "        # [START call_submit_pyspark_job]\n",
    "        job_id = submit_pyspark_job(\n",
    "            dataproc, project_id, region,\n",
    "            cluster_name, bucket_name, spark_filename)\n",
    "        # [END call_submit_pyspark_job]\n",
    "        wait_for_job(dataproc, project_id, region, job_id)\n",
    "\n",
    "        output = download_output(project_id, cluster_id, output_bucket, job_id)\n",
    "        print('Received job output {}'.format(output))\n",
    "        return output\n",
    "    finally:\n",
    "        if create_new_cluster:\n",
    "            delete_cluster(dataproc, project_id, region, cluster_name)\n",
    "        spark_file.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=__doc__,\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "            '--project_id', \n",
    "            help='Project ID you want to access.',\n",
    "            required=True\n",
    "        ),\n",
    "    parser.add_argument(\n",
    "            '--zone',\n",
    "            help='Zone to create clusters in/connect to.',\n",
    "            required=True\n",
    "        ),\n",
    "    parser.add_argument(\n",
    "            '--cluster_name',\n",
    "            help='Name of the cluster to create/connect to',\n",
    "            required=True\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--gcs_bucket',\n",
    "            help='Bucket to upload Pyspark file to',\n",
    "            required=True\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--pyspark_file',\n",
    "            help='Pyspark filename. Defaults to pyspark_sort.py'\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--create_new_cluster',\n",
    "            action='store_true',\n",
    "            help='States if the cluster should be created'\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--key_file',\n",
    "            help='Location of your key file for service account'\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--instance_type',\n",
    "            help='Instance types used for this cluster',\n",
    "            default='n1-standard-8'\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--master_nodes',\n",
    "            help='Number of master nodes',\n",
    "            default=1\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--worker_nodes',\n",
    "            help='Number of worker nodes',\n",
    "            default=2\n",
    "        )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.key_file is not None:\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = args.key_file\n",
    "\n",
    "    main(\n",
    "        args.project_id, args.zone, args.cluster_name,\n",
    "        args.gcs_bucket, args.instance_type, args.master_nodes, args.worker_nodes,\n",
    "        args.pyspark_file, args.create_new_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod a+x submit_job_to_cluster.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: h.t. create and run a spark job on a cluster using GCP\n",
    "Run the cell below to create a file called `pyspark_sort.py` in the current directory. Then run the bash cell to submit this job to GCP & spin up a cluster. (__`Note:`__ _make sure you have all the global variables set up first including the name of the spark job if you change it._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FM.py\n",
    "#!/usr/bin/env python\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\" Sample pyspark script to be uploaded to Cloud Storage and run on\n",
    "Cloud Dataproc.\n",
    "\n",
    "Note this file is not intended to be run directly, but run inside a PySpark\n",
    "environment.\n",
    "\"\"\"\n",
    "\n",
    "# [START pyspark]\n",
    "# imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import time\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"final_project\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df_k = spark.read.parquet(\"train_features_dense\")\n",
    "testRDD = df_k.rdd.map(lambda x: (x[0], tuple(x[1])))\n",
    "\n",
    "def fmLoss(dataRDD, w, w1,w0) :\n",
    "    \"\"\"\n",
    "    Computes the logloss given the data and model W\n",
    "    dataRDD - array of features, label\n",
    "    \"\"\"\n",
    "    w_bc = sc.broadcast(w)\n",
    "    w1_bc = sc.broadcast(w1)\n",
    "    w0_bc = sc.broadcast(w0)\n",
    "    def probability_value(x,W,W1,W0): \n",
    "        xa = np.array([x])\n",
    "        V =  xa.dot(W)\n",
    "        V_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(V*V - V_square).sum() + xa.dot(W1.T) + W0\n",
    "        return 1.0/(1.0 + np.exp(-phi))\n",
    "    \n",
    "    loss = dataRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x:  (probability_value(x[1],w_bc.value, w1_bc.value, w0_bc.value), x[0])) \\\n",
    "        .map(lambda x: (1 - 1e-12, x[1]) if x[0] == 1 else ((1e-12, x[1]) if x[0] == 0  else (x[0],x[1]))) \\\n",
    "        .map(lambda x: -(x[1] * np.log(x[0]) + (1-x[1])*np.log(1-x[0]))).mean()\n",
    "    \n",
    "    \n",
    "    return float(loss)\n",
    "\n",
    "def fmGradUpdate_v1(dataRDD, w, w1, w0, alpha, regParam, regParam1, regParam0):\n",
    "    \"\"\"\n",
    "    Computes the gradient and updates the model\n",
    "    \"\"\"\n",
    "    \n",
    "    w_bc = sc.broadcast(w)\n",
    "    w1_bc = sc.broadcast(w1)\n",
    "    w0_bc = sc.broadcast(w0)\n",
    "    rp_bc = sc.broadcast(regParam)\n",
    "    rp1_bc = sc.broadcast(regParam1)\n",
    "    rp0_bc = sc.broadcast(regParam0)\n",
    "    \n",
    "    #Gradient for interaction term\n",
    "    \n",
    "    def row_grad(x, y, W, W1, W0, regParam, regParam1, regParam0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        expnyt = np.exp(y*phi) \n",
    "        grad_loss = (-y/(1+expnyt))*(xa.T.dot(xa).dot(W) - np.diag(np.square(x)).dot(W))\n",
    "        return 2*regParam*W + grad_loss\n",
    "    \n",
    "    #Gradient for Linear term\n",
    "    def row_grad1(x, y, W, W1, W0, regParam, regParam1, regParam0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        expnyt = np.exp(y*phi)\n",
    "        grad_loss1 = (-y/(1+expnyt))*xa\n",
    "        return 2*regParam1*W1 + grad_loss1\n",
    "    \n",
    "    #Gradient for bias term\n",
    "    def row_grad0(x, y, W, W1, W0, regParam, regParam1, regParam0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        expnyt = np.exp(y*phi)\n",
    "        grad_loss0 = (-y/(1+expnyt))*1\n",
    "        return 2*regParam0*W0 +grad_loss0\n",
    "    \n",
    "   \n",
    "    \n",
    "    batchRDD = dataRDD.sample(False, 0.00001, 2019)  \n",
    "    grad = batchRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x: (1, row_grad(x[1], x[0], w_bc.value, w1_bc.value, w0_bc.value, rp_bc.value,rp1_bc.value,rp0_bc.value))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    model = w - alpha * grad.values().collect()[0] \n",
    "    \n",
    "    grad1 = batchRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x: (1, row_grad1(x[1], x[0], w_bc.value, w1_bc.value, w0_bc.value, rp_bc.value,rp1_bc.value,rp0_bc.value))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    model1 = w1 - alpha * grad1.values().collect()[0]\n",
    "    \n",
    "    grad0 = batchRDD.map(lambda x: (x[0],x[1]) if x[0] == 1 else (-1, x[1])).map(lambda x: (1, row_grad0(x[1], x[0], w_bc.value, w1_bc.value, w0_bc.value, rp_bc.value,rp1_bc.value,rp0_bc.value))).reduceByKey(lambda x,y: np.add(x,y))\n",
    "    model0 = w0 - alpha * grad0.values().collect()[0]\n",
    "    \n",
    "    return model, model1 ,model0\n",
    "    \n",
    "def GradientDescent(trainRDD, testRDD, model, model1, model0, nSteps = 20, \n",
    "                    learningRate = 0.01, regParam = 0.01,regParam1 = 0.01,regParam0 = 0.01, verbose = False):\n",
    "    \"\"\"\n",
    "    Perform nSteps iterations of OLS gradient descent and \n",
    "    track loss on a test and train set. Return lists of\n",
    "    test/train loss and the models themselves.\n",
    "    \"\"\"\n",
    "    # initialize lists to track model performance\n",
    "    train_history, test_history, model_history, model1_history, model0_history = [], [], [], [], []\n",
    "    \n",
    "    # perform n updates & compute test and train loss after each\n",
    "    model = wInit\n",
    "    model1 = wInit1\n",
    "    model0 = wInit0\n",
    "    for idx in range(nSteps): \n",
    "        \n",
    "        ############## YOUR CODE HERE #############\n",
    "        \n",
    "        model, model1, model0 = fmGradUpdate_v1(trainRDD, model, model1, model0, learningRate, regParam, regParam1, regParam0)\n",
    "        if idx %10 == 0:\n",
    "            training_loss = fmLoss(trainRDD, model, model1, model0) \n",
    "            test_loss = fmLoss(testRDD, model, model1, model0) \n",
    "        ############## (END) YOUR CODE #############\n",
    "        # keep track of test/train loss for plotting\n",
    "        if idx % 10 == 0:\n",
    "            train_history.append(training_loss)\n",
    "            test_history.append(test_loss)\n",
    "        model_history.append(model)\n",
    "        model1_history.append(model1)\n",
    "        model0_history.append(model0)\n",
    "        \n",
    "        # console output if desired\n",
    "        if idx % 10 == 0:\n",
    "            if verbose:\n",
    "                print(\"----------\")\n",
    "                print(f\"STEP: {idx+1}\")\n",
    "                print(f\"training loss: {training_loss}\")\n",
    "                print(f\"test loss: {test_loss}\")\n",
    "            #print(f\"Model: {[k for k in model]}\")\n",
    "   \n",
    "    return train_history, test_history, model_history, model1_history, model0_history\n",
    "\n",
    "def wInitialization(dataRDD, factor):\n",
    "    nrFeat = len(dataRDD.first()[1])\n",
    "    np.random.seed(int(time.time())) \n",
    "    w =  np.random.ranf((nrFeat, factor))\n",
    "    w = w / np.sqrt((w*w).sum())\n",
    "    \n",
    "    w1 =  np.random.ranf(nrFeat)\n",
    "    w1 = w1 / np.sqrt((w1*w1).sum())\n",
    "    \n",
    "    w0 =  np.random.ranf(1)\n",
    "    \n",
    "    return w, w1, w0\n",
    "\n",
    "train, validation, test = testRDD.randomSplit([0.6, 0.2, 0.2])\n",
    "\n",
    "start = time.time()\n",
    "wInit, wInit1, wInit0 = wInitialization(train, 2)\n",
    "logerr_train, logerr_test, models, model1s, model0s = GradientDescent(train, validation, wInit, wInit1, wInit0, nSteps = 80,\n",
    "                                                    learningRate = 0.0003, regParam = 0.01, regParam1 = 0.01, regParam0 = 0.01, verbose = True)\n",
    "\n",
    "print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")\n",
    "\n",
    "def fmMakePrediction(dataRDD, w, w1, w0, threshold):\n",
    "    \"\"\"\n",
    "    Perform one regularized gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (y, features_array)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        pred - (rdd) predicted targets\n",
    "    \"\"\"\n",
    "    w_bc = sc.broadcast(w)\n",
    "    w1_bc = sc.broadcast(w1)\n",
    "    w0_bc = sc.broadcast(w0)\n",
    "    def predict_fm(x, W, W1, W0):\n",
    "        xa = np.array([x])\n",
    "        VX =  xa.dot(W)\n",
    "        VX_square = (xa*xa).dot(W*W)\n",
    "        phi = 0.5*(VX*VX - VX_square).sum() + xa.dot(W1.T) + W0\n",
    "        return 1.0/(1.0 + np.exp(-phi))\n",
    "    \n",
    "    # compute prediction\n",
    "    pred = dataRDD.map(lambda x: (int(predict_fm(x[1],w_bc.value, w1_bc.value, w0_bc.value)>threshold), x[0] ))\n",
    "    ntp = pred.map(lambda x: int((x[0]*x[1]) == 1)).sum()\n",
    "    ntn = pred.map(lambda x: int((x[0]+x[1]) == 0)).sum()\n",
    "    nfp = pred.map(lambda x: int((x[0] == 1) * (x[1] == 0))).sum()\n",
    "    nfn = pred.map(lambda x: int((x[0] == 0) * (x[1] == 1))).sum()\n",
    "   \n",
    "    return pred, ntp, ntn, nfp, nfn\n",
    "   \n",
    "    return pred\n",
    "\n",
    "pred, ntp, ntn, nfp, nfn = fmMakePrediction(validation, models[-1], model1s[-1], model0s[-1],0.65)\n",
    "\n",
    "acc = (ntp+ntn)/(ntp+ntn+nfp+nfn)\n",
    "prec = (ntp)/(ntp+nfp)\n",
    "rec = (ntp)/(ntp+nfn)\n",
    "f1 = 2*prec*rec/(prec+rec)\n",
    "fpr = nfp/(ntn+nfp)\n",
    "tpr = ntp/(ntp+nfn)\n",
    "print('Accuracy is: ', acc)\n",
    "print('Precision is: ', prec)\n",
    "print('Recall is: ', rec)\n",
    "print('F1 score is: ', f1)\n",
    "print('False positive rate is: ', fpr)\n",
    "print('True positive rate is: ', tpr)\n",
    "\n",
    "sc.parallelize(models).saveAsTextFile(\"gs://w261_sj_data/data/models\")\n",
    "sc.parallelize(model1s).saveAsTextFile(\"gs://w261_sj_data/data/model1s\")\n",
    "sc.parallelize(model0s).saveAsTextFile(\"gs://w261_sj_data/data/model0s\")\n",
    "\n",
    "# [End pyspark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster...\n",
      "Waiting for cluster creation...\n",
      "Cluster created.\n",
      "Uploading pyspark file to GCS\n",
      "w261-sj - RUNNING\n",
      "Submitted job ID 805c03f5-13b6-46c0-ac0a-e513f4fa719c\n",
      "Waiting for job to finish...\n"
     ]
    }
   ],
   "source": [
    "!python3 submit_job_to_cluster.py \\\n",
    "    --project_id='w262-245821' \\\n",
    "    --zone=us-central1-b \\\n",
    "    --cluster_name='w261-sj' \\\n",
    "    --gcs_bucket='w261_sj_data' \\\n",
    "    --key_file=$HOME/w261.json \\\n",
    "    --create_new_cluster \\\n",
    "    --pyspark_file=FM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "PROJECT_ID = 'w262-245821' # fill in your GCP project id\n",
    "BUCKET_NAME = 'w261_sj_data' # fill in the name of your GCP bucket\n",
    "CLUSTER_NAME = 'w261-sj' # choose a cluster name, this should include only a-z, 0-9 & start with a letter\n",
    "HOME = '/Users/sid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
