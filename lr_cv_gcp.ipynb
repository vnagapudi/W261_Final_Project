{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Cross Validation Script\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "PROJECT_ID = 'w262-245821' # fill in your GCP project id\n",
    "BUCKET_NAME = 'w261_sj_data' # fill in the name of your GCP bucket\n",
    "CLUSTER_NAME = 'w261-sj' # choose a cluster name, this should include only a-z, 0-9 & start with a letter\n",
    "HOME = '/Users/sid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Script\n",
    "The cell below will create a python script in the current working directory called `submit_job_to_cluster.py` -- this script will help you run your own spark jobs on the cluster. You can read more about it in the [w261-environment](https://github.com/UCB-w261/w261-environment/tree/master/gcp-files/dataproc) repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submit_job_to_cluster.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submit_job_to_cluster.py\n",
    "#!/usr/bin/env python\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\" Sample command-line program for listing Google Dataproc Clusters\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "import googleapiclient.discovery\n",
    "\n",
    "DEFAULT_FILENAME = 'pyspark_sort.py'\n",
    "\n",
    "\n",
    "def get_default_pyspark_file():\n",
    "    \"\"\"Gets the PySpark file from this directory\"\"\"\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    f = open(os.path.join(current_dir, DEFAULT_FILENAME), 'rb')\n",
    "    return f, DEFAULT_FILENAME\n",
    "\n",
    "\n",
    "def get_pyspark_file(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    return f, os.path.basename(filename)\n",
    "\n",
    "\n",
    "def upload_pyspark_file(project_id, bucket_name, filename, file):\n",
    "    \"\"\"Uploads the PySpark file in this directory to the configured\n",
    "    input bucket.\"\"\"\n",
    "    print('Uploading pyspark file to GCS')\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(filename)\n",
    "    blob.upload_from_file(file)\n",
    "\n",
    "\n",
    "def download_output(project_id, cluster_id, output_bucket, job_id):\n",
    "    \"\"\"Downloads the output file from Cloud Storage and returns it as a\n",
    "    string.\"\"\"\n",
    "    print('Downloading output file')\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(output_bucket)\n",
    "    output_blob = (\n",
    "        'google-cloud-dataproc-metainfo/{}/jobs/{}/driveroutput.000000000'\n",
    "        .format(cluster_id, job_id))\n",
    "    return bucket.blob(output_blob).download_as_string()\n",
    "\n",
    "\n",
    "# [START create_cluster]\n",
    "def create_cluster(dataproc, project, zone, region, cluster_name,\n",
    "                   instance_type, master_nodes, worker_nodes):\n",
    "    print('Creating cluster...')\n",
    "    zone_uri = \\\n",
    "        'https://www.googleapis.com/compute/v1/projects/{}/zones/{}'.format(\n",
    "            project, zone)\n",
    "    cluster_data = {\n",
    "        'projectId': project,\n",
    "        'clusterName': cluster_name,\n",
    "        'config': {\n",
    "            'gceClusterConfig': {\n",
    "                'zoneUri': zone_uri,\n",
    "                \"metadata\": {\n",
    "                    \"CONDA_PACKAGES\": \"\\\"numpy pandas\\\"\",\n",
    "                    \"MINICONDA_VARIANT\": \"2\"\n",
    "                }\n",
    "            },\n",
    "            \"softwareConfig\": {\n",
    "                'properties': {\n",
    "                    'spark:spark.jars.packages': 'com.databricks:spark-xml_2.11:0.4.1,graphframes:graphframes:0.5.0-spark2.1-s_2.11,com.databricks:spark-avro_2.11:4.0.0'\n",
    "                }\n",
    "            },\n",
    "            'masterConfig': {\n",
    "                'numInstances': master_nodes,\n",
    "                'machineTypeUri': instance_type\n",
    "            },\n",
    "            'workerConfig': {\n",
    "                'numInstances': worker_nodes,\n",
    "                'machineTypeUri': instance_type\n",
    "            },\n",
    "            'secondaryWorkerConfig': {\n",
    "                'numInstances': \"2\",\n",
    "                'machineTypeUri': instance_type,\n",
    "                \"isPreemptible\": \"True\"\n",
    "            },\n",
    "            \"initializationActions\": [\n",
    "                {\n",
    "                    \"executableFile\": \"gs://dataproc-initialization-actions/conda/bootstrap-conda.sh\"\n",
    "                },\n",
    "                {\n",
    "                    \"executableFile\": \"gs://dataproc-initialization-actions/conda/install-conda-env.sh\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    result = dataproc.projects().regions().clusters().create(\n",
    "        projectId=project,\n",
    "        region=region,\n",
    "        body=cluster_data).execute()\n",
    "    return result\n",
    "# [END create_cluster]\n",
    "\n",
    "\n",
    "def wait_for_cluster_creation(dataproc, project_id, region, cluster_name):\n",
    "    print('Waiting for cluster creation...')\n",
    "\n",
    "    while True:\n",
    "        result = dataproc.projects().regions().clusters().list(\n",
    "            projectId=project_id,\n",
    "            region=region).execute()\n",
    "        cluster_list = result['clusters']\n",
    "        cluster = [c\n",
    "                   for c in cluster_list\n",
    "                   if c['clusterName'] == cluster_name][0]\n",
    "        if cluster['status']['state'] == 'ERROR':\n",
    "            raise Exception(result['status']['details'])\n",
    "        if cluster['status']['state'] == 'RUNNING':\n",
    "            print(\"Cluster created.\")\n",
    "            break\n",
    "\n",
    "\n",
    "# [START list_clusters_with_detail]\n",
    "def list_clusters_with_details(dataproc, project, region):\n",
    "    result = dataproc.projects().regions().clusters().list(\n",
    "        projectId=project,\n",
    "        region=region).execute()\n",
    "    cluster_list = result['clusters']\n",
    "    for cluster in cluster_list:\n",
    "        print(\"{} - {}\"\n",
    "              .format(cluster['clusterName'], cluster['status']['state']))\n",
    "    return result\n",
    "# [END list_clusters_with_detail]\n",
    "\n",
    "\n",
    "def get_cluster_id_by_name(cluster_list, cluster_name):\n",
    "    \"\"\"Helper function to retrieve the ID and output bucket of a cluster by\n",
    "    name.\"\"\"\n",
    "    cluster = [c for c in cluster_list if c['clusterName'] == cluster_name][0]\n",
    "    return cluster['clusterUuid'], cluster['config']['configBucket']\n",
    "\n",
    "\n",
    "# [START submit_pyspark_job]\n",
    "def submit_pyspark_job(dataproc, project, region,\n",
    "                       cluster_name, bucket_name, filename):\n",
    "    \"\"\"Submits the Pyspark job to the cluster, assuming `filename` has\n",
    "    already been uploaded to `bucket_name`\"\"\"\n",
    "    job_details = {\n",
    "        'projectId': project,\n",
    "        'job': {\n",
    "            'placement': {\n",
    "                'clusterName': cluster_name\n",
    "            },\n",
    "            'pysparkJob': {\n",
    "                'mainPythonFileUri': 'gs://{}/{}'.format(bucket_name, filename)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    result = dataproc.projects().regions().jobs().submit(\n",
    "        projectId=project,\n",
    "        region=region,\n",
    "        body=job_details).execute()\n",
    "    job_id = result['reference']['jobId']\n",
    "    print('Submitted job ID {}'.format(job_id))\n",
    "    return job_id\n",
    "# [END submit_pyspark_job]\n",
    "\n",
    "\n",
    "# [START delete]\n",
    "def delete_cluster(dataproc, project, region, cluster):\n",
    "    print('Tearing down cluster')\n",
    "    result = dataproc.projects().regions().clusters().delete(\n",
    "        projectId=project,\n",
    "        region=region,\n",
    "        clusterName=cluster).execute()\n",
    "    return result\n",
    "# [END delete]\n",
    "\n",
    "\n",
    "# [START wait]\n",
    "def wait_for_job(dataproc, project, region, job_id):\n",
    "    print('Waiting for job to finish...')\n",
    "    while True:\n",
    "        result = dataproc.projects().regions().jobs().get(\n",
    "            projectId=project,\n",
    "            region=region,\n",
    "            jobId=job_id).execute()\n",
    "        # Handle exceptions\n",
    "        if result['status']['state'] == 'ERROR':\n",
    "            raise Exception(result['status']['details'])\n",
    "        elif result['status']['state'] == 'DONE':\n",
    "            print('Job finished.')\n",
    "            return result\n",
    "# [END wait]\n",
    "\n",
    "\n",
    "# [START get_client]\n",
    "def get_client():\n",
    "    \"\"\"Builds an http client authenticated with the service account\n",
    "    credentials.\"\"\"\n",
    "    dataproc = googleapiclient.discovery.build('dataproc', 'v1')\n",
    "    return dataproc\n",
    "# [END get_client]\n",
    "\n",
    "\n",
    "def main(project_id, zone, cluster_name, bucket_name,\n",
    "         instance_type, master_nodes, worker_nodes,\n",
    "         pyspark_file=None, create_new_cluster=True):\n",
    "    dataproc = get_client()\n",
    "    region = 'global'\n",
    "    try:\n",
    "        if pyspark_file:\n",
    "            spark_file, spark_filename = get_pyspark_file(pyspark_file)\n",
    "        else:\n",
    "            spark_file, spark_filename = get_default_pyspark_file()\n",
    "\n",
    "        if create_new_cluster:\n",
    "            create_cluster(\n",
    "                dataproc, project_id, zone, region, cluster_name,\n",
    "                instance_type, master_nodes, worker_nodes)\n",
    "            wait_for_cluster_creation(\n",
    "                dataproc, project_id, region, cluster_name)\n",
    "\n",
    "        upload_pyspark_file(\n",
    "            project_id, bucket_name, spark_filename, spark_file)\n",
    "\n",
    "        cluster_list = list_clusters_with_details(\n",
    "            dataproc, project_id, region)['clusters']\n",
    "\n",
    "        (cluster_id, output_bucket) = (\n",
    "            get_cluster_id_by_name(cluster_list, cluster_name))\n",
    "\n",
    "        # [START call_submit_pyspark_job]\n",
    "        job_id = submit_pyspark_job(\n",
    "            dataproc, project_id, region,\n",
    "            cluster_name, bucket_name, spark_filename)\n",
    "        # [END call_submit_pyspark_job]\n",
    "        wait_for_job(dataproc, project_id, region, job_id)\n",
    "\n",
    "        output = download_output(project_id, cluster_id, output_bucket, job_id)\n",
    "        print('Received job output {}'.format(output))\n",
    "        return output\n",
    "    finally:\n",
    "        if create_new_cluster:\n",
    "            delete_cluster(dataproc, project_id, region, cluster_name)\n",
    "        spark_file.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=__doc__,\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\n",
    "            '--project_id', \n",
    "            help='Project ID you want to access.',\n",
    "            required=True\n",
    "        ),\n",
    "    parser.add_argument(\n",
    "            '--zone',\n",
    "            help='Zone to create clusters in/connect to.',\n",
    "            required=True\n",
    "        ),\n",
    "    parser.add_argument(\n",
    "            '--cluster_name',\n",
    "            help='Name of the cluster to create/connect to',\n",
    "            required=True\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--gcs_bucket',\n",
    "            help='Bucket to upload Pyspark file to',\n",
    "            required=True\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--pyspark_file',\n",
    "            help='Pyspark filename. Defaults to pyspark_sort.py'\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--create_new_cluster',\n",
    "            action='store_true',\n",
    "            help='States if the cluster should be created'\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--key_file',\n",
    "            help='Location of your key file for service account'\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--instance_type',\n",
    "            help='Instance types used for this cluster',\n",
    "            default='n1-standard-8'\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--master_nodes',\n",
    "            help='Number of master nodes',\n",
    "            default=1\n",
    "        )\n",
    "    parser.add_argument(\n",
    "            '--worker_nodes',\n",
    "            help='Number of worker nodes',\n",
    "            default=2\n",
    "        )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.key_file is not None:\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = args.key_file\n",
    "\n",
    "    main(\n",
    "        args.project_id, args.zone, args.cluster_name,\n",
    "        args.gcs_bucket, args.instance_type, args.master_nodes, args.worker_nodes,\n",
    "        args.pyspark_file, args.create_new_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod a+x submit_job_to_cluster.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: h.t. create and run a spark job on a cluster using GCP\n",
    "Run the cell below to create a file called `pyspark_sort.py` in the current directory. Then run the bash cell to submit this job to GCP & spin up a cluster. (__`Note:`__ _make sure you have all the global variables set up first including the name of the spark job if you change it._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile LR.py\n",
    "#!/usr/bin/env python\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\" Sample pyspark script to be uploaded to Cloud Storage and run on\n",
    "Cloud Dataproc.\n",
    "\n",
    "Note this file is not intended to be run directly, but run inside a PySpark\n",
    "environment.\n",
    "\"\"\"\n",
    "\n",
    "# [START pyspark]\n",
    "# imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import time\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"final_project\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#Load Parquet file into a dataframe\n",
    "df = spark.read.parquet('gs://w261_sj_data/data/train')\n",
    "#df = spark.read.parquet('sample_train')\n",
    "# Define categorical and numerical columns\n",
    "categoricalCols = []\n",
    "numericalCols = []\n",
    "numericalColsImputed = []\n",
    "numericalColsLog = []\n",
    "for c in range(2,41):\n",
    "    col = \"_\"+str(c)\n",
    "    colImp = str(c)+\"_imp\"\n",
    "    colLog = str(c)+\"_log\"\n",
    "    if (c < 15):\n",
    "        numericalCols.append(col)\n",
    "        numericalColsImputed.append(colImp)\n",
    "        numericalColsLog.append(colLog)\n",
    "    else:\n",
    "        categoricalCols.append(col)\n",
    "\n",
    "for col in numericalCols:\n",
    "    df = df.withColumn(col, df[col].cast(\"double\"))\n",
    "#Also do this for column 0\n",
    "df = df.withColumn(\"_1\", df[\"_1\"].cast(\"int\"))                        \n",
    "stages = [] # stages in our Pipeline\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "for col in numericalCols:\n",
    "    df = df.withColumn(col, df[col].cast(\"double\"))\n",
    "    df= df.withColumn(col, when(df[col]<0, None).otherwise(df[col]))\n",
    "#Also do this for column 0\n",
    "df = df.withColumn(\"_1\", df[\"_1\"].cast(\"double\"))\n",
    "\n",
    "                        \n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(inputCols=numericalCols, outputCols=numericalColsImputed)\n",
    "stages += [imputer]\n",
    "imputer_model = imputer.fit(df)\n",
    "#df2 = imputer_model.transform(df)\n",
    "#df2.select(numericalCols).show(50)\n",
    "#df2.select(numericalColsImputed).show(50)\n",
    "\n",
    "#Compute log transforms\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "#Is there a better way to do this?\n",
    "sqlTrans = SQLTransformer(statement=\"SELECT *, \\\n",
    "                          log(2_imp+1) AS 2_log, \\\n",
    "                          log(3_imp+1) AS 3_log, \\\n",
    "                          log(4_imp+1) AS 4_log, \\\n",
    "                          log(5_imp+1) AS 5_log, \\\n",
    "                          log(6_imp+1) AS 6_log, \\\n",
    "                          log(7_imp+1) AS 7_log, \\\n",
    "                          log(8_imp+1) AS 8_log, \\\n",
    "                          log(9_imp+1) AS 9_log, \\\n",
    "                          log(10_imp+1) AS 10_log, \\\n",
    "                          log(11_imp+1) AS 11_log, \\\n",
    "                          log(12_imp+1) AS 12_log, \\\n",
    "                          log(13_imp+1) AS 13_log, \\\n",
    "                          log(14_imp+1) AS 14_log \\\n",
    "                          FROM __THIS__\")\n",
    "stages += [sqlTrans]\n",
    "#df3 = sqlTrans.transform(df2)\n",
    "#df3.select(numericalColsImputed).show(2,False)\n",
    "#df3.select(numericalColsLog).show(2,False)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "numericalAssembler = VectorAssembler(inputCols=numericalColsLog, outputCol=\"log_numerical_feature_vec\")\n",
    "stages += [numericalAssembler]\n",
    "#df4 = numericalAssembler.transform(df3)\n",
    "#df4.select(\"log_numerical_feature_vec\").show(2,False)\n",
    "\n",
    "#Now normalize these numerical features\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"log_numerical_feature_vec\", outputCol=\"scaled_features\")\n",
    "stages += [scaler]\n",
    "                        \n",
    "#Next transform Categorical Features with FeatureHasher\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "hasher = FeatureHasher(numFeatures=256, inputCols=categoricalCols,outputCol=\"categorical_features\")\n",
    "stages += [hasher]\n",
    "\n",
    "#Now create vector with numerical and categorical features\n",
    "finalAssembler = VectorAssembler(inputCols=[\"scaled_features\", \"categorical_features\"], outputCol=\"features\")\n",
    "stages += [finalAssembler]\n",
    "\n",
    "start = time.time()\n",
    "#Finally create a pipeline and verify\n",
    "from pyspark.ml import Pipeline\n",
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(df)\n",
    "preppedDataDF = pipelineModel.transform(df)\n",
    "print(f\"\\n...Pipelines Completed in {time.time() - start} seconds\")\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"_1\", \"features\"]\n",
    "dataset = preppedDataDF.select(selectedcols)\n",
    "\n",
    "featureset = dataset.repartition(16).write.parquet(\"gs://w261_sj_data/data/feature_set\")\n",
    "\n",
    "start = time.time()\n",
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())\n",
    "print(f\"\\n...Split Completed in {time.time() - start} seconds\")\n",
    "                        \n",
    "\n",
    "\n",
    "#Run logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"_1\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)\n",
    "                        \n",
    "                        \n",
    "predictions = lrModel.transform(testData)\n",
    "print(f\"\\n...Initial Model Completed in {time.time() - start} seconds\")\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 1.0]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.05, 0.01]) \\\n",
    "    .build()\n",
    "    \n",
    "pipeline = Pipeline(stages=[lr])\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(labelCol=\"_1\"),\n",
    "                          numFolds=5)  # use 3+ folds in practice\n",
    "start = time.time()\n",
    "cvModel = crossval.fit(trainingData)\n",
    "cvprediction = cvModel.transform(testData)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"_1\")\n",
    "evaluator.evaluate(cvprediction)\n",
    "print(f\"\\n... Completed in {time.time() - start} seconds\")\n",
    "                        \n",
    "                    \n",
    "# [End pyspark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster...\n",
      "Waiting for cluster creation...\n",
      "Cluster created.\n",
      "Uploading pyspark file to GCS\n",
      "w261-sj - RUNNING\n",
      "Submitted job ID 805c03f5-13b6-46c0-ac0a-e513f4fa719c\n",
      "Waiting for job to finish...\n",
      "Job finished.\n",
      "Downloading output file\n",
      "Received job output b\"Ivy Default Cache set to: /root/.ivy2/cache\\nThe jars for the packages stored in: /root/.ivy2/jars\\n:: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\\ncom.databricks#spark-xml_2.11 added as a dependency\\ngraphframes#graphframes added as a dependency\\ncom.databricks#spark-avro_2.11 added as a dependency\\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-3a1f70c4-cebd-4918-a3d9-b93d90710d12;1.0\\n\\tconfs: [default]\\n\\tfound com.databricks#spark-xml_2.11;0.4.1 in central\\n\\tfound graphframes#graphframes;0.5.0-spark2.1-s_2.11 in spark-packages\\n\\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\\n\\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\\n\\tfound org.scala-lang#scala-reflect;2.11.0 in central\\n\\tfound org.slf4j#slf4j-api;1.7.7 in central\\n\\tfound com.databricks#spark-avro_2.11;4.0.0 in central\\n\\tfound org.apache.avro#avro;1.7.6 in central\\n\\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\\n\\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\\n\\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\\n\\tfound org.xerial.snappy#snappy-java;1.0.5 in central\\n\\tfound org.apache.commons#commons-compress;1.4.1 in central\\n\\tfound org.tukaani#xz;1.0 in central\\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.11/0.4.1/spark-xml_2.11-0.4.1.jar ...\\n\\t[SUCCESSFUL ] com.databricks#spark-xml_2.11;0.4.1!spark-xml_2.11.jar (77ms)\\ndownloading http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.5.0-spark2.1-s_2.11/graphframes-0.5.0-spark2.1-s_2.11.jar ...\\n\\t[SUCCESSFUL ] graphframes#graphframes;0.5.0-spark2.1-s_2.11!graphframes.jar (141ms)\\ndownloading https://repo1.maven.org/maven2/com/databricks/spark-avro_2.11/4.0.0/spark-avro_2.11-4.0.0.jar ...\\n\\t[SUCCESSFUL ] com.databricks#spark-avro_2.11;4.0.0!spark-avro_2.11.jar (27ms)\\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-api_2.11/2.1.2/scala-logging-api_2.11-2.1.2.jar ...\\n\\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2!scala-logging-api_2.11.jar (27ms)\\ndownloading https://repo1.maven.org/maven2/com/typesafe/scala-logging/scala-logging-slf4j_2.11/2.1.2/scala-logging-slf4j_2.11-2.1.2.jar ...\\n\\t[SUCCESSFUL ] com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2!scala-logging-slf4j_2.11.jar (27ms)\\ndownloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.0/scala-reflect-2.11.0.jar ...\\n\\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.11.0!scala-reflect.jar (127ms)\\ndownloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.7/slf4j-api-1.7.7.jar ...\\n\\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.7!slf4j-api.jar (27ms)\\ndownloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.6/avro-1.7.6.jar ...\\n\\t[SUCCESSFUL ] org.apache.avro#avro;1.7.6!avro.jar(bundle) (29ms)\\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\\n\\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (27ms)\\ndownloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\\n\\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (28ms)\\ndownloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar ...\\n\\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (27ms)\\ndownloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.0.5/snappy-java-1.0.5.jar ...\\n\\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.0.5!snappy-java.jar(bundle) (31ms)\\ndownloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar ...\\n\\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.4.1!commons-compress.jar (27ms)\\ndownloading https://repo1.maven.org/maven2/org/tukaani/xz/1.0/xz-1.0.jar ...\\n\\t[SUCCESSFUL ] org.tukaani#xz;1.0!xz.jar (27ms)\\n:: resolution report :: resolve 5458ms :: artifacts dl 659ms\\n\\t:: modules in use:\\n\\tcom.databricks#spark-avro_2.11;4.0.0 from central in [default]\\n\\tcom.databricks#spark-xml_2.11;0.4.1 from central in [default]\\n\\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\\n\\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\\n\\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\\n\\tgraphframes#graphframes;0.5.0-spark2.1-s_2.11 from spark-packages in [default]\\n\\torg.apache.avro#avro;1.7.6 from central in [default]\\n\\torg.apache.commons#commons-compress;1.4.1 from central in [default]\\n\\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\\n\\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\\n\\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\\n\\torg.slf4j#slf4j-api;1.7.7 from central in [default]\\n\\torg.tukaani#xz;1.0 from central in [default]\\n\\torg.xerial.snappy#snappy-java;1.0.5 from central in [default]\\n\\t:: evicted modules:\\n\\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;1.7.7] in [default]\\n\\torg.slf4j#slf4j-api;1.6.4 by [org.slf4j#slf4j-api;1.7.7] in [default]\\n\\t---------------------------------------------------------------------\\n\\t|                  |            modules            ||   artifacts   |\\n\\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\\n\\t---------------------------------------------------------------------\\n\\t|      default     |   16  |   14  |   14  |   2   ||   14  |   14  |\\n\\t---------------------------------------------------------------------\\n:: retrieving :: org.apache.spark#spark-submit-parent-3a1f70c4-cebd-4918-a3d9-b93d90710d12\\n\\tconfs: [default]\\n\\t14 artifacts copied, 0 already retrieved (7998kB/219ms)\\n19/08/05 22:25:35 INFO org.spark_project.jetty.util.log: Logging initialized @9760ms\\n19/08/05 22:25:35 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\\n19/08/05 22:25:36 INFO org.spark_project.jetty.server.Server: Started @9858ms\\n19/08/05 22:25:36 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@5990ec45{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n19/08/05 22:25:36 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\\n19/08/05 22:25:44 WARN org.apache.spark.util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\\n19/08/05 22:25:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]\\n19/08/05 22:25:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]\\n19/08/05 22:25:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]\\n19/08/05 22:25:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]\\n19/08/05 22:25:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]\\n19/08/05 22:25:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]\\n19/08/05 22:25:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]\\n19/08/05 22:25:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]\\n\\n...Pipelines Completed in 941.8291144371033 seconds\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:07:20 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:07:23 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\\n19/08/05 23:07:23 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\\n19/08/05 23:07:23 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\\n19/08/05 23:07:23 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\\n19/08/05 23:07:24 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\\n19/08/05 23:07:24 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\\n19/08/05 23:07:24 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\\n19/08/05 23:07:25 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\\n19/08/05 23:08:04 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134769828 > 134217728: flushing 1952642 records to disk.\\n19/08/05 23:08:04 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134384868\\n19/08/05 23:08:04 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134614848 > 134217728: flushing 1950100 records to disk.\\n19/08/05 23:08:04 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134266399\\n19/08/05 23:08:04 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134715711 > 134217728: flushing 1952642 records to disk.\\n19/08/05 23:08:04 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134339522\\n19/08/05 23:08:04 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134377493 > 134217728: flushing 1950100 records to disk.\\n19/08/05 23:08:04 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134030984\\n19/08/05 23:08:05 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134803843 > 134217728: flushing 1952642 records to disk.\\n19/08/05 23:08:05 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134406287\\n19/08/05 23:08:05 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134645116 > 134217728: flushing 1950100 records to disk.\\n19/08/05 23:08:05 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134303692\\n19/08/05 23:08:05 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134709779 > 134217728: flushing 1950100 records to disk.\\n19/08/05 23:08:05 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134368332\\n19/08/05 23:08:05 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134701860 > 134217728: flushing 1950100 records to disk.\\n19/08/05 23:08:05 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134339647\\n19/08/05 23:08:23 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62316456\\n19/08/05 23:08:23 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62357232\\n19/08/05 23:08:24 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62274653\\n19/08/05 23:08:24 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62242691\\n19/08/05 23:08:24 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62225247\\n19/08/05 23:08:25 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62280801\\n19/08/05 23:08:25 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62231744\\n19/08/05 23:08:25 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62299492\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:08:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:08:27 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:08:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\\n19/08/05 23:08:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\\n19/08/05 23:09:06 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 135334865 > 134217728: flushing 1960100 records to disk.\\n19/08/05 23:09:06 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134979429\\n19/08/05 23:09:07 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134830658 > 134217728: flushing 1952642 records to disk.\\n19/08/05 23:09:07 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134458846\\n19/08/05 23:09:07 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134311544 > 134217728: flushing 1950100 records to disk.\\n19/08/05 23:09:07 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 133985353\\n19/08/05 23:09:07 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134969813 > 134217728: flushing 1952642 records to disk.\\n19/08/05 23:09:07 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134586839\\n19/08/05 23:09:08 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134574017 > 134217728: flushing 1950100 records to disk.\\n19/08/05 23:09:08 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134227611\\n19/08/05 23:09:08 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134800058 > 134217728: flushing 1952642 records to disk.\\n19/08/05 23:09:08 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134418860\\n19/08/05 23:09:08 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134863118 > 134217728: flushing 1952642 records to disk.\\n19/08/05 23:09:08 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134467326\\n19/08/05 23:09:09 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 134784021 > 134217728: flushing 1950100 records to disk.\\n19/08/05 23:09:09 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 134440540\\n19/08/05 23:09:26 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 61624109\\n19/08/05 23:09:27 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62336436\\n19/08/05 23:09:27 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62242321\\n19/08/05 23:09:28 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62265721\\n19/08/05 23:09:28 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62269875\\n19/08/05 23:09:28 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62289044\\n19/08/05 23:09:28 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62252077\\n19/08/05 23:09:29 INFO org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 62330078\\n32089703\\n13750914\\n\\n...Split Completed in 2793.872659444809 seconds\\n19/08/06 00:03:48 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_0 in memory! (computed 228.5 MB so far)\\n19/08/06 00:03:48 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_0 to disk instead.\\n19/08/06 00:03:48 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_4 in memory! (computed 515.4 MB so far)\\n19/08/06 00:03:48 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_4 to disk instead.\\n19/08/06 00:03:48 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_7 in memory! (computed 345.1 MB so far)\\n19/08/06 00:03:48 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_7 to disk instead.\\n19/08/06 00:03:48 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_2 in memory! (computed 344.4 MB so far)\\n19/08/06 00:03:48 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_2 to disk instead.\\n19/08/06 00:03:48 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_5 in memory! (computed 517.9 MB so far)\\n19/08/06 00:03:48 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_5 to disk instead.\\n19/08/06 00:03:48 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_3 in memory! (computed 515.8 MB so far)\\n19/08/06 00:03:48 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_3 to disk instead.\\n19/08/06 00:03:49 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_1 in memory! (computed 515.3 MB so far)\\n19/08/06 00:03:49 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_1 to disk instead.\\n19/08/06 00:04:02 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_3 in memory! (computed 515.8 MB so far)\\n19/08/06 00:04:02 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_5 in memory! (computed 517.9 MB so far)\\n19/08/06 00:04:02 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_7 in memory! (computed 345.1 MB so far)\\n19/08/06 00:04:03 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_2 in memory! (computed 344.4 MB so far)\\n19/08/06 00:04:03 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_1 in memory! (computed 515.3 MB so far)\\n19/08/06 00:11:46 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_9 in memory! (computed 344.0 MB so far)\\n19/08/06 00:11:46 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_9 to disk instead.\\n19/08/06 00:11:46 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_12 in memory! (computed 28.6 MB so far)\\n19/08/06 00:11:46 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_12 to disk instead.\\n19/08/06 00:11:46 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_11 in memory! (computed 28.6 MB so far)\\n19/08/06 00:11:46 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_11 to disk instead.\\n19/08/06 00:11:46 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_10 in memory! (computed 18.8 MB so far)\\n19/08/06 00:11:46 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_10 to disk instead.\\n19/08/06 00:11:46 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_13 in memory! (computed 28.6 MB so far)\\n19/08/06 00:11:46 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_13 to disk instead.\\n19/08/06 00:11:46 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_14 in memory! (computed 28.1 MB so far)\\n19/08/06 00:11:46 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_14 to disk instead.\\n19/08/06 00:11:57 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_15 in memory! (computed 229.6 MB so far)\\n19/08/06 00:11:57 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_15 to disk instead.\\n19/08/06 00:11:57 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_9 in memory! (computed 344.0 MB so far)\\n19/08/06 00:12:03 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_12 in memory! (computed 341.4 MB so far)\\n19/08/06 00:12:03 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_13 in memory! (computed 343.3 MB so far)\\n19/08/06 00:12:03 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_14 in memory! (computed 343.2 MB so far)\\n19/08/06 00:12:03 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_10 in memory! (computed 343.1 MB so far)\\n19/08/06 00:20:56 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_16 in memory! (computed 518.8 MB so far)\\n19/08/06 00:20:56 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_16 to disk instead.\\n19/08/06 00:21:21 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_18 in memory! (computed 28.5 MB so far)\\n19/08/06 00:21:21 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_18 to disk instead.\\n19/08/06 00:21:21 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_22 in memory! (computed 42.9 MB so far)\\n19/08/06 00:21:21 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_22 to disk instead.\\n19/08/06 00:21:21 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_21 in memory! (computed 28.6 MB so far)\\n19/08/06 00:21:21 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_21 to disk instead.\\n19/08/06 00:21:21 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_19 in memory! (computed 42.9 MB so far)\\n19/08/06 00:21:21 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_19 to disk instead.\\n19/08/06 00:21:21 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_20 in memory! (computed 44.2 MB so far)\\n19/08/06 00:21:21 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_20 to disk instead.\\n19/08/06 00:21:21 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_17 in memory! (computed 150.1 MB so far)\\n19/08/06 00:21:21 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_17 to disk instead.\\n19/08/06 00:21:30 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_23 in memory! (computed 229.6 MB so far)\\n19/08/06 00:21:30 WARN org.apache.spark.storage.BlockManager: Persisting block rdd_65_23 to disk instead.\\n19/08/06 00:21:39 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_22 in memory! (computed 229.7 MB so far)\\n19/08/06 00:21:39 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_19 in memory! (computed 230.1 MB so far)\\n19/08/06 00:21:39 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_20 in memory! (computed 229.6 MB so far)\\n19/08/06 00:21:40 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_18 in memory! (computed 230.7 MB so far)\\n19/08/06 00:21:40 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_21 in memory! (computed 230.8 MB so far)\\n19/08/06 00:21:51 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_6 in memory! (computed 44.4 MB so far)\\n19/08/06 00:21:51 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_7 in memory! (computed 42.2 MB so far)\\n19/08/06 00:21:51 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_2 in memory! (computed 43.1 MB so far)\\n19/08/06 00:21:51 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_5 in memory! (computed 43.1 MB so far)\\n19/08/06 00:21:51 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_3 in memory! (computed 42.2 MB so far)\\n19/08/06 00:21:51 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_1 in memory! (computed 43.0 MB so far)\\n19/08/06 00:21:51 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_0 in memory! (computed 64.7 MB so far)\\n19/08/06 00:21:51 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_4 in memory! (computed 64.6 MB so far)\\n19/08/06 00:21:59 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_13 in memory! (computed 42.9 MB so far)\\n19/08/06 00:21:59 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_14 in memory! (computed 42.2 MB so far)\\n19/08/06 00:21:59 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_9 in memory! (computed 64.3 MB so far)\\n19/08/06 00:21:59 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_10 in memory! (computed 64.2 MB so far)\\n19/08/06 00:21:59 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_12 in memory! (computed 149.2 MB so far)\\n19/08/06 00:22:00 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_8 in memory! (computed 228.6 MB so far)\\n19/08/06 00:22:03 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_18 in memory! (computed 150.4 MB so far)\\n19/08/06 00:22:03 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_19 in memory! (computed 150.9 MB so far)\\n19/08/06 00:22:03 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_21 in memory! (computed 28.6 MB so far)\\n19/08/06 00:22:04 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_22 in memory! (computed 150.5 MB so far)\\n19/08/06 00:22:05 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_20 in memory! (computed 344.4 MB so far)\\n19/08/06 00:22:09 INFO com.github.fommil.jni.JniLoader: successfully loaded /tmp/jniloader4625082307109397601netlib-native_system-linux-x86_64.so\\n19/08/06 00:22:09 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_1 in memory! (computed 43.0 MB so far)\\n19/08/06 00:22:09 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_5 in memory! (computed 43.1 MB so far)\\n19/08/06 00:22:09 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_7 in memory! (computed 42.2 MB so far)\\n19/08/06 00:22:09 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_4 in memory! (computed 43.1 MB so far)\\n19/08/06 00:22:09 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_2 in memory! (computed 64.7 MB so far)\\n19/08/06 00:22:09 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_0 in memory! (computed 43.1 MB so far)\\n19/08/06 00:22:10 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_3 in memory! (computed 150.3 MB so far)\\n19/08/06 00:22:11 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_6 in memory! (computed 342.4 MB so far)\\n19/08/06 00:22:14 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_12 in memory! (computed 43.0 MB so far)\\n19/08/06 00:22:15 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_13 in memory! (computed 42.9 MB so far)\\n19/08/06 00:22:15 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_10 in memory! (computed 64.2 MB so far)\\n19/08/06 00:22:15 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_14 in memory! (computed 42.2 MB so far)\\n19/08/06 00:22:15 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_9 in memory! (computed 150.5 MB so far)\\n19/08/06 00:22:16 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_8 in memory! (computed 342.9 MB so far)\\n19/08/06 00:22:17 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_18 in memory! (computed 28.5 MB so far)\\n19/08/06 00:22:19 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_19 in memory! (computed 345.2 MB so far)\\n19/08/06 00:22:19 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_20 in memory! (computed 18.8 MB so far)\\n19/08/06 00:22:19 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_22 in memory! (computed 18.8 MB so far)\\n19/08/06 00:22:19 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_21 in memory! (computed 28.6 MB so far)\\n19/08/06 00:22:24 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_1 in memory! (computed 43.0 MB so far)\\n19/08/06 00:22:24 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_2 in memory! (computed 43.1 MB so far)\\n19/08/06 00:22:24 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_0 in memory! (computed 43.1 MB so far)\\n19/08/06 00:22:24 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_6 in memory! (computed 44.4 MB so far)\\n19/08/06 00:22:24 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_3 in memory! (computed 42.2 MB so far)\\n19/08/06 00:22:24 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_5 in memory! (computed 43.1 MB so far)\\n19/08/06 00:22:25 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_4 in memory! (computed 150.5 MB so far)\\n19/08/06 00:22:26 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_7 in memory! (computed 345.1 MB so far)\\n19/08/06 00:22:29 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_8 in memory! (computed 64.6 MB so far)\\n19/08/06 00:22:29 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_14 in memory! (computed 28.1 MB so far)\\n19/08/06 00:22:29 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_10 in memory! (computed 64.2 MB so far)\\n19/08/06 00:22:29 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_12 in memory! (computed 64.5 MB so far)\\n19/08/06 00:22:30 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_9 in memory! (computed 96.4 MB so far)\\n19/08/06 00:22:31 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_13 in memory! (computed 343.3 MB so far)\\n19/08/06 00:22:32 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_18 in memory! (computed 42.8 MB so far)\\n19/08/06 00:22:34 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_20 in memory! (computed 18.8 MB so far)\\n19/08/06 00:22:34 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_21 in memory! (computed 12.1 MB so far)\\n19/08/06 00:22:35 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_22 in memory! (computed 42.9 MB so far)\\n19/08/06 00:22:35 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_19 in memory! (computed 345.2 MB so far)\\n19/08/06 00:22:39 INFO breeze.optimize.StrongWolfeLineSearch: Line search t: 1.388785549588201 fval: 0.5288364102209532 rhs: 0.5690226756555669 cdd: 0.001445841052683602\\n19/08/06 00:22:39 INFO breeze.optimize.LBFGS: Step Size: 1.389\\n19/08/06 00:22:39 INFO breeze.optimize.LBFGS: Val and Grad Norm: 0.528836 (rel: 0.0706) 0.527745\\n19/08/06 00:22:40 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_0 in memory! (computed 43.1 MB so far)\\n19/08/06 00:22:40 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_6 in memory! (computed 44.4 MB so far)\\n19/08/06 00:22:40 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_7 in memory! (computed 42.2 MB so far)\\n19/08/06 00:22:40 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_4 in memory! (computed 43.1 MB so far)\\n19/08/06 00:22:40 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_1 in memory! (computed 43.0 MB so far)\\n19/08/06 00:22:40 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_3 in memory! (computed 95.0 MB so far)\\n19/08/06 00:22:40 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_2 in memory! (computed 150.1 MB so far)\\n19/08/06 00:22:42 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_5 in memory! (computed 345.3 MB so far)\\n19/08/06 00:22:47 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_12 in memory! (computed 43.0 MB so far)\\n19/08/06 00:22:47 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_13 in memory! (computed 28.6 MB so far)\\n19/08/06 00:22:47 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_14 in memory! (computed 28.1 MB so far)\\n19/08/06 00:22:48 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_10 in memory! (computed 97.2 MB so far)\\n19/08/06 00:22:48 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_8 in memory! (computed 149.9 MB so far)\\n19/08/06 00:22:48 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_9 in memory! (computed 150.5 MB so far)\\n19/08/06 00:22:51 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_18 in memory! (computed 230.7 MB so far)\\n19/08/06 00:22:52 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_20 in memory! (computed 28.7 MB so far)\\n19/08/06 00:22:52 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_22 in memory! (computed 7.9 MB so far)\\n19/08/06 00:22:53 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_21 in memory! (computed 43.0 MB so far)\\n19/08/06 00:22:53 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_19 in memory! (computed 345.2 MB so far)\\n19/08/06 00:22:58 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_6 in memory! (computed 44.4 MB so far)\\n19/08/06 00:22:58 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_2 in memory! (computed 43.1 MB so far)\\n19/08/06 00:22:58 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_3 in memory! (computed 63.3 MB so far)\\n19/08/06 00:22:58 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_5 in memory! (computed 64.7 MB so far)\\n19/08/06 00:22:58 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_0 in memory! (computed 97.1 MB so far)\\n19/08/06 00:22:58 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_4 in memory! (computed 97.1 MB so far)\\n19/08/06 00:22:58 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_7 in memory! (computed 94.9 MB so far)\\n19/08/06 00:23:01 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_1 in memory! (computed 343.5 MB so far)\\n19/08/06 00:23:04 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_13 in memory! (computed 42.9 MB so far)\\n19/08/06 00:23:04 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_8 in memory! (computed 97.2 MB so far)\\n19/08/06 00:23:04 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_10 in memory! (computed 64.2 MB so far)\\n19/08/06 00:23:04 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_14 in memory! (computed 28.1 MB so far)\\n19/08/06 00:23:04 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_9 in memory! (computed 96.4 MB so far)\\n19/08/06 00:23:06 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_12 in memory! (computed 341.4 MB so far)\\n19/08/06 00:23:07 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_19 in memory! (computed 150.9 MB so far)\\n19/08/06 00:23:08 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_20 in memory! (computed 18.8 MB so far)\\n19/08/06 00:23:09 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_22 in memory! (computed 18.8 MB so far)\\n19/08/06 00:23:09 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_21 in memory! (computed 28.6 MB so far)\\n19/08/06 00:23:09 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_18 in memory! (computed 346.1 MB so far)\\n19/08/06 00:23:13 INFO breeze.optimize.StrongWolfeLineSearch: Line search t: 0.3336591050654625 fval: 0.5167551269762428 rhs: 0.5288340464540894 cdd: -7.801660506888105E-4\\n19/08/06 00:23:13 INFO breeze.optimize.LBFGS: Step Size: 0.3337\\n19/08/06 00:23:13 INFO breeze.optimize.LBFGS: Val and Grad Norm: 0.516755 (rel: 0.0228) 0.0759790\\n19/08/06 00:23:14 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_2 in memory! (computed 43.1 MB so far)\\n19/08/06 00:23:14 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_0 in memory! (computed 43.1 MB so far)\\n19/08/06 00:23:14 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_6 in memory! (computed 44.4 MB so far)\\n19/08/06 00:23:14 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_5 in memory! (computed 43.1 MB so far)\\n19/08/06 00:23:14 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_4 in memory! (computed 43.1 MB so far)\\n19/08/06 00:23:14 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_7 in memory! (computed 42.2 MB so far)\\n19/08/06 00:23:15 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_1 in memory! (computed 150.9 MB so far)\\n19/08/06 00:23:16 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_3 in memory! (computed 343.9 MB so far)\\n19/08/06 00:23:19 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_8 in memory! (computed 64.6 MB so far)\\n19/08/06 00:23:19 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_9 in memory! (computed 64.3 MB so far)\\n19/08/06 00:23:19 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_10 in memory! (computed 64.2 MB so far)\\n19/08/06 00:23:19 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_12 in memory! (computed 64.5 MB so far)\\n19/08/06 00:23:20 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_14 in memory! (computed 150.4 MB so far)\\n19/08/06 00:23:21 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_13 in memory! (computed 343.3 MB so far)\\n19/08/06 00:23:22 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_19 in memory! (computed 18.8 MB so far)\\n19/08/06 00:23:22 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_18 in memory! (computed 42.8 MB so far)\\n19/08/06 00:23:25 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_22 in memory! (computed 96.5 MB so far)\\n19/08/06 00:23:25 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_21 in memory! (computed 150.4 MB so far)\\n19/08/06 00:23:25 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_20 in memory! (computed 150.1 MB so far)\\n19/08/06 00:23:29 INFO breeze.optimize.LBFGS: Step Size: 1.000\\n19/08/06 00:23:29 INFO breeze.optimize.LBFGS: Val and Grad Norm: 0.516199 (rel: 0.00108) 0.0683968\\n19/08/06 00:23:29 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_5 in memory! (computed 43.1 MB so far)\\n19/08/06 00:23:29 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_6 in memory! (computed 44.4 MB so far)\\n19/08/06 00:23:29 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_1 in memory! (computed 43.0 MB so far)\\n19/08/06 00:23:29 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_3 in memory! (computed 42.2 MB so far)\\n19/08/06 00:23:29 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_0 in memory! (computed 43.1 MB so far)\\n19/08/06 00:23:29 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_4 in memory! (computed 43.1 MB so far)\\n19/08/06 00:23:30 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_2 in memory! (computed 150.1 MB so far)\\n19/08/06 00:23:31 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_7 in memory! (computed 345.1 MB so far)\\n19/08/06 00:23:35 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_8 in memory! (computed 97.2 MB so far)\\n19/08/06 00:23:35 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_9 in memory! (computed 96.4 MB so far)\\n19/08/06 00:23:35 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_13 in memory! (computed 28.6 MB so far)\\n19/08/06 00:23:35 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_14 in memory! (computed 28.1 MB so far)\\n19/08/06 00:23:35 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_10 in memory! (computed 97.2 MB so far)\\n19/08/06 00:23:37 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_12 in memory! (computed 341.4 MB so far)\\n19/08/06 00:23:37 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_19 in memory! (computed 7.9 MB so far)\\n19/08/06 00:23:37 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_18 in memory! (computed 42.8 MB so far)\\n19/08/06 00:23:40 WARN org.apache.spark.storage.memory.MemoryStore: Not enough space to cache rdd_65_22 in memory! (computed 96.5 MB so far)\"\n",
      "Tearing down cluster\n"
     ]
    }
   ],
   "source": [
    "!python3 submit_job_to_cluster.py \\\n",
    "    --project_id='w262-245821' \\\n",
    "    --zone=us-central1-b \\\n",
    "    --cluster_name='w261-sj' \\\n",
    "    --gcs_bucket='w261_sj_data' \\\n",
    "    --key_file=$HOME/w261.json \\\n",
    "    --create_new_cluster \\\n",
    "    --pyspark_file=LR.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "PROJECT_ID = 'w262-245821' # fill in your GCP project id\n",
    "BUCKET_NAME = 'w261_sj_data' # fill in the name of your GCP bucket\n",
    "CLUSTER_NAME = 'w261-sj' # choose a cluster name, this should include only a-z, 0-9 & start with a letter\n",
    "HOME = '/Users/sid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
